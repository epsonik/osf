,ID,NAZWASEKCJI,WARTOSC,DATA_WPLYWU,WI_KONKURS_ID,TYTUL,SLOWA_KLUCZE,WI_STATUSWNIOSKU_KOD
0,916,opisProjektuStanWiedzy,Stan wiedzy w zakresie tematu badań dla projektu 1.,06/07/04,1,Paweł 1,asddvv,2
1,1055,opisProjektuStanWiedzy,"<meta http-equiv=""CONTENT-TYPE"" content=""text/html; charset=utf-8"" />
<title></title>
<meta name=""GENERATOR"" content=""OpenOffice.org 1.1.0  (Linux)"" />
<meta name=""AUTHOR"" content=""LSK"" />
<meta name=""CREATED"" content=""20060707;13290000"" />
<meta name=""CHANGEDBY"" content=""Pietraszek"" />
<meta name=""CHANGED"" content=""20060707;13290000"" />
<meta name=""CLASSIFICATION"" content=""Birthday "" />
<meta name=""DESCRIPTION"" content=""Shankar’s Birthday falls on 25th July.  Don’t Forget to wish him"" />
<meta name=""KEYWORDS"" content=""Birthday"" /><style type=""text/css"">
	<!--
		@page { margin: 2cm }
		P { margin-bottom: 0.21cm }
		P.western { font-family: ""Luxi Serif"", serif; so-language: pl-PL }
	-->
	</style>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> <font face=""Times New Roman""><font color=""#000000"">Odpalanie niekierowanych pocisków rakietowych i strzelanie z broni lufowej wymaga określenia kierunku osi wyrzutni (lufy), który zapewni trafienie pocisków w cel. Zwykle kierunek wyrzutni określa się poprawkami kątowymi względem linii celu łączącej punkt wylotu pocisku rakietowego <br />z celem. Poprawki te uwzględniają podczas celowania przeniżenie „Bn” toru lotu pocisku  pod działaniem siły ciężkości, prędkość własną nosiciela, wpływ wiatru oraz ruch celu. Ponieważ na nosicielu wyrzutnie są nieruchome, celowanie podczas odpalania pocisków rakietowych osiąga się poprzez manewrowanie nosicielem. </font></font> </p>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> <font face=""Times New Roman""><font color=""#000000"">W celu wyjaśnienia procesu celowania rozpatrzono przykładowo elementy toru niekierowanego pocisku rakietowego (rys. 1) oraz schemat celowania przy odpalaniu do celów naziemnych (rys. 2).</font></font></p>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> <font face=""Times New Roman""><font color=""#000000"">Podczas odpalania z nosiciela lecącego z prędkością V<sub>1</sub>, pocisk rakietowy pod działaniem siły ciągu silnika rakietowego w punkcie wylotu osiąga względem nosiciela prędkość V<sub>0</sub>. Ruch pocisku rakietowego w powietrzu po wylocie z lufy dzieli się na dwa odcinki: aktywny i pasywny. Odcinek od punktu wylotu do punktu zakończenia pracy silnika nazywa się odcinkiem aktywnym. Na tym odcinku masa pocisku zmniejsza się, a na pocisk działają siły:</font></font></p>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> </p>
<ul>
    <li><font color=""#000000""><font face=""Times New Roman"">ciągu silnika rakietowego;</font></font></li>
</ul>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> </p>
<ul>
    <li>
    <p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""> 	<font color=""#000000""><font face=""Times New Roman""> 	aerodynamiczna;</font></font><font color=""#000000""><font face=""Times New Roman""> <br /></font></font></p>
    </li>
    <li>
    <p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""><font color=""#000000""><font face=""Times New Roman"">ciężkości.</font></font></p>
    </li>
</ul>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""><font face=""Times New Roman""><font color=""#000000"">Pod działaniem siły ciągu prędkość pocisku zwiększa się od wartości początkowej V<sub>01</sub> <br /></font></font><br /> </p>
<p class=""western"" align=""center"" style=""margin-bottom: 0cm; line-height: 100%;""> <font color=""#000000"">V<sub>01</sub>=V<sub>1</sub>+V<sub>0</sub></font></p>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""><font color=""#000000""><font face=""Times New Roman"">					</font></font></p>
<p class=""western"" style=""margin-bottom: 0cm; line-height: 100%;""><font face=""Times New Roman""><font color=""#000000"">do maksymalnej V<sub>max1 </sub>na końcu odcinka aktywnego. Po zakończeniu pracy silnika rakietowego pocisk kontynuuje lot balistyczny pod działaniem sił aerodynamicznej i ciężkości. Ten odcinek nazywa się pasywnym lub balistycznym.</font></font></p>...",06/07/25,1,Eksperymentalna weryfikacja matematycznego modelu systemu celowniczego śmigłowca,"lotnictwo, mechanika lotu, dynamika środków bojowych, badania w locie",183
2,1076,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify""><strong style=""mso-bidi-font-weight: normal""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt; mso-bidi-font-family: ’Times New Roman’""><font size=""3"">Projekt jest całkowicie zgodny z badaniami i pr&oacute;bami realizowanymi w ośrodkach silnikowych na całym świecie.<o:p></o:p></font></span></strong></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify; mso-layout-grid-align: none""><strong style=""mso-bidi-font-weight: normal""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt; mso-bidi-font-family: ’Times New Roman’""><font size=""3"">Problemami paliw alternatywnych w lotniczych silnikach tłokowych zajmuje się Baylor University od lat 80-ych. W&oacute;wczas to na samolocie Bellanca 8KCAB Decatlon używając etanolu z 10% zawartością wody wykonano z Texasu pierwszego&nbsp;transkontynentalnego lotu do Brazylii. W Centrum Rozwoju Lotniczych Paliw Odnawialnych Uniwersytetu Baylor dokonano następnie po 13 letnim programie badawczym, modyfikacji silnika Lycoming O&nbsp;‑&nbsp;235 do zasilania alkoholem etylowym E95 z 5% zawartością benzyny spełniającego wymagania zgodne z procedurami FAA. Do certyfikacji z tym silnikiem przygotowano samolot Cessna 152. Sprawdzano także mieszanki 10% etanolu i benzyny lotniczej na 5-ciu samolotach. Wymagania certyfikacyjne zawarte są w The Federal Air Regulations Part 33. Jakby podkreślając użycie biopaliw w lotnictwie dokonano 25 września 1989 roku pierwszego lotu na czystym etanolu przez Atlantyk zaczynając w Waco - Texas a kończąc w Paryżu (specjalnie przygotowany do lotu<span style=""mso-spacerun: yes"">&nbsp; </span>samolot kompozytowy &bdquo;Velocity&rdquo; z silnikiem Lycoming HIO-360, o podniesionym stopniu sprężania!). Koszt paliwa bioetanolowego przy przelocie przez Atlantyk&nbsp;był o 30% niższy niż gdyby użyto benzyny&nbsp;lotniczej! Podczas pr&oacute;b silnikowych z etanolem stwierdzono wydłużenie się czasu przeglądu tzw. TBO niemal dwukrotnie. Wersje tego samego silnika, ale na klasyczne ołowiowe paliwo lotnicze tzw. Avgas stosuje się w kompozytowym samolocie I-23 &bdquo;Manager&rdquo; opracowanym w Instytucie Lotnictwa. W sprawie silnika HIO 360 nawiązano kontakt z firmą Lycoming i dr Max Shauckiem. Dr Max Schauck odwiedził Instytut Lotnictwa i wygłosił referat w dniu 30.09.2002 roku na seminarium pt.: &bdquo;Review of biofuel activieties in Aviation&rdquo;. W Baylor University uważa się, że wprowadzenie biopaliw do lotnictwa będzie<span style=""mso-spacerun: yes"">&nbsp; </span>miało duże znaczenie dla rozwoju produkcji biopaliw jak i czystości powietrza. Uniwersytet ten rozpoczął ostatnio także akcję &bdquo;Clean Airports&rdquo;, kt&oacute;ra zainteresowała europejskie środowisko lotnicze. W ramach VI programu Unii Europejskiej będzie zajmowała się tym problemem sieć Aeronet III, w kt&oacute;rej uczestniczy Instytut Lotnictwa a także i ECATS, jak się spodziewamy. Wprowadzenie biopaliw do lotnictwa jest kwestia czasu, gł&oacute;wnie z uwagi na to, że problem wprowadzenia wodoru jako paliwa alternatywnego wymaga zupełnie nowego podejścia do problemu bezpieczeństwa jak i zmiany konstrukcji samolotu, co opisano np. w ramach projektu CRYOPLANE. Należy także podkreślić zasługi Brazylii - gdzie w 1994 firma <em style=""mso-bidi-font-style: normal""><span style=""COLOR: black"">&ndash; </span></em><span style=""COLOR: black"">Ind&uacute;stria Aeron&aacute;utica Neiva (własciciel Embraer) otrzymała certyfikat na samolot </span>rolniczy IPANEMA z silnikiem Lycoming 0 540 od <span style=""COLOR: black"">Centro T&eacute;cnico Aeroespacial (CTA).</span> Jak podają, w Brazyli etanol jako paliwo lotnicze jest 3 razy tańszy od popularnego Avgasu!<span style=""COLOR: black""> Neiva zarejestrowała nazwę &ldquo;AvAlc&rdquo; (Aviation Alcohol) jako nazwę dla tego nowego paliwa lotniczego w Brazylii. Modyfikacja jest niedroga i efektywna kosztowo. Zmodyfikowano już 69 sam...",06/07/20,1,Eksperymentalna ocena paliw z bioetanolem do tłokowych silników lotniczych,"biopaliwo, paliwo lotnicze, emisja, cząstki stałe, ekologia, tłokowy silnik lotniczy ",173
3,1155,opisProjektuStanWiedzy,"<p>W latach 40. ubiegłego wieku Claude Shannon [2] położył podwaliny pod wsp&oacute;łczesną teorię informacji i komunikacji, podając dwa twierdzenia na bazie, kt&oacute;rych można szacować ilość zasob&oacute;w potrzebnych do przechowywania informacji oraz oceniać możliwość przesyłania wiadomości przez zaszumione kanały. Badania w tej dziedzinie przyniosły wiele imponujących wynik&oacute;w, kt&oacute;rym zawdzięczamy obecnie wysoko rozwinięte techniki komunikacyjne. Mechanika kwantowa pozwala jeszcze bardziej rozszerzyć możliwości komunikacji o np. kwantową teleportację [3], polegającą na przesyłaniu na dowolną odległość stanu obiektu kwantowego (atomu, fotonu). Aby skorzystać z możliwości jakie daje mechanika kwantowa w przesyłaniu informacji istnieje potrzeba tworzenia tzw. kwantowych kanał&oacute;w przesyłu informacji [4]. Ponadto, ich efektywne wykorzystanie zależy od dogłębnego poznania i zrozumienia wszystkich proces&oacute;w towarzyszących ich użyciu. Stąd badania, najpierw teoretyczne a następnie eksperymentalne, nad przesyłaniem informacji kwantowymi kanałami są konieczne. W proponowanych badaniach będziemy skupiać się na wpływie szum&oacute;w i nieporządk&oacute;w w kwantowych kanałach na przesyłaną informację, by potem m&oacute;c zaproponować efektywne kody do korekcji błęd&oacute;w kwantowych powstających podczas transmisji danych.</p>
<p>Przesyłanie informacji wiąże się z narażeniem ich na wpływ środowiska w kt&oacute;rym są przekazywane. Szum w kanale przesyłu informacji, będzie indukował błędy w przesyłanej informacji. Jednakże częstokroć nie mamy wpływu na środowisko w kt&oacute;rym informacja będzie przesyłana, dlatego też istotną rzeczą jest umiejętność niwelowania błed&oacute;w powstałych w trakcie transmisji. W tym celu używa się kod&oacute;w do korekcji błęd&oacute;w. &nbsp;Najprostszym przykładem wpływu środowiska na przekaz informacji może być pr&oacute;ba rozmowy podczas, gdy wok&oacute;ł jest głośno, np. jesteśmy w tłumie. Za tym przykładem idzie propozycja najprostszego kodu do korekcji - powtarzamy kilkukrotnie naszą wypowiedź, aż dotrze ona do adresata.&nbsp;Także kanały kwantowe nie są pozbawione szum&oacute;w &nbsp;i generują błędy kwantowe. Stąd jest potrzeba konstrukcji kod&oacute;w do korekcji kwantowych błęd&oacute;w [5, 6]. Proponowane badania są nakierowane na opracowanie nowych schemat&oacute;w do korekcji kwantowych błęd&oacute;w oraz na poprawienie już istniejących kod&oacute;w tego typu.</p>
<p>Nie wszystkie informacje mogą być przekazywane w formie og&oacute;lnodostępnej. Pojawia się zatem potrzeba szyfrowania i deszyfrowania przesyłanych wiadomości. Techniki kryptograficzne dzielą sie na dwa gł&oacute;wne nurty ze względu na dystrybucję klucza służącego do szyfrowania i odszyfrowywania informacji. &nbsp;Są to techniki wykorzystujące publiczny klucz [7] oraz techniki korzystające z prywatnego klucza. Pierwsze opierają swoje bezpieczeństwo o fakt istnienia trudnych do rozwiązania problem&oacute;w natury matematycznej, takich jak np. faktoryzacja dużych liczb całkowitych (metoda RSA) [8]. Bazują one na następującej zasadzie: odbiorca wiadomości publikuje otwarcie klucz do szyfrowania, tzn. każdy zna ten klucz (klucz publiczny). Z kluczem publicznym jest związany klucz sekretny, kt&oacute;ry znany jest jedynie odbiorcy. Związek między kluczem publicznym a prywatnym jest oparty na trudnym do rozwiązaniu zagadnieniu matematycznym, tzn. znając jedynie klucz publiczny odgadnięcie klucza sekretnego w skończonym czasie jest bardzo trudne. Nadawca wykorzystuje klucz publiczny do zaszyfrowania informacji, kt&oacute;rą następnie jedynie odbiorca, posiadający klucz sekretny, może odszyfrować.</p>
<p>Techniki korzystające z prywatnego klucza, wymagają natomiast komunikacji pomiędzy nadawcą i odbiorcą w celu wymiany prywatnego klucza służącego do kodowania i odkodowywania wiadomości. Jest to słaby punkt tych metod, gdyż daje możliwość przejęcia sekretnego klucza w taki spos&oacute;b, iż podsłuchiwan...",06/07/24,1,Kwantowo-mechaniczne systemy bezpiecznego przepływu informacji,"kryptografia kwantowa, kwantowa korekcja błędów",173
4,1235,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""BACKGROUND: white; MARGIN: 0cm 0cm 0pt 18pt; TEXT-INDENT: 18pt; TEXT-ALIGN: justify""><font size=""3""><font face=""Times New Roman""><strong><span style=""COLOR: black; mso-ansi-language: PL"">Ostatnie dekady XX wieku oraz początek nowego stulecia stanowiły okres burzliwego rozwoju r&oacute;żnego rodzaju informatycznych system&oacute;w wspomagania dowodzenia. Liczne tego rodzaju systemy zostały już wprowadzone na wyposażenie wiodących armii świata, kolejne - są w trakcie badań, opracowywania lub wdrażania. Dalsze perspektywy w tym zakresie, stwarzając jednocześnie nowe wymagania i możliwości, odsłoni prawdopodobnie wdrożenie w życie koncepcji działań sieciocentrycznych (Network Centric Warfare - NCW). Wspomniany proces nie ominął także Sił Zbrojnych RP. W minionych latach powstał szereg system&oacute;w wspomagania dowodzenia, adresowanych czy to do określonego poziomu działań, czy też do kon&shy;kretnego rodzaju wojsk (sił zbrojnych). Systemy te znajdują się na r&oacute;żnym poziomie rozwoju, począwszy od prac badawczo-rozwojowych, na praktycznym zastosowaniu skończywszy.</span></strong><strong><span style=""mso-ansi-language: PL""><o:p></o:p></span></strong></font></font></p>
<p class=""MsoNormal"" style=""BACKGROUND: white; MARGIN: 0cm 0cm 0pt 18pt; TEXT-INDENT: 21.25pt; TEXT-ALIGN: justify""><font size=""3""><font face=""Times New Roman""><strong><span style=""COLOR: black; mso-ansi-language: PL"">Nie ulega wątpliwości, iż wprowadzenie na szeroką skalę do wojsk tego rodzaju narzędzi wpłynie bezpośrednio i pośrednio na specyfiką sprawowania dowodzenia. Jednocześnie stwierdzić można, iż powstałe dotychczas opracowania o charakterze naukowym czy też popularno-naukowym ukierunkowane były gł&oacute;wnie na możliwości techniczne nowych zautomatyzowanych system&oacute;w dowodzenia. Stosunkowo rzadko natomiast pr&oacute;bowano ustalić rzeczywisty wpływ, jaki na system dowodzenia będzie miało upowszechnienie tego rodzaju system&oacute;w. Trudno jednocześnie nie dostrzegać, iż fakt istnienia r&oacute;żnych system&oacute;w, o r&oacute;żnych możliwościach i wynikające z tego niejednolite oddziaływanie na system dowodzenia, wart był zbadania metodami naukowymi.</span></strong><strong><span style=""mso-ansi-language: PL""><o:p></o:p></span></strong></font></font></p>
<p class=""MsoBodyTextIndent"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-INDENT: 22.7pt; TEXT-ALIGN: justify""><strong><span style=""mso-ansi-language: PL""><font size=""3""><font face=""Times New Roman"">Wsp&oacute;łczesność wskazuje, że coraz więcej zadań dla organ&oacute;w decyzyjnych wojsk lądowych będzie sytuowane poza granicami państwa w ramach zobowiązań politycznych. Jednocześnie będzie rozwijana sfera zadań na własnym terytorium w zakresie uczestnictwa w systemie reagowania kryzysowego. Nadal wymagają realizacji zadania w ramach Sił Pokojowych ONZ oraz UE. Jeżeli nastąpi jednoznaczne rozr&oacute;żnienie zadań pokojowych i kryzysowych o podłożu militarnym, to r&oacute;wnież operacje pokojowe ulegną ewolucji bądź w kierunku wymuszania pokoju, co powinno być domeną między innymi operacji reagowania kryzysowego lub w kierunku tradycyjnych zadań wynikających z rozdziału sz&oacute;stego karty narod&oacute;w zjednoczonych. Wsp&oacute;łczesne funkcjonowanie organ&oacute;w decyzyjnych wojsk lądowych wskazuje, że zbyt szybko rzeczywistość zaczyna wyprzedzać teorię, stąd konieczność na kanwie dotychczasowych doświadczeń i wiedzy oraz prognoz opracowania założeń funkcjonowania organ&oacute;w decyzyjnych wojsk lądowych w nowych uwarunkowaniach wsp&oacute;łczesnego świata, by w swych teoretycznych kierunkach rozwoju zapowiadały przyszłość, a nie ją stwierdzały. <o:p></o:p></font></font></span></strong></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-INDENT: 22.7pt; TEXT-ALIGN: justify""><strong><span style=""mso-ansi-language: PL""><font size=""3""><font face=""Times New Roman"">Problematyka funkcjonowania organ&oacute;w decyzyjnych wojsk lądowych w działaniach sojuszniczyc...",06/07/16,1,ORGANA DECYZYJNE WOJSK LĄDOWYCH W NOWYCH UWARUNKOWANIACH WSPŁCZESNEGO ŚWIATA,"Dowództwo Wojsk Lądowych, organa dowodzenia, proces doskonalenia organizacji",173
5,1275,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial"">Istniejące w chwili obecnej rozwiązanie wspomagające zwalczanie procederu przestępczego wykorzystywania fałszywych kart płatniczych oparte jest wyłącznie o bazę danych Interpolu. Baza ta zawiera informacje o fałszywych kartach płatniczych, kt&oacute;re można ustalić w wyniku odczytu danych zapisanych na karcie płatniczej. Jest to zbyt mało danych, aby dokonać analizy pozwalającej na ujawnienie producent&oacute;w tych kart.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial"">Dostęp do bazy Interpolu jest praktycznie niemożliwy dla organ&oacute;w ścigania z poziom&oacute;w wykonawczych. Z tego powodu nie są oni zainteresowani dostarczaniem do niej danych.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial"">Podejmowane w chwili obecnej badania fałszywych kart płatniczych prowadzone są wyłacznie w zakresie zawartości danych na ścieżce magnetycznej (bez badania jej parametr&oacute;w) oraz badania kart jako dokument&oacute;w (bez określania materiału, użytych technik, użytych narzędzi).<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial"">W trakcie kolejnych edycji międzynarodowej Konferencji nt.&rdquo;Przestępczość z wykorzystaniem elektronicznych instrument&oacute;w płatniczych&rdquo; stawiane było pytanie, kiedy powstanie baza, kt&oacute;ra zawierałaby pełne informacje o fałszywych kartach płatniczych. <o:p></o:p></span></p>",06/07/20,1,Projekt i wykonanie stanowiska badania fałszywych kart płatniczych,"carding, karta płatnicza, cyberprzestępczość, inforensics",173
6,1315,opisProjektuStanWiedzy,"&nbsp;
<div>Zagadnienia bezpiecznych sieci typu Internet/Intranet wchodzą w skład dyscypliny naukowej: Telekomunikacja, w kt&oacute;rej można wyr&oacute;żnić obszary tematyczne: bezpieczeństwo sieci telekomunikacyjnych oraz sieci typu Internet/Intranet.</div>
<div>Zagadnienia bezpiecznych sterownik&oacute;w i komputer&oacute;w wchodzą w skład dwu dyscyplin: Elektronika i Techniki Informacyjne.</div>
<div>Rozwiązania zadania budowy bezpiecznej sieci Internet/Intranet, jak r&oacute;wnież zadania budowy bezpiecznych sterownik&oacute;w i komputer&oacute;w nie są prezentowane w publicznie dostępnej literaturze światowej.</div>
<div>Można zakładać, że zadania te nie zostały rozwiązane.</div>
<div>Celem projektu jest opracowanie wstępnych rozwiązań obu w/w zadań i będzie to realizowane w oparciu o dwa założenia:</div>
<div>1)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span>bezpieczne sieci typu Internet/Intranet można zbudować jedynie poprzez opracowanie bezpiecznych urządzeń dla sieci komputerowych czyli urządzeń sieciowych o nowej konstrukcji, kt&oacute;ra uniemożliwi ukrywania i fałszowania tożsamości, czyli adresu IP, przez użytkownik&oacute;w sieci,</div>
<div>2)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span>bezpieczne sterowniki i komputery, czyli sterowniki i komputery odporne na włamania, kt&oacute;re muszą mieć inną konstrukcje i architekturę niż sterowniki i komputery stosowane w chwili obecnej.</div>
<div>&nbsp;</div>
<div>W świecie prowadzone są intensywne badania w zakresie bezpieczeństwa w sieciach typu Internet/Intranet. Prowadzone przeglądy literatury pokazują, że zagadnienia te są przedmiotem setek publikacji, przede wszystkim w prestiżowych czasopismach w zakresie techniki komputerowej i telekomunikacyjnej wydawanych przez IEEE i ACM, a także na licznych konferencjach naukowo-technicznych.</div>
<div>Przeprowadzona została analiza kilkudziesięciu artykuł&oacute;w z ostatnich lat dotyczących bezpieczeństwa w sieciach Internat/ Intranet. Wnioski z tej analizy są następujące.</div>
<div>1. W artykułach rozpatrywane są przede wszystkim możliwe rodzaje atak&oacute;w na sieć (gł&oacute;wnie ataki typu DDoS ( Distributed Denial of Service [9], [10]) oraz możliwości poprawy bezpieczeństwa w ramach istniejącego Internetu przez przeciwdziałanie tym atakom poprzez rozbudowę zakresu i sposobu działania ruter&oacute;w (gł&oacute;wnie analizowane są metody tzw. IP traceback &ndash; odszukania źr&oacute;dła ataku [11], [12]). <br />Końcowym wnioskiem większości artykuł&oacute;w jest stwierdzenie, że proponowane metoda daje pewną poprawę bezpieczeństwa, ale nie może zapewnić 100% skuteczności. W żadnej z analizowanych publikacji nie jest rozpatrywana koncepcja utworzenia oddzielnego bezpiecznego Internetu zbudowanego przy wykorzystaniu urządzeń sieciowych o nowej konstrukcji. </div>
<div>2. W żadnym z artykuł&oacute;w nie jest rozpatrywane zagadnienie konstrukcji bezpiecznego sterownika czy komputera o konstrukcji nie pozwalającej na włamania. <br /></div>
<div>Firma Microsoft zapowiada wprowadzenie bezpiecznego systemu operacyjnego VISTA - bezpieczeństwo ma być uzyskane przez nową konstrukcję systemu operacyjnego czyli oprogramowania. Doświadczenie pokazuje, że każde zabezpieczenie programowe może być przełamane. System VISTA jest to więc rozwiązanie częściowe, włamania do komputer&oacute;w przez &bdquo;hacker&rsquo;&oacute;w&rdquo; będą trudniejsze niż obecnie, ale nadal możliwe. <br />Całkowitą ochronę przed włamaniami można uzyskać jedynie przez nową konstrukcję sterownik&oacute;w i komputer&oacute;w. </div>
<div>&nbsp;</div>
<div>W ramach projektu opracowane zostaną dwa nowe, oryginalne rozwiązania:</div>
<div>- rozwiązanie problemu budowy bezpiecznej sieci typu Internetu/Intranet,</div>
<div>- rozwiązanie problemu budowy bezpiecznych sterownik&oacute;w i komputer&oacute;w.</div>
<div>Koncepcje i rozwiązania realizowane w ramach projektu nie są rozpatrywane w analizowanych publikacjach.</div>",06/07/31,1,Bezpieczne sieci typu Internet/Intranet oraz bezpieczne komputery i sterowniki do pracy w sieciach tego typu.,"Internet, Intranet, bezpieczeństwo, sieci komputerowe, komputery",173
7,1335,opisProjektuStanWiedzy,"&nbsp;
<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp; Dotychczas badania nad tego typu rozwiązaniami prowadzone są gł&oacute;wnie w Stanach Zjednoczonych. Opracowano tam system JDAM (Joint Direct Attack Munnition) służący do naprowadzania bomb lotniczych przy pomocy systemu opartego na połączeniu nawigacji GPS z układem nawigacji inercjalnej lub w oparciu jedynie o układ inercjalny. Brak jest natomiast krajowych badań na ten temat. Ponadto rozwiązania amerykańskie korzystają ze ster&oacute;w aerodynamicznych. Zastosowanie do sterowania bomb lotniczych ster&oacute;w gazodynamicznych będzie opracowaniem oryginalnym.</div>
<div align=""justify""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Szeroko rozwijane są w USA r&oacute;wnież programy dotyczące bezzałogowych pojazd&oacute;w naziemnych i robot&oacute;w pola walki. Ich sterowanie i nawigacja w przypadku platform do pracy autonomicznej są z reguły r&oacute;wnież oparte na układach GPS i INS. Jak na razie jeszcze tego typu urządzenia są w fazie pr&oacute;b. Następuje jednak bardzo szybki rozw&oacute;j tych konstrukcji w odpowiedzi na duże zapotrzebowanie armii amerykańskiej. Wiąże się to z rozwojem metod sieciocentrycznego zarządzania polem walki. Brak natomiast krajowych opracowań w tej dziedzinie dotyczących pojazd&oacute;w naziemnych.</span></div>",06/07/21,1,Autonomiczny system sterowania bomb lotniczych,"sterowanie autonomiczne, układy nawigacyjne",183
8,1415,opisProjektuStanWiedzy,"Na projekt składa się kilka powiązanych ze sobą obszar&oacute;w tematycznych: <br />1. Zagrożenia kleptograficzne i metody przeciwdziałania im, <br />2. Problem zachowania anonimowości w elektronicznym obrocie informacji, <br />3. Ochrona danych i uwierzytelnianie w protokołach sieciowych, <br />4. Teoretyczne aspekty kryptografii i kryptoanalizy. <br />Stan wiedzy dla nich om&oacute;wiony jest poniżej. <br /><br />
<div style=""TEXT-ALIGN: justify"">1. Możliwość przeprowadzenia efektywnych atak&oacute;w kleptograficznych została wykazana w serii prac Moti Yunga i Andrew Younga (por&oacute;wnaj np. [24]). Mimo dużego rozgłosu nie zostały dotychczas opublikowane skuteczne metody ochrony przed tego typu atakami, zaś projektowanie system&oacute;w kryptograficznych zazwyczaj nie bierze pod uwagę tych aspekt&oacute;w. Wyjątkiem jest schemat wybor&oacute;w elektronicznych Davida Chauma, kt&oacute;rego autor przypuszczalnie zdawał sobie sprawę z tych niebezpieczeństw, gdyż schemat ten jest prawie odporny na ataki kleptograficzne; niemniej jednak temat ten nie był dotychczas poruszany w dyskusji nad tym schematem. Częściowe wyniki dotyczące ochrony przed kleptografią zostały opublikowane w pracy [16]. W ramach ukończonego niedawno projektu (T00A 003 23) zesp&oacute;ł uzyskał już obiecujące rezultaty dotyczące zagrożeń atakami kleptograficznymi wykorzystywanych schemat&oacute;w elektronicznych. Owocem tego są dwie prace. Pierwsza Kleptographic attacks on e-voting schems, już opublikowana, porusza kwestię bezpieczeństwa niekt&oacute;rych schemat&oacute;w wybor&oacute;w elektronicznych. Druga, koncentrująca się na problemie procedur aukcji elektronicznych została złożona do publikacji. Prace te stanowić mogą jeden z punkt&oacute;w wyjścia proponowanych w projekcie badań. <br /></div>
<br />
<div style=""TEXT-ALIGN: justify"">2. Stan wiedzy w omawianej dziedzinie stanowi solidną bazę do dalszych badań jednak większości proponowanych rozwiązań brakuje albo podstaw teoretycznych, albo możliwości bezpośredniej implementacji (ze względu na koszty lub nierealistyczne założenia). Jako przykład pracy uwypuklającej jeden tylko z możliwych problem&oacute;w można podać [7]. Za początki prac w dziedzinie anonimowości można uznać m. in. prace [3, 4]. Pomimo, że w ostatnich latach pojawiło się wiele prac poświęconych tej tematyce, nadal jest to dział stosunkowo słabo zbadany, o czym świadczyć może choćby wielość r&oacute;żnorakich proponowanych definicji anonimowości. Punktem wyjścia proponowanych w tym temacie badań są wyniki już uzyskane przez nasz zesp&oacute;ł dotyczące szeroko pojętej anonimowości, w tym także prace związane z wyborami elektronicznymi, [7, 15, 9, 8]. <br /></div>
<br />
<div style=""TEXT-ALIGN: justify"">3. Dotychczasowy rozw&oacute;j badań nad sposobami uwierzytelniania skupiał się gł&oacute;wnie na problemie uwierzytelnienia <br />użytkownika względem usługi czy też urządzenia. Rzadziej analizowane były metody uwierzytelniania urządzeń wobec użytkownik&oacute;w. Zaproponowane dotychczas metody bazują z reguły na trudności pewnych problem&oacute;w algebraicznych, kombinatorycznych bądź obliczeniowych. Ich skuteczność wynika często r&oacute;wnież z założenia poprawności działania pewnych urządzeń. Bezpieczeństwo uwierzytelniania opiera się także na założeniu, że użytkownik zachowuje się zgodnie z protokołem. W projektowaniu powyższych metod &bdquo;czynnik ludzki&rdquo; często nie był brany pod uwagę. Większość z nich jest całkowicie bezradna w obliczu zainfekowania maszyny uczestnika protokołu. Rozwiązania stosowane obecnie w praktyce (systemy dostępu do system&oacute;w operacyjnych, systemy dostępu do rachunk&oacute;w bankowych, itp.) trudno nazwać bezpiecznymi. Z tego powodu coraz większym zagrożeniem jest zorganizowana przestępczość opierająca swą działalność na wykorzystaniu słabości system&oacute;w komputerowych. Ze względu na szerokie zastosowania, coraz większą wagę przywiązuje się do tworzenia &bdquo;lekkiej kryptografii&r...",06/07/27,1,Bezpieczeństwo informacyjne e-społeczeństwa,"kryptologia, struktury losowe, kryptografia, kleptografia, anonimowość, uwierzytelnianie",173
9,1455,opisProjektuStanWiedzy,"Już w 1961 roku w firmie Union Carbide zostały opracowane ogniwa tlenek srebra &ndash; cynk. Baterie, w kt&oacute;rych został użyty ten układ, znalazły zastosowanie w technice wojskowej. Przykładem są systemy naprowadzania rakiet lub elektrycznego napędzania torped. Baterie takie aktywowane są na odległość sygnałem radiowym, kt&oacute;ry powoduje przerwanie diafragmy i wtłoczenie pod ciśnieniem elektrolitu do strefy anodowo &ndash; katodowej. Baterie np. w torpedzie musiały dostarczać prądu o natężeniu około 500A i mocy 300kW w ciągu 10 minut. Miały one około 80kWh/kg. <br />Ogniwa tlenek srebra &ndash; cynk znalazły szerokie zastosowanie ze względu na doskonałe parametry pracy &ndash; wysoką energię, moc, niezawodność i bezpieczeństwo. Jednocześnie od wielu lat prowadzone są badania nad poprawą ich żywotności cyklicznej i odporności na wilgoć. Opracowano nowe rozwiązania konstrukcyjne, w kt&oacute;rych zastosowano nowe rodzaje separator&oacute;w [5], także w postaci membran [6]; wprowadzono do masy czynnej anody dodatki zapobiegające powstawaniu dendryt&oacute;w, a do elektrody srebrowej dodatki mające op&oacute;źniać proces jej starzenia. Udoskonalenia dotyczyły gł&oacute;wnie ogniw odwracalnych. W literaturze naukowej mało jest publikacji dotyczących badań nad poprawą pracy baterii rezerwowych. <br />W latach 70-tych prowadzone były badania w Centralnym Laboratorium Akumulator&oacute;w i Ogniw nad zastosowaniem rezerwowej baterii do zasilania pocisku artyleryjskiego. Skonstruowano baterie rezerwowe ampułowe o pojemności około 8,5 mAh i napięciu 12V przystosowane do kilkuminutowej pracy przy obciążeniu 100 mA. Baterie te, oparte były na układach elektrochemicznych &ndash; AgO &ndash; Zn i PbO2 &ndash; Pb. Ponadto w latach 90-tych została w CLAiO zaprojektowana i wdrożona do produkcji bateria BTR-03, będąca baterią rezerwową termiczną. Bateria ta, produkowana jest przez CLAiO z powodzeniem do dziś [7,8]. Obecnie w trakcie wdrożenia znajduje się inna bateria termiczna o symbolu BTR-05. <br />Obecny stan wiedzy pozwala stwierdzić, że baterie rezerwowe nadają się znakomicie tam, gdzie jest wymagana wysoka moc w bardzo kr&oacute;tkim czasie, a także tam gdzie wymagana jest bardzo szybka aktywacja rzędu kilku sekund. <br />Układ Ag2O &ndash; Zn stosowany jest w urządzeniach, w kt&oacute;rych potrzebne jest chemiczne źr&oacute;dło prądu charakteryzujące się wysoką mocą, wysoką energią, szybką aktywacją czy też pracą pod wodą. <br />Obserwuje się także duże zainteresowanie materiałami na separatory do akumulator&oacute;w. Zastąpienie dotychczas stosowanych separator&oacute;w hydrofilową membraną polipropylenową pozwala na wprowadzenie na rynek, źr&oacute;deł zasilania o znacznie zmniejszonych gabarytach, podwyższonej pojemności i mniejszym samowyładowaniu. Literatura fachowa zawiera jednak tylko wycinkowe informacje dotyczące zastosowania hydrofilowych membran polipropylenowych [9,10]. Są to dane niewystarczające. <br />",06/07/26,1,"Określenie wzajemnej korelacji między materiałem katodowym, separatorem i elektrolitem w długopracującej rezerwowej baterii niskoprądowej Ag2O-Zn do zastosowań wojskowych.","baterie/ogniwa rezerwowe, Zn/Ag2O, separatory",173
10,1475,opisProjektuStanWiedzy,"W Europie Środkowej znaczące zagrożenie dla samolot&oacute;w wojskowych stanowią: szpak Strunus vulgaris, mewy śmieszki Larus ridibundus, mewy srebrzyste Larus argentatus, jask&oacute;łki dym&oacute;wki Hirundo rustica, i okn&oacute;wki Delichon urbica, pustułki Falco tinnunculus, gawrony Corvus frugilegus, skowronki Alauda arvense, myszołowy Buteo buteo, niekt&oacute;re sowy Strigiformes, oraz czajki Vanelu vanelus. bociany Ciconia ciconia, gołebie domowe Columba livia (Jacoby 1998, Krupka 2000, Richardson &amp; West 2000). <br />Wskazane gatunki posiadają szereg cech ekologii, kt&oacute;re je predestynują do tego, by stanowić zagrożenie dla wojskowego czy też cywilnego ruchu lotniczego. Kolizje śmieszek, czajek i szpak&oacute;w ze statkami powietrznymi wypływają z faktu tworzeniu wieloosobniczych agregacji stad po zakończeniu okresu rozrodu preferujących przebywanie na otwartych terenach dla poszukiwania pokarmu na obszarze w pobliżu lotnisk. Pustułki i sowy uszate żerują r&oacute;wnież na otwartych terenach, a koszone powierzchnie trawiaste przy betonowych drogach startowych wydają się być optymalnym miejscem żerowiska. Jerzyki, jask&oacute;łki dym&oacute;wki, gawrony, poza gniazdowaniem kolonijnym, mogą r&oacute;wnież żerować w dużych zagęszczeniach. Niekt&oacute;re wskazane gatunki w wielu wypadkach gniazdują na otwartych terenach krajobrazu rolniczego (skowronki, czajki, myszołowy). Dla wojskowego ruchu lotniczego (cywilnego r&oacute;wnież) na obszarze Europy Środkowej znaczące zagrożenie stanowi śmieszka Larus ridibundus. Fakt żerowania na otwartych przestrzenia teren&oacute;w podmiejskich i na śmietniskach przynosi swoje negatywne konsekwencje (Jacoby 1998, Krupka 2000). <br />W Siłach Powietrznych Republiki Czeskiej w latach 1993-1999 śmieszki spowodowały 12 (12.2%) z 98 kolizji, w kt&oacute;rych ustalono gatunki ptak&oacute;w. Także w połowie lat osiemdziesiątych odnotowano na jednym z czeskich lotnisk wojskowych przypadek kolizji Miga 21 ze śmieszkami (Krupka 2000 ). <br />Mieszane stado czajek i szpak&oacute;w było sprawcami najbardziej brzemiennej w skutkach wojskowej katastrofy lotniczej ostatnich lat spowodowanej przez ptaki. Na lotnisku w Eindhoven (Holandia) 15 lipca 1996 samolot transportowy (C-130) Belgijskich Sił Powietrznych rozbił się z 34 ludźmi na pokładzie. R&oacute;wnież inny samolot Sił Powierznych Belgii (SF 200) zderzył się ze stadem czajek na lotnisku w Morsele (Richardson &amp; West 2000). Na początku lat 90. jedno zderzenie z czajkami miało miejsce w Siłach Powietrznych Republiki Czech (Krupka 2000). Poza wskazanymi przypadkami szpaki przyczyniły się do szeregu innych kolizji ze statkami powietrznymi innych państw. Przypadki zderzeń ze stadami rozważanych ptak&oacute;w w chwili startu znamy z Grecji (Richardson &amp; West 2000). Z lat 70. minionego stulecia znamy z obszaru byłego NRD dobrze udokumentowane zderzenia rosyjskich samolot&oacute;w wojskowych ( Mig-21 i SU-7) ze stadami szpak&oacute;w podczas wykonywania operacji powietrznych w poblizu lotniska (Jacoby 1998). W świetle danych przytaczanych z lat 1993-99 w Siłach Powietrznych Republiki Czech, stada szpak&oacute;w spowodowały 8 (8.2%) z 98 wypadk&oacute;w, w kt&oacute;rych ustalono gatunek ptaka. R&oacute;wnież, 13 września 1969 w wyniku zderzenia z nieokreślonym gatunkiem gęsi rozbił się samolot Tu-16 Floty P&oacute;łnocnej ZSRR w poblizu p&oacute;łwyspu Kola, grzebiąc 7 os&oacute;b. Z analiz wynika, że największe zagrożenie ze strony ptak&oacute;w ma miejsce we wrześniu. (Krupka 2000). <br />W krajach Europy Południowej notuje się zderzenia r&oacute;wnież z sępami. Problem dotyczy głownie lotnictwa Hiszpanii. Szczeg&oacute;lnie brzemienne w skutkach: śmierć pilot&oacute;w, jak i wysokie koszty strat przyniosły 2 zderzenia myśliwc&oacute;w (F-16A, i F-4E) z sępami plamistymi Gyps rueppelli w okolicach Bardenas w drugiej połowie lat 80. (Alonso F &ndash;inf ustna) <br />Na P&oacute;łwyspie Indyjskim najwięcej przypadk&oacute;w zderzeń s...",06/07/27,1,Zagrożenia i metody ochrony statków powietrznych przed zderzeniami z ptakami w aspekcie sytuacji ornitologicznej lotniska wojskowego w Dęblinie,"statki powietrzne, ptaki, zderzenia, lotnisko, bezpieczeństwo lotów",183
11,1536,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 1cm; TEXT-ALIGN: justify; tab-stops: list 0cm""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial"">Parki pontonowe typu wstęga (jak polski park PP-64) są wdrażane do sił zbrojnych świata od 50 lat. Aktualnie parki te często nie spełniają wymagań dotyczących nośności (STANAG 2021) a&nbsp;jednocześnie brak jest naukowych metod oceny ich rzeczywistej nośności przy uwzględnieniu ich aktualnego stanu technicznego.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 1cm; TEXT-ALIGN: justify; tab-stops: list 0cm""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt"">Dotychczasowe badania most&oacute;w pływających prowadzone w WAT dotyczyły gł&oacute;wnie statycznych analiz numerycznych takich konstrukcji [7 &ndash; 9, 12]. W ramach projektu przewiduje się wykonanie analiz kompletnej wstęgi PP-64 w zakresie dynamicznym. Analiza taka jest niezbędna do pełnej oceny wytężenia takiej złożonej konstrukcji, poddanej r&oacute;żnorodnym oddziaływaniom zewnętrznym (obciążenia od przeprawianych pojazd&oacute;w, oddziaływanie wody na most, kontakt poszczeg&oacute;lnych blok&oacute;w pontonowych). W tym celu zbudowane zostaną przestrzenne modele dyskretne pojedynczego bloku pontonowego wstęgi PP-64 oraz przestrzenne modele dyskretne kompletnej wstęgi o długości co najmniej 40m. Są już opracowane modele globalne mostu pływającego, uwzględniające budowę pojedynczego bloku pontonowego oraz występowanie połączeń z luzami montażowymi pomiędzy nimi. Do pełnego rozwiązania omawianego problemu, z uwzględnieniem nieliniowości geometrycznej spowodowanej obecnością więz&oacute;w jednostronnych, niezbędny jest model przestrzenny, posiadający powyżej 500000 stopni swobody [10, 11]. Zastosowanie wielopoziomowej metodyki analizy numerycznej, wykorzystywanej powszechnie w badaniach statycznych, jest niemożliwe w przypadku analiz dynamicznych mostu pływającego [1 &ndash; 4, 13 &ndash; 17]. Dodatkowo konieczne jest opracowanie modeli oddziaływania naporu wody na bloki pontonowe mostu. Opracowano już modele obciążeń zewnętrznych, pochodzących od r&oacute;żnego typu pojedynczych pojazd&oacute;w (kołowych, gąsienicowych). Wstępne analizy numeryczne w tak przygotowanych modelach dyskretnych wykonano dla r&oacute;żnych prędkości przepraw i r&oacute;żnych obciążeń zewnętrznych. Teoretycznie przebadano wpływ wielkości luz&oacute;w pomiędzy blokami pontonowymi na nośność mostu pływającego [12].<o:p></o:p></span></p>",06/07/28,1,Metodyka oceny rzeczywistej nośności mostów pontonowych z parku PP-64,"most pontonowy, analizy numeryczne, metoda elementów skończonych",183
12,1575,opisProjektuStanWiedzy,"W istniejących w świecie dokumentach normalizujących problematykę prowadzenia pirostatycznych badań doświadczalnych (m.in. STANAG 4115, MIL STD 286 (USA), TL 1386-600 (Niemcy), OESTAN 26.314 (Austria)) zaleca się realizację zapłonu stałych materiał&oacute;w miotających przy zastosowaniu możliwie małego (co do wartości) impulsu zapłonowego zakładając jednocześnie przy tym, że proces zapłonu prochu jest zgodny z geometrycznym modelem zapłonu i spalania. Także dotychczasowa praktyka realizacji tego typu badań w Polsce zakłada wytworzenie w komorze manometrycznej minimalnego poziomu ciśnienia gaz&oacute;w zapłonowych (zwykle z zapłonnika z prochu czarnego), umożliwiającego skuteczny (w założeniu zgodny z modelem geometrycznym) zapłon prochu badanego. <br />W klasycznym podejściu termodynamicznym w procesie analizy zjawisk balistyki wewnętrznej w prochowym układzie miotającym nie analizuje się fazy zapłonu, a wyniki teoretycznych analiz symulacyjnych pracy prochowych układ&oacute;w miotających z uwzględnieniem, jako danych wejściowych, wartości charakterystyk energetyczno-balistycznych proch&oacute;w otrzymanych z badań pirostatycznych &ldquo;dopasowuje&rdquo; się następnie do rzeczywistych wynik&oacute;w badań doświadczalnych poprzez zastosowanie odpowiednich wsp&oacute;łczynnik&oacute;w dopasowujących. Zdaniem wnioskodawcy wsp&oacute;łczynniki te odpowiednio dopasowują wyniki badań teoretycznych i doświadczalnych, ale tylko w stosunku do określonego układu miotającego i zastosowanego w nim prochu. <br />W modelu gazodynamicznym, w przeciwieństwie do modelu termodynamicznego, możliwa jest analiza fazy zapłonu. Wykorzystywany jest w tym celu cieplny model zapłonu J.H. Fraser&rsquo;a i B.L. Nichs&rsquo;a [15,19,21 &ndash; wykaz literatury str. 10-12], kt&oacute;ry pozwala r&oacute;wnież określić kryteria zapłonu ziarna ładunku miotającego w następującej postaci: <br />- temperatura na powierzchni ziarna powinna być większa od temperatury zapłonu; <br />- gradient temperatury w warstwie przypowierzchniowej ziarna powinien być r&oacute;wny gradientowi, jaki występuje podczas spalania ustalonego pod danym ciśnieniem co wiąże się z występowaniem określonego czasu, jaki upływa od chwili rozpoczęcia oddziaływania termicznego na ziarno do chwili zapłonu. <br /><br />Analiza teoretycznych modeli zapłonu w ujęciu gazodynamicznym często jest wspomagana metodami eksperymentalnymi, z kt&oacute;rych wykorzystuje się wyniki pomiar&oacute;w ciśnienia gaz&oacute;w <br />i propagacji fal spalania ziaren prochowych w p&oacute;łzamkniętych (z możliwym - dla określonych warunk&oacute;w spalania - wypływem gaz&oacute;w) komorach spalania. <br /><br />Efektem finalnym projektu badawczego będzie zatem opracowanie bardziej obiektywnej metodyki prowadzenia badań pirostatycznych ze szczeg&oacute;lnym uwzględnieniem fazy zapłonu prochu, kt&oacute;ra pozwoli na pozyskanie urealnionych wartości charakterystyk energetyczno-balistycznych proch&oacute;w, a tym samym większe urealnienie modelu matematycznego zjawiska strzału w ujęciu termodynamicznym. <br />",06/07/28,1,Badania pirostatyczne fazy zapłonu stałych materiałów miotajacych w aspekcie obiektywizacji wyznaczenia wiarygodnych wartości charakterystyk dynamicznych spalania,"spalanie prochów, zapłon, pirostatyka",183
13,1576,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 6pt 0cm 0pt; TEXT-INDENT: 36pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt"">Rozw&oacute;j materiał&oacute;w kompozytowych i kontrukcyjnych tworzyw adhezyjnych na przestrzeni ostatnich kilkunastu lat spowodował, że właściwości samych materiał&oacute;w i tworzyw są rozpoznane. Znane są r&oacute;wnież wiarygodne metody oceny ich właściwości mechanicznych. Wciąż nierozwiąany pozostaje problem obiektywnej oceny właściwości mechanicznych oraz prognozowania trwałości wezł&oacute;w konstrukcyjnych gdzie jednocześnie wykorzystywane są materiały metalowe i kompozytowe spajane konstrukcyjnymi tworzywami adhezyjnymi. W kraju znane są og&oacute;lne zalecenia dotyczące napraw wykonywanych z wykorzystaniem tworzyw adhezyjnych podawane przez ich producent&oacute;w, natomiast brak jest wytycznych opartych na rzetelnych i wiarygodnych badaniach, kt&oacute;re <span style=""mso-spacerun: yes"">&nbsp;</span>umożliwiją projektowanie napraw. W przypadku konstrukcji lotniczych, gdzie dodatkowo podczas projektowania należy koniecznie uwzględniać wysokie wymagania bezpieczeństwa trudno tworzyć dokumentację techniczną napraw tylko w oparciu o og&oacute;lne wymagania. Konieczne staje się posiadanie szerokiej wiedzy z zakresu wykorzystania właściwych materiał&oacute;w, technologii wykonywania napraw oraz zachowania się naprawianej struktury w warunkach zbliżonych do rzeczywistych (np. wpływ podwyższonej temperatury czy czynnik&oacute;w środowiskowych na trwałość konstrukcji).<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 6pt 0cm 0pt; TEXT-INDENT: 36pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt"">Na świecie prowadzone są badania związane z naprawami struktur lotniczych wojskowych statk&oacute;w powietrznych, natomiast nie są publikowane szczeg&oacute;łowe rozwiązania na podstawie, kt&oacute;rych można projektować naprawy ich struktur. Podawane są og&oacute;lne wskaz&oacute;wki, natomiast szczeg&oacute;łowe rozwiązania stanowią swoistą tajemnicę jednostek badawczych. Wyniki badań w postaci wyczerpujących informacji i wskaz&oacute;wek są udostępniane, i na bieżąco aktualizowane, tylko i wyłącznie użytkownikom system&oacute;w napraw.<o:p></o:p></span></p>",06/07/28,1,Analiza skuteczności napraw struktur wojskowych statków powietrznych w warunkach polowych z wykorzystaniem materiałów kompozytowych i tworzyw adhezyjnych,"struktury lotnicze, naprawy polowe, technologia napraw, połaczenia adhezyjne, kompozyty",173
14,1595,opisProjektuStanWiedzy,"Problematyka wysokiej odporności na obciążenia udarowe bądź ograniczania ich negatywnego oddziałwania, rozpatrywana jest gł&oacute;wnie w literaturze dotycząceń zawieszeń pojazd&oacute;w, zderzak&oacute;w wagon&oacute;w kolejowych, a także niekt&oacute;rych narzedzi o napędzie hydraulicznym. <br />Wsp&oacute;lną cechą występujących tam obciążeń udarowych jest ściśle zdefiniowany kierunek ich działania. Dzięki temu stosunkowo łatwe jest w takich przypadkach zastosowanie element&oacute;w podatnych lub sprężysto-tłumiących, pochłniających energię uderzenia lub ograniczających impuls uderzenia. <br />Złożony chrakter obciążeń udarowych, jaki występuje podczas pracy osprzętu torującego sprawia, że nie można ściśle zdefiniować kierunku ich działania. Jest to więc bardziej złożone zjawisko w stosunku do omawianych wcześniej zagadnień, kt&oacute;re trudno jest znaleźć w dosępnej literaturze, a szczeg&oacute;lnie trudnym wyzwaniem jest potrzeba ograniczenia masy osprzętu. <br />",06/07/31,1,Opracowanie wielozadaniowego systemu osprzętów inżynieryjnych o wysokiej odporności na obciążenia udarowe,"wielofunkcyjne osprzęty inżynieryjne, odporność udarowa, kinematyka osprzętów",173
15,1596,opisProjektuStanWiedzy,"&nbsp; Badania w zakresie rozpoznania mechanizm&oacute;w i określenia parametr&oacute;w detonacji w gazowych zawiesinach cząsteczek i kropel palnych prowadzone są od połowy lat sześćdziesiątych ubiegłego wieku. Podstawowy zas&oacute;b wiedzy pochodzi z badań eksperymentalnych prowadzonych w rurach uderzeniowych. Jedną z pierwszych rejestracji detonacji w zawiesinie kropel paliwa węglowodorowego w tlenie przedstawiono w pracy [4]. Rozpatrywano krople o średnicy 2600 mm, przy czym wg przeprowadzonych oszacowań około 20 % energii wydzielonej w strefie reakcji tracone jest do ścianek urządzenia.&nbsp;Niedogodność ta w większej lub mniejszej mierze dotyczy&nbsp;wszystkich badań prowadzonych w układach o symetrii płaskiej (rurach uderzeniowych). Jedyne znane badania detonacji w zawiesinie kroplowo-gazowej w układzie o symetrii cylindrycznej przeprowadzone zostały na Uniwersytecie w Ann Arbor [5, 6]. Układ obejmował wycinek cylindra o kącie rozwarcia 20<span>&deg;</span>. Wysokość układu wynosiła 5,08 cm, promień 137,67 cm. Krople wytwarzane były przez układ 147 igieł. Dla kropel o benzyny średnicy 380 mm w powietrzu na odległości 60 cm uzyskano falę o <em>M</em> = 4,3 kt&oacute;ra powyżej promienia 90 cm przechodziła w falę <em>M</em> = 3,2. Dla kropel benzyny w atmosferze tlenowej uzyskano stacjonarną predkość detonacji 1600 m/s, wobec wynikającej z bilansu energetycznego wartośći 1876 m/s. Tak więc, r&oacute;wnież w tym przypadku &nbsp;wpływ na obniżenie rejestrowanej prędkości detonacji miały straty energii do g&oacute;rnej i dolnej powierzchni układu pomiarowego (w pracach [5, 6] nie oceniano strat energii do ścianek układu).
<div>&nbsp;W układach praktycznych, w przypadku wybuchu zamierzonego lub niekontrolowanego (w obłoku zwiesinowym powstałym np. w wyniku awarii) inicjacja wybuchu rozwija się jako proces przestrzenny. Przy intensywnym źr&oacute;dle inicjującym proces zachowuje symetrię sferyczną. Badania rozwoju sferycznej detonacji w obłoku zawiesinowym prowadzone były teoretycznie [7 &ndash; 9].</div>
<div>Ośodek zwiesinowy opisywany był modelem ośrodka dwufazowego, rozw&oacute;j procesu śledzono za pomocą symulacji numerycznej. Należy zwr&oacute;cić uwagę na duże uproszczenia &nbsp;modelowe jakie były przyjmowane w pracach [7 &ndash; 9]. W szczeg&oacute;lności, w prowadzonych obliczeniach przyjmowano, że paliwo (mikrokrople) oderwane od kropel wyjściwoych ulega natychmiastowemu przereagowaniu. Stosowano r&oacute;wnież bardzo uproszczone r&oacute;wnie stanu, r&oacute;wnanie stanu gazu idealnego o stałym (jednakowym) wykładniku politropy dla ośrodka wyjściowego (powietrze, tlen) oraz produkt&oacute;w spalania.</div>
<div>Uproszczenia te nie zostały po dzień dzisiejszy przezwyciężone. W pracy [10], gdzie oceniana jest krytyczna energia inicjacji detonacji w zawiesinie kroplowo-gazowej przedstawiane zależności wyprowadzane są przy założeniu o natychmistowym przereagowaniu paliwa oddzielonego od podlegającej fragmetacji kropli (por. wiersz 10 od g&oacute;ry, str. 29 w pracy [10]).</div>
<div>W stosunku do prac zamieszczonych w opublikowanej dotychczas literaturze, model będący podstawą przedstawianego projektu stanowi istotny postęp. Można wymienić trzy elementy modelu, kt&oacute;re stanowią o jego istotnym rozszerzeniu w stosunku do prezentowanych dotychczas:</div>
<div>(1) uwzględniana jest obecność w zawiesinie mikrokropel (określanych jako faza &bdquo;Z&rdquo;) &nbsp;powstałych w wyniku fragmentacji kropel wyjściowych. Krople fazy &bdquo;Z&rdquo; cechują się własnym tempem przemiany, wynikającym z szybkości ich odparowania. </div>
<div>(2) uwzględniania jest kinetyka przereagowania odparowanego paliwa z tlenem zawartym w atmosferze gazowej. Stosowany jest&nbsp;jedno- lub dwu-stopniowy schemat kinetyczny (wg [11]). </div>
<div>(3) Faza gazowa opisywana jest jako ośrodek wieloskładnikowy. Skład fazy gazowej zmienia się wraz z rozwojem procesu przereagowania paliwa z tlenem (umożliwia to poprawne zastosowanie schemt&oa...",06/07/28,1,Modelowanie rozwoju spalania i wybuchu w niejednorodnych fizycznie ośrodkach reaktywnych,"heterogeniczne mieszaniny wybuchowe, przepływy dwufazowe, spalanie, wybuch, detonacja",183
16,1636,opisProjektuStanWiedzy,"&nbsp;
<div>Proponowane badania należą do jednego z podstawowych kierunk&oacute;w badań nad metodami wykrywania substancji zapachowych, prowadzonych r&oacute;wnolegle w wielu ośrodkach naukowych na świecie. Efektem badań będzie uzyskanie wzrostu czułości i selektywności wykrywanych gaz&oacute;w jedynie za pomocą nowatorskiej metody pomiarowej, w oparciu o dostępne, obecnie wykorzystywane czujniki gazu. Wyniki dotychczasowych badań autor&oacute;w projektu wskazują jednoznacznie na możliwość poprawy detekcji r&oacute;żnych gaz&oacute;w za pomocą analizy fluktuacji rezystancji czujnik&oacute;w gazu. Podobne wyniki uzyskano dla kilku typ&oacute;w przebadanych sensor&oacute;w gazu.</div>
<div>Dotychczas brak jest szczeg&oacute;łowych badań eskperymentalnych pozwalających sformułować optymalne kryteria detekcji r&oacute;żnych gaz&oacute;w za pomocą analizy fluktuacji rezystancji stosowanych czujnik&oacute;w gazu w praktycznych sytuacjach, gdy identyfikowana jest mieszanina gaz&oacute;w o r&oacute;żnej wilgotności oraz przy r&oacute;żnej temperaturze otoczenia. Na podstawie wstępnych wynik&oacute;w wiadomo, że zmianie ulegają takie wielkości opisujące fluktuacje rezystancji jak gęstość widmowa mocy, gęstości prawdopodobieństwa, spektra wyższych rzęd&oacute;w.</div>
<div>Opracowanie metody detekcji gaz&oacute;w na podstawie właściwości statystycznych badanych fluktuacji rezystancji czujnik&oacute;w gazu będzie stanowiło, oryginalny wkład w rozw&oacute;j dyscypliny naukowej, w kt&oacute;rej zawiera się tematyka projektu badawczego. Zaletą proponowanej metody jest wzrost selektywności oraz obniżenie progu detekcji r&oacute;żnych gaz&oacute;w przy użyciu mniejszej liczby sensor&oacute;w w warunkach pomiarowych odpowiadających rzeczywistym sytuacjom w kt&oacute;rych występuje potrzeba identyfikacji mieszanin gaz&oacute;w. Pozwoli to na upowszechnienie stosowania diagnostyki mieszanin gaz&oacute;w za pomocą przenośnych urządzeń według proponowanej metody.</div>",06/08/22,1,Badania wpływu warunków środowiskowych na czułość i selektywność detekcji gazów metodą z wykorzystaniem zjawisk fluktuacyjnych czujników,"sensory gazu, nose elektroniczny, szumy",173
17,1675,opisProjektuStanWiedzy,"<p align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Problematyka metod energetycznych w zakresie modelowania parametr&oacute;w silnik&oacute;w spalinowych realizowana była w Polsce pod kierunkiem płk prof. Stefana Szczecińskiego, wybitnego specjalisty z zakresu silnik&oacute;w tłokowych i turbinowych. W jego zespole (do kt&oacute;rego zaliczają się autorzy niniejszego projektu) opracowano skuteczne, zweryfikowane modele w zakresie badań numerycznych procesu wymiany ładunku i filtracji powietrza wlotowego. Wyniki tych badań zastosowano do modernizacji układ&oacute;w zasilania samochod&oacute;w FIAT126 i 125p, samolotu PZL Gawron czy filtracji powietrza wlotowego samolot&oacute;w bojowych. Opracowane w tym zespole eksperymentalno-obliczeniowe modele procesu wymiany ładunku (w tym model opracowany przez wykonawcę projektu [13], oryginalne w skali światowej, umożliwiają oceną efektywności napełnienia silnika i jakości powietrza wlotowego, ocenę energii spalin i możliwości turbosprężarki oraz parametr&oacute;w związanych z wymianą ciepła. Pr&oacute;ba implementacji tych modeli do oceny możliwości poprawy parametr&oacute;w efektywnych silnika UTD-20 jest jedną z istotnych części proponowanego projektu. </p>
<p align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp; Innym elementem projektu jest wyposażenie BWP-1 w elementy diagnostyki pokładowej i przystosowanie pojazdu do wsp&oacute;łpracy z Komputerowym Zestawem Diagnostycznym (opracowanym w zespole wnioskodawcy) znajdującym się na wyposażeniu Batalion&oacute;w Remontowych. Jest to szczeg&oacute;lnie istotne ze względu na fakt, że istnieje efektywne oprogramowanie diagnostyczne silnika UTD-20, zainstalowane w tym zestawie. Elementy diagnostyki pokładowej wykonane będą w standardzie EOBD &ndash; obowiązującym w Europie dla silnik&oacute;w o ZS od 2006 roku &ndash; i zorientowanym na elementy emisyjnie krytyczne, kt&oacute;rych zmiana stanu technicznego może powodować wzrost udziału toksycznych składnik&oacute;w spalin powyżej dopuszczalnego normą poziomu. Wymienione wyżej zagadnienia są nowymi w skali kraju, a niekt&oacute;rymi parametrami przewyższają opracowania zagraniczne. Jako przykład można podać wyposażenie diagnostyczne pojazdu Hammer, ustępujące możliwościami proponowanemu w projekcie wyposażeniu BWP w tym zakresie. </p>
<p align=""justify""><br /></p>",06/08/08,1,Analiza możliwości poprawy parametrów efektywnych i trwałości  zespołu napędowego bojowego wozu piechoty BWP-1,"pojazdy terenowe, silnik spalinowy, układy silnika, trwałość silnika, skojarzenie tłok -pierścienie tłokowe - tulej cylindrowa, mikroobróbka laserowa",183
18,1695,opisProjektuStanWiedzy,"<p align=""justify"">Pociski amunicji fragmentującej produkowane są zazwyczaj z materiał&oacute;w kompozytowych, kt&oacute;re składają się z materiału-wypełniacza, najczęściej o dużej gęstości, oraz z materiału wiążącego, zapewniającego sp&oacute;jność pociskowi zar&oacute;wno w przewodzie lufy jak i na trajektorii lotu. Wyr&oacute;żnia się kilka metod wytwarzania pocisk&oacute;w fragmentujących. Do najbardziej rozpowszechnionej należy zaliczyć technologię metalurgii proszk&oacute;w, kt&oacute;ra pozwala otrzymać materiał kompozytowy o właściwościach fizycznych podobnych do materiał&oacute;w stosowanych w amunicji tradycyjnej. Technologia ta polega og&oacute;lnie na prasowaniu w matrycy stalowej mieszanki proszkowej, kt&oacute;rej skład chemiczny jest tak dobrany, aby podczas kolejnej operacji procesu technologicznego, jakim jest spiekanie, otrzymać fazę kruchą, kt&oacute;ra zapewnia materiałowi pocisku wysoką zdolność do fragmentacji po uderzeniu w tarczę. W ten spos&oacute;b na przykład produkuje się pociski fragmentujące z materiału spiekanego Cu-Sn. Inną często stosowaną technologią wytwarzania pocisk&oacute;w fragmentujących jest metoda wtryskowego formowania proszk&oacute;w. Polega ona na tym, że pocisk kształtowany jest na drodze wtryskiwania mieszanki proszkowo-polimerowej do specjalnie przygotowanej formy. Tworzywo sztuczne pełni rolę nie tylko lepiszcza zapewniającego odpowiednią wytrzymałość materiału kompozytowego, ale także jest fazą zapewniającą prawidłowy przebieg wypełnienia przestrzeni wtryskowej, a co za tym idzie decyduje o poprawności geometrycznej otrzymanego wyrobu. Tą technologią na przykład produkowane są pociski z materiału składającego się z proszku miedzi i nylonu. <br />Tradycyjne sposoby wytwarzania spiek&oacute;w ceramiczno-metalowych polegają na przygotowaniu mieszanki proszk&oacute;w bazowych, składającej się z metalicznego proszku osnowy i proszku fazy ceramicznej. Następnie formowanie kształtu gotowego wyrobu i spiekania w fazie stałej, bądź z udziałem fazy ciekłej w atmosferze ochronnej, aż do uzyskania połączenia dyfuzyjnego. W końcowym etapie często stosuje się obr&oacute;bki wykańczające takie jak: obr&oacute;bki cieplne, nasycanie olejami, żywicami lub niżej topliwymi metalami. Taka technologia jest z powodzeniem wykorzystywana w produkcji materiał&oacute;w ceramiczno-metalowych i węglik&oacute;w spiekanych. Większość takich wyrob&oacute;w charakteryzuje się z dużą kruchością, kt&oacute;rą w przypadku węglik&oacute;w można zmniejszyć poprzez infiltrację metalami. Badania technologiczne wykazały, że zmniejszenie kruchości można osiągnąć stosując fazy ceramiczne o wysokiej dyspersji. Wytwarzane poprzez mechaniczne rozdrabnianie fazy ceramiczne wykazują wielkość cząstek w zakresie od kilkunastu do kilkudziesięciu &micro;m, istnieją sposoby wytwarzania faz drobniejszych, lecz w tym przypadku istnieje silna tendencja do zbijania się cząstek fazy ceramicznej w konglomeraty wielu cząstek. Utrudnia to proces mieszania i podwyższa ryzyko niejednorodnego rozkładu. Sposobem na przezwyciężenie tych trudności jest wytwarzanie faz ceramicznych bezpośrednio wewnątrz spieku podczas procesu spiekania, nazywanego w tym przypadku spiekaniem reakcyjnym. <br /></p>",06/07/19,1,Badania technologiczno-balistyczne nad wytwarzaniem fragmentujących pocisków broni strzeleckiej,amunicja pociski spiekanie,173
19,1696,opisProjektuStanWiedzy,"Budowa siatek obliczeniowych jest zadaniem bardzo czasochłonnym i skomplikowanym. Analizując dostępna literaturę można znaleźć jedynie bardzo og&oacute;lne wytyczne w kwestii generacji siatek obliczeniowych. Wytyczne te sprawdzają się jedynie w przypadku prostych zagadnień. Budowa właściwej siatki dla określonych , skomplikowanych zagadnień jest bardzo trudne. Poza tym sam proces generacji poprawnej siatki jest zagadnieniem bardzo czasochłonnym, z reguły kilkukrotnie dłuższym niż czas rozwiązania sformułowanego zagadnienia. Dlatego zasadnym jest pr&oacute;ba opracowania metodyki budowy siatek dla bardziej złozonych obiekt&oacute;w. <br />Obecnie na świecie prowadzi się prace mające na celu rozwijanie metod DNS (Direct Numerical Simulation) oraz LES (Large Eddy Simulation). Jednakże ze wzgędu na ograniczenia mocy obliczeniowych dostępnego dziś sprzętu komputerowego niemożliwe jest (jeszcze przez dłuższy czas nie będzie) wykonanie obliczeń aerodynamicznych złożonego obiektu z wykorzystaniem DNS. Metoda LES natomiast nie sprawdza się w przypadkach obliczeń z przejściem lamiarno-turbulentnym, z czym często spotykamy się w aerodynamice. Z tego względu nadal prowadzone są intensywne prace mające na celu rozwianie i doskonalenie metod opartych o RANS. Przykładami mogą być zakończony niedawno europejski projekt badawczy QNET-CFD poświęcony w całości ocenie przydatności kod&oacute;w obliczeniowych, kt&oacute;ry opr&oacute;cz pewnych ustaleń w zakresie stosowania takich metod wykazał potrzebę dalszych prac oraz uruchomiony w 2005 roku projekt WALLTURB poświęcony zagadnieniom modelowania turbulencji. <br />W celu przyspieszenia długotrwałych obliczeń numerycznych zaczęto stosować paralelizację algorytmu obliczeniowego. Początkowo wykorzystywano do obliczeń tego typu specjalnie do tego celu budowanych superkomputerach wieloprocesorowych, gdzie na jednej płycie gł&oacute;wnej jest wiele procesor&oacute;w. Jednak ze względu na olbrzymi koszt budowy takiego komputera i praktycznie brak możliwości jego rozbudowy zaczęto stosować klastry obliczeniowe, czyli łączone ze sobą bardzo szybkimi sieciami komputerowymi jedno, dwu lub czteroprocesorowych maszyny robocze. Koszt budowy takiego klastra jest wielokrotnie mniejszy od kosztu superkomputera przy zachowaniu zbliżonej wydajności. W związku z powyższym na świecie wraz z poszukiwaniem i udoskonalaniem metod obliczeniowych poszukuje się najefektywniejszych metod paralelizacji obliczeń. <br />",06/07/31,1,Numeryczna analiza aerodynamiki złożonych obiektów metodą objętości skończonych,"badania numeryczne, turbulencja, modele turbulencji, siatki obliczeniowe, obliczenia równoległe",183
20,1735,opisProjektuStanWiedzy,"3. Istniejący stan wiedzy w zakresie tematu badań (jaki oryginalny wkład wniesie rozwiązanie postawionego problemu do dorobku danej dyscypliny naukowej w kraju i na świecie, czy w kraju i na świecie jest to problem nowy czy kontynuowany i w jakim zakresie weryfikuje utarte poglądy i dotychczasowy stan wiedzy) <br /><br />Rozw&oacute;j nowoczesnych system&oacute;w uzbrojenia i sprzętu wojskowego, ich żywotność i niezawodność, a także eksploatacja, logistyka i likwidacja są przedmiotem badań naukowych, kt&oacute;rych wyniki wnoszą określoną wartość dla gospodarki narodowej. <br />Według polskiej normy PN-82/N-4001, eksploatacja to zesp&oacute;ł celowych działań organizacyjno-technicznych i ekonomicznych ludzi z obiektem technicznym oraz wzajemne relacje, występujące pomiędzy nimi od chwili przejęcia obiektu do wykorzystania zgodnie z przeznaczeniem aż do jego likwidacji. Eksploatacja techniki w SZ RP realizowana jest w systemie planowo &ndash; zapobiegawczym i w systemie według stanu technicznego. System planowo &ndash; zapobiegawczy polega na obowiązkowym wykonaniu określonych czynności obsługowych na obiektach technicznych w ściśle określonych okresach podczas ich użytkowania, przechowywania, obsługiwania technicznego i remontu. Planowo-zapobiegawczy system obsługiwania technicznego i remontu jest wykorzystywany w procesie eksploatacji złożonej techniki starszej generacji o małej podatności diagnostycznej oraz techniki wojskowej, od kt&oacute;rej wymaga się dużej niezawodności użytkowania. Odmianą powyższego systemu jest system planowo-zapobiegawczy z diagnozowaniem. Jego ideą jest wykonywanie badań diagnostycznych przed każdym obsługiwaniem i remontem, a celem gł&oacute;wnym polepszenie jakości obsługiwania technicznego i remontu, a w efekcie podwyższenie gotowości technicznej i niezawodności obiekt&oacute;w technicznych. <br />Najważniejszym problemem jest optymalizacja tych proces&oacute;w z punktu widzenia realizacji celu, z uwzględnieniem wymog&oacute;w zdolności i gotowości bojowej Sił Zbrojnych RP oraz zasady racjonalnego gospodarowania, a także ocena systemu eksploatacji uzbrojenia i sprzętu wojskowego w relacji koszt - efekt. Obecny system obrony narodowej powinien dysponować informacjami, kt&oacute;re pozwalają na kompleksową ocenę systemu eksploatacji ( jego skuteczności obronnej i efektywności ekonomicznej). Dow&oacute;dcy wszystkich szczebli dowodzenia powinni dysponować nie tylko danymi dotyczącymi koszt&oacute;w jednostkowych, ale także koszt&oacute;w system&oacute;w i podsystem&oacute;w eksploatacji z uwzględnieniem podział&oacute;w klasyfikacyjnych. <br />Na łamach wojskowych wydawnictw naukowych i popularno &ndash; naukowych brak jest doniesień o istnieniu opracowania teoretycznego na temat szacowania pełnych koszt&oacute;w system&oacute;w uzbrojenia. Istnieje natomiast szereg materiał&oacute;w dotyczących poszczeg&oacute;lnych segment&oacute;w teorii szacowania koszt&oacute;w, tj.: koszt&oacute;w zakupu, eksploatacji i logistyki oraz koszt&oacute;w likwidacji, a także opracowań ekonomicznych o prognozowaniu i szacowaniu koszt&oacute;w. Autorzy opracowań i publikacji ( teoretycy i praktycy ) r&oacute;żnie definiują i interpretują obszar pojęciowy terminu &bdquo; pełne koszty system&oacute;w uzbrojenia&rdquo;. Najczęściej zawężają je do koszt&oacute;w zakupu i eksploatacji pomijając aspekt wycofania z użytku i likwidacji tych system&oacute;w. <br />Przykładami publikacji ekonomicznych przydatnych w opracowaniu metodyki szacowania pełnych koszt&oacute;w, kt&oacute;re zesp&oacute;ł autorski zamierza szerzej wykorzystać są następujące publikacje: <br />Deakin E. B., Maher M. W.: Cost Accounting. Książka stanowi kompendium wiedzy z zakresu rachunku koszt&oacute;w. Składa się z czterech części i 26 rozdział&oacute;w. Zawiera 1036 stron. Dla potrzeb pracy badawczej szczeg&oacute;lnie cenne bedą rozdziały: 5, 6, 10, 19. W rozdziale 5 i 6 omawiane są dwa systemy rachunku koszt&oacute;w: rachunek koszt&oacute;w jednost...",06/08/09,1,Metodyka szacowania pełnych kosztów uzbrojenia,"LCC, całkowity koszt życia, metodyka, systemy uzbrojenia",173
21,1815,opisProjektuStanWiedzy,"&nbsp;
<div>Problemy zasilania silnik&oacute;w pojazd&oacute;w wojskowych narastały wraz ze wzrostem liczby pojazd&oacute;w w armiach. Już podczas drugiej wojny światowej wiele operacji kończyło się niepowodzeniem ze względu na brak paliwa. Stracono też wiele pojazd&oacute;w z tego powodu. Dlatego po drugiej wojnie zaczęto rozwijać silniki wielopaliwowe, kt&oacute;re mogły być zasilane r&oacute;żnego rodzaju paliwami płynnymi, od benzyny niskooktanowej do oleju napędowego. Działania wietnamskie, w kt&oacute;rych wojska lądowe korzystały z dużej liczby śmigłowc&oacute;w i samolot&oacute;w, spowodowały narastanie problem&oacute;w logistycznych z dostawani r&oacute;żnego rodzaju paliw. W efekcie zdecydowano się ujednolicić paliwa do silnik&oacute;w tłokowych i turbinowych pojazd&oacute;w mechanicznych oraz samolot&oacute;w stacjonujących na lądzie. Wprowadzono paliwo oznaczone symbolem F34/35, kt&oacute;re ma identyczną bazę jak paliwo lotnicze JP8, a jego właściwości końcowe wynikają <br />z dodatk&oacute;w wprowadzanych przed wlaniem paliwa do zbiornika samochodu. Jedynie marynarka wojenna stosuje inne paliwo oznaczone symbolem F75. Jednolite paliwo jest obecnie stosowane we wszystkich operacjach NATO.</div>
<div>Od czasu prac prowadzonych nad wdrożeniem tego paliwa zmienił się osprzęt silnik&oacute;w i zamiast tłoczkowych pomp wtryskowych obecnie stosowane są systemy wtrysku wysokociśnieniowego Common Rail i w dużo mniejszym stopniu pompowtryskiwacze. W układach tych ciśnienie paliwa jest podnoszone do 140...200 MPa i takie ciśnienie jest utrzymywane przez cały czas pracy. Zmienia to istotnie warunki termiczne paliwa przed wtryskiem do komory spalania, a jego temperatura jest dużo wyższa w por&oacute;wnaniu z temperaturą paliwa w pompie tłoczkowej. Z problemem tym spotkali się wykonawcy projektu podczas budowy stanowiska z układem CR. Zastosowanie przepływomierza o jednym kierunku przepływu paliwa spowodowało konieczność chłodzenia paliwa krążącego na stanowisku, czego nie było w starszych silnikach. <br />W pojeździe nadmiar paliwa przepływa do zbiornika, a ciągłe jego krążenie powoduje ochładzanie paliwa w przewodach paliwowych i zbiorniku. Jakkolwiek modelowe badania trwałościowe pomp wtryskowych przy wykorzystaniu ograniczonej ilości paliwa wskazują na dużą stabilność cieplną oleju napędowego, to jednak biokomponenty organiczne zawierające tlen mogą ulegać przemianom w podwyższonej temperaturze, a skala tych przemian wymaga dokładniejszych badań. </div>
<table cellspacing=""0"" cellpadding=""0"" border=""0"">
    <tbody>
        <tr>
            <td valign=""top"" width=""305"">
            <div>a)<img style=""WIDTH: 288px; HEIGHT: 149px"" height=""137"" width=""215"" alt="""" src=""/OSFImageLoader.do?idImageDB=308"" /></div>
            </td>
            <td valign=""top"" width=""300"">
            <div>b)<img height=""151"" width=""280"" alt="""" src=""/OSFImageLoader.do?idImageDB=309"" /></div>
            </td>
        </tr>
        <tr>
            <td valign=""top"" width=""305"">
            <div>c)<img style=""WIDTH: 277px; HEIGHT: 159px"" height=""137"" width=""277"" alt="""" src=""/OSFImageLoader.do?idImageDB=310"" /></div>
            </td>
            <td valign=""top"" width=""300"">
            <div>d)<img style=""WIDTH: 272px; HEIGHT: 162px"" height=""131"" width=""272"" alt="""" src=""/OSFImageLoader.do?idImageDB=311"" /></div>
            </td>
        </tr>
        <tr>
            <td valign=""top"" width=""605"" colspan=""2"">
            <div align=""center"">Rys. 2. <a name=""OLE_LINK1"" target=_blank>Charakterystyki prędkościowe silnika G9T: a) moment obrotowy, b) temperatura spalin, <br />c) emisja tlenk&oacute;w azotu NO<sub>X</sub>,</a> d) jednostkowe zużycie paliwa</div>
            </td>
        </tr>
    </tbody>
</table>
<div>&nbsp;</div>
<div>Już zastosowanie samego paliwa F34 do silnika z układem CR może spowodować istotne zmiany parametr&oacute;w pracy silnika. Rozpoznawcze badania własne silnika G9T z układem CR wykazały wyraźne...",06/08/16,1,Określenie wpływu jednolitego paliwa F-34/35 z biokomponentami na pracę wysokociśieniowego układu zasilania typu Common Rail,"paliwo zastępcze, silnik spalinowy, emisja spalin",183
22,1835,opisProjektuStanWiedzy,"<p align=""left""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Chemiczna analiza śladowych ilości środk&oacute;w wybuchowych nie jest zagadnieniem nowym. Pierwsze metody stosowane do wykrywania środk&oacute;w wybuchowych bazowały na stosowaniu odczynnik&oacute;w dających barwne reakcje z analitami (ang. spot tests) [3]. Obecnie do tego celu stosuje się techniki analizy instrumentalnej, w szczeg&oacute;lności metody chromatograficzne. <br />&nbsp;&nbsp;&nbsp;&nbsp; Analizę ślad&oacute;w środk&oacute;w wybuchowych prowadzi się nie tylko dla potrzeb wymiaru sprawiedliwości. Toksyczność niekt&oacute;rych środk&oacute;w wybuchowych i produkt&oacute;w ich rozkładu spowodowała, że stały się one r&oacute;wnież obiektem zainteresowania agencji zajmujących się monitoringiem stanu środowiska [13,14].&nbsp;</font><font face=""Times New Roman"" size=""3""><br /><strong>&nbsp;&nbsp;&nbsp;&nbsp; </strong></font></p>
<p align=""left""><font face=""Times New Roman"" size=""3""><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analiza związk&oacute;w organicznych</strong> <br />&nbsp;&nbsp;&nbsp;&nbsp; Wśr&oacute;d związk&oacute;w organicznych mogących wchodzić w skład środk&oacute;w wybuchowych wyr&oacute;żnia się m.in. związki nitroaromatyczne (np. TNT, tetryl), estry kwasu azotowego (nitrogliceryna, EGDN, PETN), nitroaminy (np. RDX, HMX) i nadtlenki (np. HMTD, TATP) [21]. Metodą uznawaną za najbardziej odpowiednią do analizy organicznych środk&oacute;w wybuchowych jest wysokosprawna chromatografia cieczowa. Jej wyższość nad chromatografią gazową polega na tym, że stosując chromatografię cieczową można analizować r&oacute;wnież związki, kt&oacute;re są termicznie niestabilne (rozkładają się w podwyższonej temperaturze). Chromatografy cieczowe przeznaczone do analizy środk&oacute;w wybuchowych wyposaża się zwykle w detektory typu diode array lub sprzęga ze spektrometrem mas. <br />&nbsp;&nbsp;&nbsp;&nbsp; Zaletą detektor&oacute;w diode array jest ich uniwersalność (pozwalają na wykrycie największej ilości środk&oacute;w wybuchowych i produkt&oacute;w ich rozkładu) i niezawodność. Analiza widma UV-VIS uzyskanego dla danego związku, w połączeniu z informacją o czasie retencji jest wystarczająca do jego identyfikacji, dzięki czemu unika się konieczności prowadzenia dodatkowej analizy potwierdzającej [11]. <br />&nbsp;&nbsp;&nbsp;&nbsp; Zaletą pełniących rolę detektora spektrometr&oacute;w mas jest przede wszystkim możliwość jednoznacznej identyfikacji rozdzielonych chromatograficznie analit&oacute;w, na podstawie uzyskanych dla nich widm masowych.&nbsp;<br />&nbsp;&nbsp;&nbsp;&nbsp; Czynnikiem utrudniającym wykrycie i identyfikację środk&oacute;w wybuchowych jest fakt, że zwykle w pr&oacute;bkach znajdują się jedynie śladowe ilości analit&oacute;w, rozproszonych w złożonej matrycy, zawierającej liczne związki przeszkadzające. W związku z tym jednym z najistotniejszych etap&oacute;w procesu analitycznego jest wyosobnienie analizowanych związk&oacute;w z matrycy i ich zatężenie. Ponieważ zwykle analizowane pr&oacute;bki mają postać ciał stałych (np. materiał z miejsca wybuchu, odnalezione domniemane elementy ładunku wybuchowego), a większość analit&oacute;w cechuje się zbyt małą lotnością, by m&oacute;c przeprowadzić ich adsorpcję z fazy nadpowierzchniowej, do ich wyosobnienia stosuje się zwykle ekstrakcję rozpuszczalnikową, najczęściej z wykorzystaniem acetonitrylu. Efekty zastosowania samej ekstrakcji rozpuszczalnikowej są jednak mało zadowalające &ndash; wraz z analitami z pr&oacute;bki ekstrahowane są r&oacute;wnież substancje przeszkadzające. Stężenie analit&oacute;w w ekstrakcie może być niedostateczne do ich wykrycia, a zatężenie ekstraktu przez odparowanie rozpuszczalnika powoduje r&oacute;wnocześnie zatężenie związk&oacute;w przeszkadzających i niesie niebezpieczeństwo rozkładu związk&oacute;w nietrwałych i utraty bardziej lotnych z analit&oacute;w. Przeprowadzone wstępne badania sugerują, że rozwiązaniem tego problemu może być połączenie dwu ...",06/08/18,1,"Wykrywanie i identyfikacja śladów środków wybuchowych w materiale z miejsca eksplozji, dla potrzeb wymiaru sprawiedliwości","ekspertyza sądowa, identyfikacja kryminalistyczna, materiały wybuchowe, HPLC, GC/MS, IR, ",183
23,1855,opisProjektuStanWiedzy,"<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><strong style=""mso-bidi-font-weight: normal""><span style=""mso-bidi-font-family: Arial""><font size=""3"">Szczepionka roślinnego pochodzenia - logistyka szczepień w warunkach zagrożenia bioterroryzmem. <o:p></o:p></font></span></strong></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">&nbsp;<o:p></o:p></font></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">Rośliny jako producenci rekombinowanych szczepionek podjednostkowych okazują się być efektywne i skuteczne na wielu płaszczyznach istotnych z punktu widzenia taniego i prostego ich wykorzystania. <o:p></o:p></font></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><strong><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">Ważną własnością roślin jest, iż będąc z natury wolne od ludzkich i zwierzęcych patogen&oacute;w, są zdolne jako organizmy transgeniczne do wytwarzania wybranych białkowych składnik&oacute;w tych patogen&oacute;w w formie natywnej i immunogennej, dzięki czemu spełniają w spos&oacute;b unikalny i niepowtarzalny rolę bezpiecznego nośnika i wydajnego &bdquo;bioreaktora&rdquo; dla szczepionek. <o:p></o:p></font></span></strong></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font size=""3""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">Dobry przykład dla tak rozumianego wykorzystania roślin transgenicznych jako nośnika szczepionek stanowi ekspresja antygenu HBs wirusa zapalenia wątroby typu B. Wytworzony w roślinie transgenicznej antygen HBs charakteryzuje się immunogennością identyczną co do jakości wzbudzanej odpowiedzi z antygenem używanym w komercyjnej szczepionce. Zostało to potwierdzone nie tylko w pr&oacute;bach szczepienia oczyszczonym antygenem na drodze parenteralnej </span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">(Tacket et al., 1998)</span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">, ale r&oacute;wnież, co stanowiło całkowite <em style=""mso-bidi-font-style: normal"">novum</em>, w pr&oacute;bach immunizacji pokarmowej polegających na skarmianiu zwierząt kalusem łubinu </span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">(Kapusta et al., 1999)</span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""> produkującym antygen HBs.</span> <span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">Nową jakość dla perspektywy użycia<span style=""mso-spacerun: yes"">&nbsp; </span>roślin jako nośnika szczepionek wniosły przede wszystkim pozytywne wyniki pr&oacute;b klinicznych z prototypową doustną szczepionką przeciwko wzw B roślinnego pochodzenia. W wyniku immunizacji polegającej na spożyciu sałaty produkującej antygen HBs, doszło u części szczepionych w ten spos&oacute;b ludzi do zaindukowania wytwarzania przeciwciał antywirusowych, kt&oacute;rych miano w surowicy osiągnęło poziom ochronny </span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">(Kapusta et al., 2001; Kapusta et al., 1999)</span><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">.<o:p></o:p></span></font></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">Z niepublikowanych dotychczas danych immunizacji przeciwko HBV wynika, że podawana doustnie szczepionka roślinnego pochodzenia może być stosowana r&oacute;wnież jako szczepionka przypominająca u os&oacute;b uprzednio zaszczepionych, u kt&oacute;rych po upływie kilku lat od pierwotnej serii szczepień, poziom przeciwciał anty-HBs, jest niższy od nieodzownego minimum (poniżej10 mIU/ml). <o:p></o:p></font></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""...",06/08/20,1,Ekspresja oraz ocena immunogenności rekombinowanego antygenu protekcyjnego laseczki wąglika (Bacillus anthracis) dla potrzeby opracowania prototypu szczepionki podjednostkowej przeciwko wąglikowi,"bioterroryzm, laseczka wąglika (Bacillus anthracis), antygen protekcyjny PA, Escherichia coli, transgeniczna sałata, immunizacja,  rekombinowana szczepionka podjednostkowa ",183
24,1856,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0pt 0pt 0pt 19.85pt; TEXT-INDENT: 12.35pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; LINE-HEIGHT: 150%; FONT-FAMILY: Arial"">Chromatografia cienkowarstwowa (TLC) jest najstarszą metodą analizy MW. Wprawdzie wsp&oacute;łcześnie bardziej skomplikowane techniki analiz stosowane są powszechnie [1-4], to jednak nie słabnie zapotrzebowanie na prostsze metody badań, wykorzystywane przede wszystkim do analiz skriningowych. <span style=""COLOR: black"">Skrining dotyczyć może zbioru pr&oacute;bek laboratoryjnych (wstępna selekcja pr&oacute;bek) lub określonego terytorium (selekcja terytori&oacute;w). Efektem takich analiz jest obniżenie koszt&oacute;w i czasu badań. Dobrym przykładem, potwierdzającym korzyści zastosowania TLC do wstępnej selekcji pr&oacute;bek zawierających MW, może być praca Haasa i Storka [5]; zasady selekcji terytori&oacute;w (podejrzanych <br />o skażenia materiałami wybuchowymi) przy użyciu TLC opisane są w pracy Nama [6].<o:p></o:p></span></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0pt 0pt 0pt 18pt; TEXT-INDENT: 14.2pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; LINE-HEIGHT: 150%; mso-bidi-font-family: Arial"">Największa liczba publikacji poświęconych zastosowaniu TLC do analizy MW ukazała się w latach sześćdziesiątych i siedemdziesiątych ubiegłego wieku (tabela 1). Dotyczyły one poszukiwania układ&oacute;w chromatograficznych, metod wizualizacji i technik rozdziału. Szczeg&oacute;łowe wyniki tych badań przedstawione są w monografii Kirchnera [7], a jej doskonałym uzupełnieniem obejmującym bibliografię do początku lat dziewięćdziesiątych są prace Yinona i Zitrina [8]. Kr&oacute;tka charakterystyka dalszych prac poświęconych tej tematyce zawarta jest w pracy Błądka [9].<o:p></o:p></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0pt 0pt 0pt 18pt; TEXT-INDENT: 14.2pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt; LINE-HEIGHT: 150%; mso-bidi-font-family: Arial""><o:p>&nbsp;</o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0pt 0pt 6pt 27pt; TEXT-ALIGN: justify; mso-outline-level: 1""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt; mso-bidi-font-weight: bold"">Tabela 1. Przykłady zastosowania TLC do analiz MW<o:p></o:p></span></p>
<div align=""center"">
<table class=""MsoNormalTable"" style=""mso-cellspacing: 2.0pt; mso-table-layout-alt: fixed; mso-padding-alt: 0pt 5.4pt 0pt 5.4pt; mso-border-insideh: .75pt outset windowtext; mso-border-insidev: .75pt outset windowtext"" cellspacing=""3"" cellpadding=""0"" border=""1"">
    <tbody>
        <tr style=""page-break-inside: avoid"">
            <td style=""BORDER-RIGHT: #d4d0c8; PADDING-RIGHT: 5.4pt; BORDER-TOP: #d4d0c8; PADDING-LEFT: 5.4pt; PADDING-BOTTOM: 0pt; BORDER-LEFT: #d4d0c8; WIDTH: 104.4pt; PADDING-TOP: 0pt; BORDER-BOTTOM: #d4d0c8; BACKGROUND-COLOR: transparent"" valign=""top"" width=""139"" rowspan=""2"">
            <p class=""MsoNormal"" style=""MARGIN: 0pt; TEXT-ALIGN: center"" align=""center""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt; mso-bidi-font-weight: bold"">Analit <o:p></o:p></span></p>
            </td>
            <td style=""BORDER-RIGHT: #d4d0c8; PADDING-RIGHT: 5.4pt; BORDER-TOP: #d4d0c8; PADDING-LEFT: 5.4pt; PADDING-BOTTOM: 0pt; BORDER-LEFT: #d4d0c8; WIDTH: 217.7pt; PADDING-TOP: 0pt; BORDER-BOTTOM: #d4d0c8; BACKGROUND-COLOR: transparent"" valign=""top"" width=""290"" colspan=""2"">
            <p class=""MsoNormal"" style=""MARGIN: 0pt; TEXT-ALIGN: center"" align=""center""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Arial; mso-bidi-font-size: 12.0pt; mso-bidi-font-weight: bold"">Układ chromatograficzny<o:p></o:p></span></p>
            </td>
            <td style=""BORDER-RIGHT: #d4d0c8; PADDING-RIGHT: 5.4pt; BORDER-TOP: #d4d0c8; PADDING-LEFT: 5.4pt; PADDING-BOTTOM: 0pt; BORDER-LEFT: #d4d0c8; WIDTH: 95.15pt; PADDING-TOP: 0pt; BORDER-BOTTOM: #d4d0c8; BACKGROUND-COLOR: transparent"" valign=""top"" width...",06/08/16,1,Zastosowanie chromatografii cienkowarstwowej do analizy materiałów wybuchowych,"chromatografia cienkowarstwowa, analizy środowiskowe, materiały wybuchowe, prochy ",183
25,1875,opisProjektuStanWiedzy,"&nbsp;
<div>Termograficzne badania nieniszczące nie są nową koncepcją w ocenie materiał&oacute;w. Pierwsze badania z tej dziedziny były wykonane na przełomie lat 50 i 60-tych XX wieku. Dopiero jednak możliwości komputerowej analizy obraz&oacute;w oraz pojawienie się system&oacute;w zobrazowania termalnego o wysokiej rozdzielczości temperaturowej spowodowało rozw&oacute;j badań nieniszczących z zastosowaniem termografii w podczerwieni zwiększając znacznie zakres możliwych aplikacji jak i jakości przeprowadzonych badań. Za istotny w technologii można uznać przełom wiek&oacute;w i symbolicznie rok 2000. Najważniejszym praktycznym wydarzeniem w tym czasie było pojawienie się jako produktu rynkowego kamer&nbsp;z niechłodzoną matrycą detektor&oacute;w FPA (focal plane array). Dzisiaj, kiedy standardem stały się kamery o NETD poniżej 20 mK <sup>1</sup> , możliwości ich zastosowania w nowych aplikacjach znacznie wzrosły. </div>
<div>Najważniejszym czynnikiem sukcesu w termograficznych badaniach nieniszczących w podczerwieni opr&oacute;cz sprzętu jest wiedza, doświadczenie i kwalifikacje personelu obsługującego ten sprzęt. W badaniach nieniszczących wykorzystujących termografię w podczerwieni jest szereg metod umożliwiających wykrywanie poszukiwanego defektu czy obiektu. Bardzo ważne jest zastosowanie właściwej metody, kt&oacute;ra będzie w danym przypadku najskuteczniejsza. W badaniach nieniszczących z zastosowaniem termografii w podczerwieni jest ściśle ukształtowane powiązanie i wzajemna stymulacja teorii i praktyki. Teoria to poznanie warunk&oacute;w powstania wad, bądź uszkodzeń, analiza sygnał&oacute;w, oddziaływań, wpływu i znaczenia wad i anomalii struktury materiału oraz synteza wiedzy o materiale zdobywana dzięki informacjom uzyskanym przez badania nieniszczące. Praktyka natomiast obejmuje rozw&oacute;j aparatury, metod i technologii badań, opracowanie zaleceń, norm i wytycznych oraz gromadzenie informacji, kt&oacute;re mogą stać się podstawą do opracowania realistycznych kryteri&oacute;w selekcji materiał&oacute;w oraz ustalenia warunk&oacute;w technicznych badań.</div>
<div>Wnikliwą analizę rozwoju i stanu termalnych technik nieniszczących na początku lat 90-tych na konferencji QIRT w Paryżu przedstawili: Gartenberg (138 pozycji piśmiennictwa) i Vavilov (40 pozycji piśmiennictwa). Uzupełnił to na początku obecnego wieku obszernym opracowaniem monograficznym Maldague.</div>
<div>Zwiększające się zainteresowanie diagnostyką termograficzną w podczerwieni wynika z jej nieinwazyjności, wysokiej efektywności a także uniwersalnego charakteru. Udowodniono to w trakcie wielu międzynarodowych konferencji poświęconych badaniom nieniszczącym, inżynierii materiałowej i termografii w podczerwieni. Uważa się, że do chwili obecnej ukazało się na świecie ponad 500 prac poświęconych tej problematyce.&nbsp;
<div>W kraju badania materiał&oacute;w kompozytowych wykonywano dotąd sporadycznie. W czasie krajowych konferencji badań nieniszczących oraz konferencji termograficznych tematyka ta pojawiała się stosunkowo rzadko. Publikacje, kt&oacute;re powstały w kraju, dotyczące badań nieniszczących materiał&oacute;w kompozytowych o zastosowaniach specjalnych przy użyciu metod termalnych, były w zdecydowanej większości autorstwa pracownik&oacute;w&nbsp;WITU.&nbsp;</div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp; Zastosowanie termografii w podczerwieni jest związane ze znanymi zaletami i wadami tej techniki. </span></div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp; Gł&oacute;wne zalety to: 1) przejrzysta prezentacja wynik&oacute;w (kolorowe termogramy); 2) duża szybkość badania (w por&oacute;wnaniu z innymi metodami), kt&oacute;ra pozwala na przebadanie dużego obszaru w stosunkowo kr&oacute;tkim czasie; 3) przydatność dla r&oacute;żnego typu materiał&oacute;w, w kt&oacute;rych wady wykazują miejscowe zmiany cieplnych własności. </span></div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp; Wady to: 1) wysoki poziom szum&oacute;w, kt&oacute;ry towarzyszy pr&oacute;bie ciep...",06/08/16,1,Metody termograficzne w badaniach nieniszczących materiałów kompozytowych do zastosowań specjalnych,"termografia w podczerwieni, badania nieniszczące, materiały kompozytowe",183
26,1895,opisProjektuStanWiedzy,Fragmenty obcych badań są fragmentarycznie znane i odnoszą się do r&oacute;żnych silnik&oacute;w i r&oacute;żnych stosowań samolot&oacute;w a nam chodzi o wiedzę uog&oacute;lniającą popartą dostępnymi przykładami jednego konkretnego silnika użytkowanego na określonym typie samolotu i jego przeznaczeniu.,06/08/24,1,Analiza parametrów przepływowych i cech dynamicznych turbinowego silnika odrzutowego podczas realizacji różnych misji samolotu bojowego jako nośników informacji diagnostycznej,"lotnicze silniki turbinowe, akceleracja i deceleracja podczas startu i w warunkach lotu, cykle zmeczeniowe",173
27,1915,opisProjektuStanWiedzy,"&nbsp;
<div><em>Jaki oryginalny wkład wniesie rozwiązanie postawionego problemu</em></div>
<div>W normach zharmonizowanych (np. PN-EN-250) postawione są wymagania co do pomiar&oacute;w bez wskazania sposobu ich realizacji. Badania w tym zakresie należy przeprowadzić samodzielnie.&nbsp;Możliwość ich realizacji ze specjalnym ukierunkowaniem na poszukiwanie modeli matematycznych pracy oddechowej, procesu wentylacji aparat&oacute;w nurkowych, wpływu ciśnienia na elementy automatyki i elektroniki przyczynią się do rozwoju takich dziedzin naukowych, jak: budowa i eksploatacja maszyn, mechanika precyzyjna, mechanika płyn&oacute;w i automatyka. </div>
<div>&nbsp;</div>
<div><em>Czy jest to problem nowy?</em></div>
<div>Dotychczasowe wdrożenia nowo opracowanych technologii nurkowych oraz system&oacute;w oddechowych wchodzących w skład system&oacute;w zachowania życia, wiązały się z koniecznością wykonywania potencjalnie niebezpiecznych eksperyment&oacute;w na ludziach. Taki stan rzeczy spowodowany był brakiem stanowiska metabolicznego symulatora oddechowego. Urządzeniem takim dysponują jedynie Szwedzi [<strong>Loucar M. 1992</strong>; <strong>Kłos R. 1999]</strong>. Prototyp takiego stanowiska został zbudowany także w Polsce <strong>[Kłos R.: 2002(b)]</strong>. Zbudowanie zestandaryzowanego stanowiska &bdquo;sztuczne płuca&rdquo; wraz z symulatorem metabolicznym byłoby osiągnięciem na skalę światową. Wykonywanie badań z jego wykorzystaniem daje możliwość precyzyjnego modelowania system&oacute;w oddechowych, gdyż symulator w przeciwieństwie do badań z udziałem nurk&oacute;w umożliwia prowadzenie eksperyment&oacute;w w powtarzalnych warunkach ze znana niepewnością. </div>",06/08/17,1,Nowa generacja hiperbarycznego symulatora oddechowego,"ratownictwo, nurkowanie",173
28,1935,opisProjektuStanWiedzy,"<p align=""justify"">Zagadnienie terroryzmu jednej sprawy omawiane jest w zdecydowanej większości wsp&oacute;łczesnych kompendi&oacute;w poświęconych problemowi terroryzmu. Niejednokrotnie poruszali tę kwestię najwybitniejsi badacze terroryzmu z Walterem Laquerem na czele (za przykład posłużyć może jedna z ostatnich jego publikacji The New Terrorism. Fanaticism and the Arms of Mass Destruction). Bez względu na to, jakiej terminologii się używa (czy najpopularniejszej - single issue terrorism - czy innej) - problem terrorystycznej działalności stanowiącej przedmiot niniejszego projektu uznać należy za istotny element wsp&oacute;łczesnych badań nad zagadnieniem terroryzmu. Zaskakującym przeto jest fakt, iż nie powstała dotąd monografia zagadnienia terroryzmu jednej sprawy. (Efektem prac nad niniejszym projektem ma być monografia wypełniająca tę lukę). Istnieje wiele prac analizujących rozmaite aspekty działalności radykalnych ruch&oacute;w społecznych zaangażowanych w konflikty światopoglądowe generujące terroryzm jednej sprawy. Niewiele jednakże miejsca w tej literaturze przedmiotu poświęcano dotąd ekstremizmowi i terroryzmowi jednej sprawy. W tej materii najobszerniejsza i absolutnie dominująca jest amerykańska i brytyjska literatura przedmiotu. Istnieje już kilka publikacji książkowych poświęconych poszczeg&oacute;lnym wsp&oacute;łczesnym formom terroryzmu jednej sprawy - gł&oacute;wnie terroryzmowi skrajnych ugrupowań ruchu obrony praw zwierząt. Trzeba jednakże nadmienić, iż część tych publikacji nie spełnia wymog&oacute;w naukowej rzetelności, gł&oacute;wnie z tego względu, iż nie zachowuje aksjologicznie neutralnej perspektywy analiz (wyjątkowo istotnej w badaniach nad konfliktami system&oacute;w wartości). Polscy badacze zagadnieniu ekstremizmu i terroryzmu jednej sprawy poświęcili dotąd mało uwagi, choć dostrzec już można symptomy zmiany tego stanu rzeczy. W ostatnich latach ukazało bowiem się kilka publikacji poświęconych kwestii ekoterroryzmu. Niemniej jednak w dalszym ciągu polska literatura przedmiotu w tej materii jest bardzo uboga, a nieliczne publikacje mają charakter bardziej faktograficzny niż analityczny. Ten stan rzeczy nie powinien dziwić, jako że skrajne odłamy ruch&oacute;w obrońc&oacute;w zwierząt i środowiska dawały o sobie znać stosunkowo rzadko i w relatywnie łagodny spos&oacute;b (najostrzejszą formę działania stanowiły niezagrażające zdrowiu i życiu ludzkiemu tzw. akcjach ekotażowe, czyli akcje ekologicznego i animalistycznego sabotażu, inaczej: monkeywrenching). Biorąc pod uwagę fakt, iż działalność wielu terrorystycznych ugrupowań jednej sprawy ma charakter zdecydowanie ponadnarodowy, a informacje na temat ich aktywności (odgrywające kluczową rolę w jej rozwoju) są powszechnie dostępne, fenomenowi temu polscy badacze winni poświęcić zdecydowanie większą uwagę niż dotąd. (W projektowanej monografii kwestia oddziaływania terrorystycznych przesłań jednej sprawy na niekt&oacute;re polskie kom&oacute;rki radykalnych ruch&oacute;w społecznych, z całą pewnością musi zostać podjęta i poddana analizom). Niezwykle istotny, a jednocześnie całkowicie zaniedbany w badaniach nad interesującym nas fenomenem jest problem ideologiczno-politycznych powiązań między ekstremistycznymi i terrorystycznymi grupami jednej sprawy a innymi ugrupowaniami terrorystycznymi o <br /><br />charakterze religijnym czy rewolucyjnym. (Kwestia ta r&oacute;wnież zostanie podjęta w projektowanej monografii). <br /></p>",06/08/17,1,Terroryzm jednej sprawy,"terroryzm jednej sprawy, ekstremizm, sabotaż, opór bez przywództwa, walka sieciowa, walka informacyjna, zagrożenie asymetryczne, konflikt systemów wartości",173
29,1955,opisProjektuStanWiedzy,"Problematyka amerykańskiej interwencji w Afganistanie nie doczekała się odpowiedniego opracowania na gruncie polskim. W Polsce opublikowana została jedna książka, kt&oacute;ra, mimo tytułu, tylko w niewielkim stopniu opisuje amerykańskie działania w Afganistanie. Jest to H.M.Kr&oacute;likowski, Cz.Marcinkowski, Afganistan 2002, Warszawa 2003. Jeżeli chodzi o piśmiennictwo zagraniczne, to wbrew pozorom nie ma aż tak wielu publikacji, jak można byłoby oczekiwać. Warto wspomnieć tu o takich opracowaniach jak: Hamilton, John, Operation Enduring Freedom, Edina, Minn., ABDO &amp; Daughters, 2002; Maloney, Sean M., Enduring thefreedom: a rogue historian in Afghanistan, Washington, D.C., Potomac Books, 2005; Oakes Mark, Operation Enduring <br /><br />Freedom and the conflict in Afghanistan: an update, London, House of Commons Library, 2001; Reddy L.R., Inside Afghanistan: end of the Taliban era?, New Delhi, APH, 2002; Friedman, Norman, Terrońsm, Afghanistan, and Ameńca's new way ofwar, Annapolis, Md., Naval Institute Press, 2003; The anatomy of a conflict:Afghanistan and 9/11 / Anand Giridharadas, New Delhi, Lotus Collection, 2002; Rogers, Paul, A war on terror: Afghanistan and after, London, Sterling, Va., Pluto Press, 2004. Ta relatywnie niewielka liczba opracowań wynika najprawdopodobniej z tego, że jeszcze podczas działań w Afganistanie, Amerykanie podjęli przygotowania do działań na innym froncie - w Iraku. To kolejne przedsięwzięcie odwr&oacute;ciło uwagę tak opinii międzynarodowej, medi&oacute;w, jak i naukowc&oacute;w od operacji &bdquo;Enduring Freedom&quot;. <br />",06/08/17,1,"Amerykańska interwencja w Afganistanie ,,Enduring Freedom, 2001.","terroryzm, Afganistan, amerykańska polityka, wojna w Afganistanie",183
30,1975,opisProjektuStanWiedzy,"Istniejący stan wiedzy dostarcza informacji og&oacute;lnych na temat budowy, pracy i eksploatacji turbinowych zespoł&oacute;w napędowych statk&oacute;w powietrznych, kt&oacute;ry jest powszechnie znany w dostępnej literaturze przedmiotu. Niniejsze zadanie badawcze dotyczy natomiast wyżej wymienionych zagadnień w odniesieniu do silnik&oacute;w, co do kt&oacute;rych brak jest dokumentacji technicznej. Przykładem takim jest zesp&oacute;ł napędowy samolotu F-16 przewidzianego do eksploatacji czy zesp&oacute;ł napędowy samolotu MiG-29 eksploatowanego w Siłach Powietrznych RP. Stąd istnieje pilna konieczność zdobycia, rozpoznania i powiększenia wiedzy dotyczącej tych konkretnych zespoł&oacute;w napędowych, tj. silnik&oacute;w F100-PW-229 i RD-33. <br />Analiza konstrukcji będzie oparta o dostępne materiały, zawierać będzie ocenę poszczeg&oacute;lnych rozwiązań konstrukcyjnych w por&oacute;wnaniu z innymi tego typu silnikami wraz z określeniem stopnia zaawansowania technologicznego poszczeg&oacute;lnych podzespoł&oacute;w. <br />Przeprowadzone będą wstępne obliczenia gazodynamiczne wzdłuż kanału przepływowego silnika F-100-PW-229. Wyniki te będą punktem wyjściowym do obliczeń obciążeń cieplnych za pomocą zaawansowanych system&oacute;w komputerowych (ANSYS, NASTRAN, FLUENT, FASTRAN). Zostanie przeprowadzona r&oacute;wnież analiza zagrożeń związanych z zasysaniem ciał obcych przez wloty samolot&oacute;w, kt&oacute;re ze względu na swoje położenie są narożne na tego typu zjawisko. <br />W ramach realizacji projektu rozpatrywane będą r&oacute;wnież zagadnienia dynamiczne, związane z oceną bieżącego stanu technicznego silnika za pomocą pomiaru wibroprzeciążeń. Nowością rozwiązania zagadnień dynamicznych jest propozycja globalnych obliczeń całego silnika, bez podziału na podzespoły (sekcje) przy wykorzystaniu modeli dyskretnych i ich matematycznego opisu. Stąd propozycja modelu mas skupionych (MMS) w zastosowaniu do całego zespołu napędowego. Walorem takiego podejścia jest łatwe por&oacute;wnanie otrzymanych wynik&oacute;w z analiz numerycznych z wynikami pomiar&oacute;w widma drgań rejestrowanymi w czasie lotu. <br />Proponowany temat ściśle wpisuje się w światowe kierunki badań o charakterze poznawczym i aplikacyjnym z zastosowaniem najnowszych technik inżynierii odwrotnej (rewers engineering). <br />",06/08/17,1,Zastosowanie inżynierii odwrotnej do identyfikacji właściwości przepływowych i dynamicznych turbinowych silników odrzutowych,"lotnictwo,silnik turbinowy,komputerowe wspomaganie projektowania konstrukcji CAD/CAM/CAE, inżynieria odwrotna",183
31,1995,opisProjektuStanWiedzy,"<p class=""MsoBodyTextIndent3"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 17.85pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt"">Synteza spaleniowa (znana r&oacute;wnież jako </span><span style=""FONT-SIZE: 11pt; mso-bidi-font-size: 10.0pt"">wysokotemperaturowa samopodtrzymująca się synteza, SHS)</span><span style=""FONT-SIZE: 11pt""> jest z powodzeniem wykorzystywana od połowy lat siedemdziesiątych ubiegłego wieku [9] do otrzymywania wielu materiał&oacute;w o specyficznych właściwościach. </span><span style=""FONT-SIZE: 11pt; mso-bidi-font-size: 10.0pt"">W ten spos&oacute;b wytworzono submikronowe proszki tlenku glinu [10], przezroczystą ceramikę opartą o La<sub>2</sub>Hf<sub>2</sub>O<sub>7 </sub>[11] lub Ni<sub>0,5</sub>Zn<sub>0,5</sub>Fe<sub>2</sub>O<sub>4</sub>/(SiO<sub>2</sub>)<sub>x</sub> [12], nanokompozyty Al<sub>2</sub>O<sub>3</sub>-ZrO<sub>2 </sub>[13], wysokotemperaturowe nadprzewodniki, np. MgB<sub>2</sub> [14], sole domieszkowane kationami ziem rzadkich [15] oraz mn&oacute;stwo innych interesujących materiał&oacute;w, np. azotki, borki, krzemki oraz cermetale [9]. Fakty te świadczą, że synteza spaleniowa jest tanim i efektywnym sposobem pozyskiwania nowych nanomateriał&oacute;w, szczeg&oacute;lnie o morfologii nanoproszk&oacute;w i wiskers&oacute;w. Od niedawna wiadomo, że produkty syntezy spaleniowej mogą występować także w formie nanorurek węglowych [8] i nanowł&oacute;kien węglika krzemu [5, 6].<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial"">Nanostrukturalne materiały węglowe otrzymuje się wsp&oacute;łcześnie kilkoma sposobami, z&nbsp;kt&oacute;rych najważniejsze to metody pirolityczne i synteza elektrołukowa. Pierwsza z wspomnianych metod polega na wprowadzaniu niskowrzących węglowodor&oacute;w wraz z rozpuszczonymi w nich katalizatorami do rury kwarcowej umieszczonej w strefie wysokiej temperatury w piecu oporowym. W metodzie elektrołukowej wykorzystuje się wyładowanie elektryczne pomiędzy dwoma elektrodami grafitowymi w atmosferze gazu obojętnego, pod niskim ciśnieniem. Obie techniki syntezy posiadają liczne wady, wśr&oacute;d kt&oacute;rych najważniejsze to energochłonność i małe wydajności produkt&oacute;w. <o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial"">Badania prowadzone w Instytucie Chemii WAT wykazały, że w autotermicznych reakcjach perhalogenowanych związk&oacute;w węglowych z reduktorami w postaci metali, krzemk&oacute;w metali oraz azydku sodu z dużą wydajnością otrzymuje się materiały nanostrukturalne o morfologii: nanowł&oacute;kien, nanopręt&oacute;w, nanokulek oraz rozwarstwiony grafit o dużej powierzchni właściwej, a&nbsp;także węglowe enkapsulaty żelaza. W roli utleniaczy stosowano do tej pory gł&oacute;wnie politetrafluoroeten (PTFE), heksachloroetan i heksachlorobenzen. <o:p></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 18pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial"">Kontynując te badania planujemy poszukiwanie innych utleniaczy z grupy perhalogenowanych węglowodor&oacute;w alifatycznych i aromatycznych. Szczeg&oacute;lnie dokładnie zbadamy możliwość zastosowania fluorowanych grafit&oacute;w, w kt&oacute;rych energia wiązania fluor wegiel jest mniejsza niż w PTFE i w&nbsp;związku z tym ciepło i temperatura reakcji mogą osiągać większe wartości. Zajmiemy się także poszukiwaniem związk&oacute;w, kt&oacute;re można zastosować jako reduktory. Oczekujemy, że krzemki żelazowc&oacute;w będą zdolne do autotermicznych reakcji ze związkami halogenowęglowymi. Obecność w środowisku reakcji żelazowc&oacute;w powinna sprzyjać tworzeniu nanorurek, a zatem reduktory te byłyby nie tylko źr&oacute;dłem krzemu, ale jednocześnie pełniłyby funkcje katalityczne. Sprawdzimy także katalityczne właściwości metalocen&oacute;w. Ich p...",06/08/18,1,Otrzymywanie nanostrukturalnych materiałów węglowych i ceramicznych na drodze syntezy spaleniowej,"synteza spaleniowa, materiały ceramiczne i węglowe, nanostruktury",183
32,2055,opisProjektuStanWiedzy,Zawarto w punkcie 2 oraz w bibliografii,06/07/31,1,Nowe metody oceny stanu technicznego metalowych łopat wirników nośnych śmigłowców w procesie eksploatacji,"łopaty wirnika nośnego, badania nieniszczące, resurs resurs godzinowy i kalendarzowy",173
33,2075,opisProjektuStanWiedzy,"Rozwiązania ochrony balistycznej zostały w większości ukształtowane w latach 60-tych i 70-tych ubiegłego wieku. Rozw&oacute;j środk&oacute;w niszczenia &ndash; powszechne wprowadzenie przeciwpancernych pocisk&oacute;w rdzeniowych (wolframowych i uranowych) i zwiększenie ich prędkości &ndash; spowodowały, że stosowane, najczęściej monolityczne, osłony (opancerzenie) stalowe są niewystarczające. Prowadzone prace zastosowania wysokowytrzymałej stali zwiększyły wsp&oacute;łczynnik efektywności masowej Em do ok. 1,8. Przy stosowaniu układ&oacute;w heterozyjnych (warstwowych) r&oacute;żnych materiał&oacute;w o zr&oacute;żnicowanych właściwościach (np. stali o malejącej w głąb twardości &ndash; od ok. 600HB na powierzchni i rosnącej w głąb plastyczności, wysokowytrzymałej i twardej ceramiki, hybrydowych zbrojonych wysokowytrzymałymi wł&oacute;knami polimerowych kompozyt&oacute;w) można osiągnąć wartość wsp&oacute;łczynnika Em na poziomie 2,2. <br />Dynamiczny rozw&oacute;j technologii materiał&oacute;w ostatnich 10 lat powoduje, że osiągnięcie wsp&oacute;łczynnika Em = 3 nie jest już iluzją [1]. Ilustruje to prognoza (rys. 1) możliwości rozwoju osłon balistycznych [1] poprzez przedstawienie wzrostu balistycznego wsp&oacute;łczynnika efektywności masowej Em w latach 19802020. <br />
<p align=""center""><img style=""WIDTH: 363px; HEIGHT: 272px"" height=""371"" width=""398"" alt="""" src=""/OSFImageLoader.do?idImageDB=447"" /></p>
<br /><br />Rys.1. Ilustracja wzrostu skuteczności osłon balistycznych - wzrostu ich balistycznego wsp&oacute;łczynnika efektywności masowej Em. <br />Przykładem postępu w tym zakresie jest zastosowanie, w miejsce osłon wykonanych z płyt stalowych homogenicznych (litych), osłon warstwowych o zr&oacute;żnicowanych własnościach poszczeg&oacute;lnych warstw. Charakteryzują to przykłady konstrukcji pancerzy niekt&oacute;rych czołg&oacute;w: <br />Leopard 2 &ndash; kadłub i wieża wykonane są z płyt pancernych spawanych, przednia płyta wykonana jest z pancerza warstwowego. Dominują w nim zatopione w stopie lekkim elementy ceramiczne charakteryzujące się wysoką twardością oraz gruba warstwa z tworzywa sztucznego pochłaniająca i rozpraszająca energię pocisk&oacute;w. Ocenia się, że zastosowanie udoskonalonego pancerza zwiększyło odporność przedniej części wieży i kadłuba około 3 razy w stosunku do pancerza stalowego. <br />M1 Abrams &ndash; kadłub jest spawany, zastosowano w nim pancerz warstwowy laminowany z wbudowanymi elementami ceramicznymi. Poszczeg&oacute;lne warstwy ułożone są w następujący spos&oacute;b: gruba warstwa stali pancernej, ceramika o strukturze plastra miodu, stal, warstwa płyt z tworzywa sztucznego, stal, warstwa antyodpryskowa. Najnowsze rozwiązania zawierają dodatkowo pancerz wykonany ze zubożonego uranu. Pancerz chroni przed przebiciem pociskami kinetycznymi kalibru 120mm z odległości większej niż 1000m. <br />Leclerc &ndash; zastosowano rozwiązanie modułowego pancerza, kt&oacute;ry ma wymienne elementy. Bezpośrednią ochronę tworzy pancerz kompozytowy oparty na materiałach ceramicznych jak r&oacute;wnież na płytach pancernych dużej twardości ułożonych na przemian z płytami ze stali ciągnionej. Zakłada się, że stal twarda zniszczy rdzeń pocisku podkalibrowego, a stal ciągniona przejmie jego energię. <br />Dostępna literatura na temat osłon balistycznych jest bardzo skromna, a producenci ukrywają najistotniejsze informacje o użytych materiałach oraz metodach ich łączenia. Dość dobrze poznane są właściwości balistycznych osłon z jednorodnych materiał&oacute;w metalicznych, chociaż brak jest opisanych zależności pomiędzy własnościami wytrzymałościowymi i plastycznymi a ich odpornością balistyczną [3, 4]. Są tylko przesłanki, że odporność balistyczna osłon metalowych w znacznym stopniu determinowana jest wartością ich dynamicznej granicy wytrzymałości, odpowiedniej plastyczności i twardości [5, 6]. <br />Dla ceramiki literatura podaje, że wytrzymałość balistyczna jest proporcjonalna do granicy objętościowego...",06/08/18,1,Opracowanie podstaw technologii elementów składowych modułu lekkiej kompozytowej osłony balistycznej,"tytan, aluminium, utlenianie i azotowanie jarzeniowe, zgrzewanie dyfuzyjne, fazy międzymetaliczne typu Al2O3, TixAly, Ti(Al)N, kompozyty warstwowe, technologia, osłona balistyczna",183
34,2095,opisProjektuStanWiedzy,"<p style=""margin: 0cm 0cm 0pt 14.2pt; text-align: justify;"" class=""MsoNormal""><span lang=""PL"" style=""font-size: 12pt;""><font face=""Times New Roman"">Jednym z ważnych osiągnięć wspomnianego wcześniej projektu badawczego KBN Nr 9T12D00717 [<span style="""">22</span>] było przeprowadzenie opisanego niżej eksperymentu, pokazującego jaki jest błąd dynamicznego pomiaru temperatury gazu w dyszy silnika K-15 za pomocą termopar oraz dowodzącego, że proponowane dotychczas w literaturze przedmiotu matematyczne modele termopar w postaci nieliniowego równania różniczkowego I rzędu (rys. <span style="""">1</span>), są nieprawidłowe. W ramach uprzednio prowadzonych prac udało się, między innymi, sformułować, opracować i poddać weryfikacji z wykorzystaniem danych rzeczywistych, szereg tzw. nieliniowych obserwatorów turbinowego silnika odrzutowego K-15 [<span style="""">22</span>], które z kolei wykorzystano do wykazania nie<span lang=""PL"" style=""font-size: 12pt;""><font face=""Times New Roman""></font></span>poprawności znanych modeli dynamicznych termopar, i które będą miały kluczowe znaczenie w trakcie opracowania neuronowych modeli termopar. Warto wyjaśnić, że obserwator silnika odrzutowego jest dość skomplikowanym algorytmem iteracyjnym, służącym do obliczania wartości trudno mierzalnych lub niemierzalnych parametrów pracy silnika w stanach nieustalonych, na podstawie innych, łatwo mierzalnych parametrów jego pracy [<span style="""">24</span>]. Przykładem parametru praktycznie niemierzalnego bezpośrednio jest chwilowa temperatura spiętrzenia gazów we wlocie turbiny, zaś przykładem parametru trudno mierzalnego bezpośrednio jest chwilowa średnia wartość temperatury spiętrzenia gazów wylotowych w dyszy lub ciąg silnika odrzutowego na samolocie podczas lotu.<o:p></o:p></font></span></p>
<p style=""margin: 0cm 0cm 0pt 14.2pt; text-indent: 18pt; text-align: justify;"" class=""MsoNormal""><span lang=""PL""><font size=""2""><font face=""Times New Roman""> </font></font></span><span lang=""PL""><font size=""2""><font face=""Times New Roman""><span lang=""PL"" style=""font-size: 12pt;""><font face=""Times New Roman""><img src=""/OSFImageLoader.do?idImageDB=628"" alt="""" /><br /></font></span></font></font></span></p>
<p style=""margin: 0cm 0cm 0pt 14.2pt; text-indent: 18pt; text-align: justify;"" class=""MsoNormal""><span lang=""PL""><font size=""2""><font face=""Times New Roman""> <span lang=""PL"" style=""font-size: 12pt;""></span><o:p></o:p></font></font></span></p>
<p style=""margin: 0cm 0cm 0pt 14.2pt; text-indent: 18pt; text-align: justify;"" class=""MsoNormal""><v:shapetype stroked=""f"" filled=""f"" path=""m@4@5l@4@11@9@11@9@5xe"" o:preferrelative=""t"" o:spt=""75"" coordsize=""21600,21600"" id=""_x0000_t75""><v:stroke joinstyle=""miter""></v:stroke><v:formulas><v:f eqn=""if lineDrawn pixelLineWidth 0""></v:f><v:f eqn=""sum @0 1 0""></v:f><v:f eqn=""sum 0 0 @1""></v:f><v:f eqn=""prod @2 1 2""></v:f><v:f eqn=""prod @3 21600 pixelWidth""></v:f><v:f eqn=""prod @3 21600 pixelHeight""></v:f><v:f eqn=""sum @0 0 1""></v:f><v:f eqn=""prod @6 1 2""></v:f><v:f eqn=""prod @7 21600 pixelWidth""></v:f><v:f eqn=""sum @8 21600 0""></v:f><v:f eqn=""prod @7 21600 pixelHeight""></v:f><v:f eqn=""sum @10 21600 0""></v:f></v:formulas><v:path o:connecttype=""rect"" gradientshapeok=""t"" o:extrusionok=""f""></v:path><o:lock aspectratio=""t"" v:ext=""edit""></o:lock></v:shapetype><v:shape fillcolor=""window"" type=""#_x0000_t75"" style=""margin-top: 7.85pt; z-index: 1; left: 0px; margin-left: 16.3pt; width: 235.8pt; position: absolute; height: 110.1pt; text-align: left;"" id=""_x0000_s1026""><font size=""2""><font face=""Times New Roman""><v:imagedata o:title=""ModelTermop"" src=""file:///C:/DOCUME~1/Wojtek/USTAWI~1/Temp/msoclip1/03/clip_image001.png""></v:imagedata><w:wrap type=""square""></w:wrap></font></font></v:shape></p>
<p style=""margin: 0cm 0cm 0pt 14.2pt; text-indent: 18pt; text-align: justify;"" class=""MsoNormal""><span lang=""PL""><font size=""2"" face=""Times New Roman"">Rys. </font><a name=""rys_zly_model"" target=""_blank""></a></span><font size=""2""><font face=""Times New Roman"">...",06/08/25,1,Budowa matematycznego symulacyjnego modelu termopary przeznaczonej do pomiarów nieustalonej temperatury gazu w przepływie o dużym zakresie jednoczesnych zmian prędkości i temperatury - przy wykorzystaniu nieliniowego obserwatora turbinowego silnika odrzutowego oraz techniki sztucznych sieci neuronowych,"pomiary temperatury gazów, nieliniowy obserwator, sztuczne sieci neuronowe, cyfrowy model symulacyjny, modele statyczne i dynamiczne",173
35,2096,opisProjektuStanWiedzy,"<p align=""justify"" style=""text-indent: 1.25cm; margin-bottom: 0cm;""><font face=""Arial, sans-serif""><font size=""2"" style=""font-size: 11pt;"">W przedstawionym wyżej zamiarze naukowym zarysowany jest nowy problem badawczy, obejmujący kompleksowe ujęcie działalności wojskowej służby dyplomatycznej w okresie dwudziestolecia międzywojennego. Istnieje dość obszerna literatura przedmiotu, obejmuje ona jednak przede wszystkim zagadnienia poświęcone dyplomacji polskiej II Rzeczypospolitej w których odnajdujemy jedynie szczątkowe informacje odnośnie dyplomatów wojskowych. Miało to związek z brakiem możliwości dotarcia do materiałów archiwalnych, szczególnie tych dotyczących działań polskiego wywiadu wojskowego, gdyż dyplomaci wojskowi takową działalnością się również zajmowali. Należy podkreślić, iż do chwili obecnej, brak jest całościowego, syntetycznego i nowoczesnego ujęcia działalności informacyjnej dyplomacji wojskowej okresu międzywojennego. </font></font> </p>",06/08/21,1,"Przygotowanie i obrona rozprawy doktorskiej pt.: Struktura organizacyjna, formy i metody pracy attachatów wojskowych oraz ich udział w działalności informacyjnej Oddziału II Sztabu Generalnego (Głównego) w latach 1921-1939",Wojsko Polskie – Oddział II Sztabu Generalnego – Attachaty wojskowe ,183
36,2115,opisProjektuStanWiedzy,"<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp; Problem automatycznego wyznaczania optymalnego toru ruchu pojazdu podwodnego do zadanego celu jako problem cząstkowy znacznie większej problematyki automatycznego sterowania robotem podwodnym podejmowany jest przez kilka ośrodk&oacute;w naukowych zagranicą i w kraju. Należy wspomnieć tutaj o: Massachusetts Institute of Technology, Florida Atlantic University, University of Trondheim oraz o Politechnice Szczecińskiej i Akademii Marynarki Wojennej. </div>
<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp; Ze względu na militarny lub komercyjny charakter zastosowań wyniki badań ośrodk&oacute;w zagranicznych są w większości niedostępne lub też dotyczą podstawowych zagadnień sterowania i optymalizacji toru ruchu. Szczątkowe informacje dotyczące tej problematyki spotkać można w materiałach konferencyjnych: IEEE International Conference on Robotics and Automation, International Conference on Marine Craft Maneuvering and Control. </div>
<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp; Analiza dostępnej literatury polsko- i obcojęzycznej wykazuje brak kompleksowych wynik&oacute;w badań dotyczących rozważanego zadania optymalizacyjnego. Dlatego też celowym wydaje się opracowanie systemu automatyki, kt&oacute;ry w praktyce ułatwi realizację misji przeciwminowej przy wykorzystaniu pojazdu podwodnego typu Głuptak. </div>",06/08/22,1,Wyznaczanie optymalnego toru ruchu pojazdu podwodnego do zadanego celu w obecności oddziaływujących prądów morskich z wykorzystaniem algorytmów genetycznych,"pojazd podwodny, algorytmy genetyczne ",173
37,2175,opisProjektuStanWiedzy,"<p align=""justify"">Eksploatowane obecnie w kraju tranzystorowe systemy oraz układy zasilania urządzeń elektronicznych małej i średniej mocy (do kilku kilowat&oacute;w), zar&oacute;wno cywilne jak i wojskowe, są praktycznie całkowicie oparte na klasycznych rozwiązaniach zasilaczy impulsowych z tzw. twardym kluczowaniem i regulacją napięcia (mocy) za pomocą modulacji szerokości impuls&oacute;w (ang. PWM). Zasilacze PWM obarczone są szeregiem wad takich jak: bardzo szerokie widmo (ponad 100MHz) emitowanych sygnał&oacute;w zakł&oacute;cających, ograniczona sprawność energetyczna i niezawodność wskutek często nadmiernych strat komutacyjnych w elementach p&oacute;łprzewodnikowych i ograniczony stopień miniaturyzacji układu. <br />Proponowane nowoczesne rozwiązanie zasilacza rezonansowego nie ma powyższych wad dzięki rezonansowemu kształtowaniu przebieg&oacute;w napięć i prąd&oacute;w w układzie a zwłaszcza w elementach p&oacute;łprzewodnikowych. Umożliwia to zredukowanie strat komutacyjnych, zwiększenie niezawodności pracy układu, podniesienie jego sprawności energetycznej, zwiększenie częstotliwości pracy i w rezultacie zwiększenie stopnia miniaturyzacji zasilacza. W przypadku wojskowych system&oacute;w elektronicznych bardzo istotną zaletą zasilaczy rezonansowych jest ich niski poziom emitowanych zakł&oacute;ceń radioelektrycznych zwiększający poufność transmisji danych i utrudniający wykrycie obiekt&oacute;w wojskowych. <br />Zasilacze rezonansowe są rozwijane na świecie od wielu lat [2,3,5] szczeg&oacute;lnie w nowoczesnych zastosowaniach wojskowych i technice kosmicznej [6]. Dopiero jednakże w ostatnich latach postęp technologiczny i związane z nim pojawienie się stosunkowo tanich i szybkich przełączających tranzystor&oacute;w i diod mocy oraz niskostratnych kondensator&oacute;w i niskostratnych ferrytowych materiał&oacute;w magnetycznych (wykorzystywanych w cewkach, dławikach i transformatorach) umożliwiło dalszy rozw&oacute;j układ&oacute;w rezonansowych. <br />Proponowane rozwiązanie zasilacza rezonansowego jest częściowo opisane w literaturze światowej gł&oacute;wnie w prostszej, przeznaczonej do niższych mocy wersji p&oacute;łmostkowej [2,5]. Jednakże nie są znane publikacje dotyczące optymalizacji proponowanego rozwiązania ze względu na poziom emitowanych zakł&oacute;ceń jak r&oacute;wnież brak jest skutecznych (opisujących dokładnie, ale możliwie niewieloma r&oacute;wnaniami) procedur projektowania takich zasilaczy szczeg&oacute;lnie z uwzględnieniem aspektu zakł&oacute;ceń. Podane w literaturze [2,3,5] procedury dotyczące układ&oacute;w p&oacute;łmostkowych są mało dokładne ponieważ nie uwzględniają rzeczywistego przebiegu napięć na tranzystorach (przebiegi te są przybliżane idealną falą prostokątną). Tym samym nie pozwala to na oszacowanie emitowanych zakł&oacute;ceń przewodzonych i promieniowanych zasilacza oraz ocenę szybkości zmian napięcia dren-źr&oacute;dło (dvDS/dt) tranzystor&oacute;w, a to z kolei uniemożliwia stwierdzenie czy nie przekroczono dopuszczalnej wartości dvDS/dt mogącej zniszczyć tranzystory MOSFET (poprzez uaktywnienie pasożytniczego tranzystora bipolarnego). Problem ten ma bardzo istotne znaczenie dla niezawodnej pracy układu. Planowane w ramach niniejszej pracy analiza, optymalizacja i procedura projektowania zasilacza uwzględniają między innymi problem zakł&oacute;ceń i rzeczywiste przebiegi czasowe napięć dren-źr&oacute;dło tranzystor&oacute;w. Wszystko to sprawia, że proponowany projekt badawczy wniesie istotny wkład w rozw&oacute;j tej dyscypliny zar&oacute;wno od strony teoretycznej jak i praktycznej sprawdzając rezultaty przeprowadzonej analizy teoretycznej w modelu użytkowym nowoczesnego zasilacza. <br /></p>",06/08/25,1,Polowy zasilacz rezonansowy,"zasilanie, układy rezonansowe",173
38,2195,opisProjektuStanWiedzy,"<p align=""justify"">Ochrona przed inwigilacją ze strony płetwonurk&oacute;w, może być realizowana z wykorzystaniem system&oacute;w aktywnych lub pasywnych, przy czym względy taktyczne przemawiają na korzyść tych ostatnich. Oferta czołowych producent&oacute;w obejmuje między inymi: <br />- zintegrowane systemy ochrony przed inwigilacją w&oacute;d przybrzeżnych, np. TONES &ndash; firmy Ultra Electronics, Coastal Surveillance System &ndash; firmy Siemens, <br />- zintegrowane systemy pokładowe, np. Atlas Acoustic Identification System &ndash; firmy Atlas Elektronik, HIRES TTP &ndash; firmy Konsberg, <br />- przenośne systemy wykrywania obiekt&oacute;w podwodnych wykorzystujące sonoboje, np. Airborne Acoustic System firmy Thomson Marconi, HIRES SBP &ndash; firmy Konsberg, <br />- stacjonarny system wykrywania płetwonurk&oacute;w Blue Easter Egg angielskiej firmy QinetiQ, <br />- Diver Detection Sonar izraelskiej firmy dsIT Solutions Ltd., <br />- Diver Detection Sonar firmy Westminster International Ldt. <br />Na szczeg&oacute;lną uwagę zasługuje przenośny zestaw do wykrywania i sledzenia płetwonurka Underwater Detection Unit firmy Naval Technology, dodatkowo wyposażony w system głosowego ostrzegania zlokalizowanego w toni wodnej obiektu. <br />Przytoczone wyżej przykłady dotyczą rozwiązań o charakterze kompleksowym przewidzianych dla jednostek pływających oraz stacjonarnych system&oacute;w brzegowych. <br />Konsultacje ze służbami operacyjnymi Marynarki Wojennej wskazują na potrzebę opracowania i wdrożenia autonomicznego, pasywnego systemu ostrzegającego o ewentualnych zagrożeniach dywersyjnych. System ten powinien być dobrze dostosowany do cech środowiska podwodnego w strefie przybrzeżnej. <br /></p>",06/08/23,1,System akustycznego wykrywania płetwonurków w środowisku podwodnym,"szumy podwodne, identyfikacja",173
39,2215,opisProjektuStanWiedzy,"&nbsp;
<div>Oryginalność wnioskowanego projektu badawczego polega na tym, że:</div>
<div><span>-<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>jest pierwszą w Polsce pr&oacute;bą kompleksowego rozwiązania tego zagadnienia dla potrzeb techniki walki przeciwminowej,</div>
<div><span>-<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>może stworzyć podstawy teoretyczne i dostarczyć danych do badań w pokrewnych dziedzinach techniki (np. techniki eksploracji dna morskiego, robotyki podwodnej, automatycznej nawigacji).</div>
<div>Zastosowanie praktyczne rezultat&oacute;w wnioskowanego projektu umożliwi rozwinięcie metodyki projektowania system&oacute;w sterowania i eksploatacji obiekt&oacute;w oraz świadome operowanie r&oacute;żnymi postaciami i poziomami uproszczeń modeli matematycznych układ&oacute;w napędowych i ich inteligentnymi systemami sterowania stosowanymi do rozwiązywania zagadnień r&oacute;żnej klasy. W konsekwencji otrzymany zostanie, zweryfikowany w oparciu o badania eksperymentalne, zestaw algorytm&oacute;w realizujących sterowanie adaptacyjne, zmierzające do zmiany zasady sterowania zgodnie ze zmieniającymi się rzeczywistymi charakterystykami hydrodynamicznymi okrętu, warunkami pływania i rodzajem wykonywanych zadań,<span> kt&oacute;ry może być podstawą do dalszych prac z następujących dziedzin:</span></div>
<div><span>-<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>inteligentne układy sterowania ruchem okrętu,</div>
<div><span>-<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>systemy automatycznego planowania misji bojowych jednostek pływających,</div>
<div><span>-<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>symulatory ruchu jednostek pływających.</div>",06/08/22,1,Sterowanie precyzyjne okrętem przy małych prędkościach ruchu z wykorzystaniem metod sztucznej inteligencji,"Sterowanie precyzyjne, Poszukiwanie min, Regulator, Sztuczna inteligencja",183
40,2255,opisProjektuStanWiedzy,"<p align=""justify""><font size=""3"">Rozw&oacute;j techniki laserowej i coraz szersze ich zastosowanie w metrologii sprawiło, że na początku lat sześćdziesiątych narodziła się koncepcja wykorzystania źr&oacute;deł światła monochromatycznego do prac podwodnych. Wszystkie opracowane urządzenia tego typu działają na zasadzie pomiaru r&oacute;żnicy czasu przyjścia impulsu promieniowania laserowego odbitego od powierzchni morza (w przypadku zastosowania statk&oacute;w powietrznych jako nosicieli uzyskiwano w ten spos&oacute;b pułap lotu) i impulsu promieniowania laserowego, odbitego od przeszkody wodnej (dna, niejednorodności ośrodka propagacji) oraz analizie parametr&oacute;w impuls&oacute;w po odbiciu od powierzchni morza - rys. 1.</font></p>
<p align=""justify""><font size=""3"">Na podstawie badań uznano, że najlepszym rozwiązaniem jest zastosowanie promienia w podczerwieni do pomiar&oacute;w na powierzchni i zielono-niebieskiego do pomiaru w głębi toni wodnej (np. lasera Nd:YAG).</font></p>
<p align=""justify""><font size=""3"">Prace w tej dziedzinie były prowadzone w USA przez NASA i US Navy i zaowocowały opracowaniem podstaw teoretycznych zjawisk zachodzących podczas propagacji promienia laserowego w wodzie oraz opracowaniem pierwszych urządzeń sondujących, a ich ciągłe doskonalenie wkr&oacute;tce pozwoliło na opracowanie efektywnego narzędzia o możliwościach eksploatacyjnych przewyższających w niekt&oacute;rych aspektach powszechnie dotychczas stosowane systemy akustyczne. W ten spos&oacute;b w kilku krajach rozpoczęto prace nad laserowym urządzeniem do skanowania profilu dna oraz pomiaru właściwości optvcznvch wodv.</font></p>
<p align=""justify""><font size=""3""></font>&nbsp;</p>
<em>
<p align=""justify""><font size=""3"">Rys. 1. Schemat ideowy zasady pracy urządzeń laserowych w aplikacjach morskich</font></p>
<p align=""justify""><font size=""3"">(źr&oacute;dło-lnternet:http:shoals.sam.usace.army)</font></p>
</em>
<p align=""justify""><font size=""3"">Analiza możliwości eksploatacyjnych pozwoliła na stwierdzenie, że gł&oacute;wnymi zaletami system&oacute; optycznych w aplikacjach morskich jest:</font></p>
<p align=""justify""><font size=""3"">&bull; szybkie rozpoznanie dużego akwenu;</font></p>
<p align=""justify""><font size=""3"">&bull; wysokie bezpieczeństwo realizacji prac;</font></p>
<p align=""justify""><font size=""3"">&bull; mobilność systemu przez zastosowanie statk&oacute;w powietrznych (samolot&oacute;w) jako nosicieli;</font></p>
<p align=""justify""><font size=""3"">&bull; możliwość prowadzenia prac na akwenach płytkich i w strefie przyboju, kt&oacute;re są niedostępne dla akustycznych urządzeń okrętowych.</font></p>
<p align=""justify""><font size=""3"">Wymienione powyżej cechy sprawiły, że liczne kraje rozpoczęły własne programy badawcze, mające na celu opracowanie i wprowadzenie do eksploatacji własnych konstrukcji urządzeń batymetrycznych.</font></p>
<p align=""justify""><font size=""3"">Wśr&oacute;d pionier&oacute;w zastosowania laser&oacute;w w pracach podwodnych wymienić należy Kanadę (kanadyjskie Centrum Zdalnego Monitorowania we wsp&oacute;łpracy z firmą Optech) i Australię (Organizacja Badań Uzbrojenia RAN). Ten ostatni kraj wprowadził na wyposażenie pierwszy system batymetrii laserowej WRELADS-1, kt&oacute;ry przeszedł pomyślnie pr&oacute;by w latach 1976-77 i stanowił punkt wyjścia dla prac podejmowanych w innych krajach. Korzystając z przyjętych w nim rozwiązań US Navy i NASA opracowały systemy sondowania optycznego, natomiast Szwecja, wraz z Kanadą i Australią opracowały sw&oacute;j własny system pomiarowy, zwany Mark-2, kt&oacute;ry wszedł na wyposażenie marynarek wojennych uczestnik&oacute;w projektu w na początku lat osiemdziesiątych. W tym czasie nastąpił gwałtowny rozw&oacute;j możliwości konstrukcji tego typu. W kr&oacute;tkim czasie na wyposażenie merynarek wojennych wprowadzono bardziej zaawansowane systemy optoelektroniki morskiej, jak LARSEN, SHOALS, FLASH, HAWKEYE oraz LADS,. Poszerzono też zakres stosowania urządzeń te...",06/08/23,1,URZĄDZENIE DO OPTYCZNEGO WYKRYWANIA PŁYWAJĄCYCH OBIEKTW PODWODNYCH,"lasery, elektronika, detekcja",173
41,2256,opisProjektuStanWiedzy,"<div align=""justify"">Zdecydowana większość pocisk&oacute;w kumulacyjnych jest wyposażona we wkładki kumulacyjne o kształcie stożkowym. Nie można jednak wskazać wyraźnego związku pomiędzy kątem wierzchołkowym a zdolnością przebicia pancerza. Istnieje natomiast taka zależność międzi odległością ogniskowania (SO &ndash; <em>stand of</em>) i zdolnością przebicia pancerza. Według stacjonarnej, hydrodynamicznej teorii kumulacji wraz ze zmniejszaniem kąta wierzchołkowego wkładki wzrasta znacznie prędkość strumienia lecz jednocześnie maleje jego masa. Strumień wygenerowany z wkładki o małym kącie jest mniej stabilny, a więc nie zawsze skutkuje to zwiększeniem zdolności przebicia.</div>
<div align=""justify"">Gł&oacute;wną charakterystyką determinującą efektywność pancernej osłony przed pociskami kinetycznymi KE i kumulacyjnymi SC jest głębokość penetracji (<em>P</em>). Wartość penetracji dla obydw&oacute;ch rodzaj&oacute;w pocisk&oacute;w można oszacować za pomocą wzoru:</div>
<div align=""justify""><span><img alt="""" src=""/OSFImageLoader.do?idImageDB=649"" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1)</span></div>
<div align=""justify"">gdzie: <em><span>h</span></em>- wsp&oacute;łczynnik efektywności, <em>L</em> &ndash; początkową długość pocisku KE lub długość strumienia kumulacyjnego, <em><span>r</span></em><em><sub>p</sub></em> &ndash; gęstość materiału pocisku (strumienia), <em><span>r</span></em><em><sub>t</sub></em> &ndash; gęstość materiału pancerza (tarczy).</div>
<div align=""justify"">Dla układ&oacute;w kumulacyjnych do wzoru (1) zamiast długości pocisku <em>L</em> należy podstawić długość w pełni uformowanego strumienia ciągłego <em>L</em><sub>SC</sub>.Strumień na skutek gradientu poosiowej prędkości masowej cząstek materiału ulega wyciąganiu, a następnie &ndash; poczynając od jego czoła &ndash; fragmentowaniu. Zjawisko fragmentowania strumienia jest procesem trwającym w skończonym przedziale czasu. Eksperymentalne określenie początku procesu fragmentacji, a tym samym długości w pełni uformowanego strumienia ciągłego, jest bardzo trudne i wymaga dużej ilości pr&oacute;b, kt&oacute;re podrażają koszty eksperymentu. Okazuje się, że jeśli elementy strumienia w czasie lotu do tarczy nie odkształcają się i nie tracą masy po procesie fragmentacji oraz są kolinearne z osią strumienia, to można we wzorze (1) stosować tzw. skumulowaną długość strumienia, kt&oacute;ra jest sumą długości poszczeg&oacute;lnych element&oacute;w i&nbsp;r&oacute;wna się <em>L<sub>SC</sub></em>.</div>
<div align=""justify"">Zgodnie z danymi literaturowymi, gł&oacute;wnym parametrem umożliwiającym optymalizację element&oacute;w układu kumulacyjnego jest średni czas fragmentacji strumienia, kt&oacute;ry oznaczono symbolem <em>t<sub>B</sub></em>. Oryginalny spos&oacute;b określania czasu <em>t<sub>B</sub> </em>podał E.&nbsp;Włodarczyk wraz ze wsp&oacute;łpracownikami w publikacji [1]. Zgodnie z&nbsp;wynikami tej pracy, długość <em>L<sub>SC </sub></em>&nbsp;można określić za pomocą wzor...",06/08/31,1,Badania możliwości zwiększenia głębokości przebicia pancerza strumieniem granatnikowej głowicy kumulacyjnej poprzez optymalizację kształtu jej wkładki,"kumulacja, głowica kumulacyjna, pocisk kumulacyjny, wkładka kumulacyjna",183
42,2275,opisProjektuStanWiedzy,"<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt 18pt; TEXT-INDENT: 18pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span style=""LINE-HEIGHT: 150%; FONT-FAMILY: &quot;Times New Roman&quot;; mso-bidi-font-size: 12.0pt""><font size=""3"">W Wojskach Rakietowych Sił Powietrznych przystosowano funkcjonujące analogowe stacje radiolokacyjne P-18 &bdquo;LAURA&rdquo; do wsp&oacute;łpracy z SNR NEWA SC. Ograniczeniem stosowanego rozwiązania jest: zakres przetwarzanej odległości do 100km, możliwość wsp&oacute;łpracy tyko z jedną SNR, przekazywanie wykryć w postaci plot&oacute;w. Proponowane rozwiązanie umożliwi wsp&oacute;łpracę RSWP z kilkoma PZR NEWA SC, oraz włączenie informacji radiolokacyjnej do sieci OPNET. <o:p></o:p></font></span></p>",06/08/24,1,"Opracowanie cyfrowego interfejsu wydawania informacji o sytuacji powietrznej z radiolokacyjnej stacji wstępnego wykrywania P-18, dla potrzeb Wojsk Rakietowych Sił powietrznych RP","stacja radiolokacyjna, przeciwlotniczy zestaw rakietowy",173
43,2276,opisProjektuStanWiedzy,"Analizowanie możliwości określania odległości przelotu pocisku względem ruchomej tarczy powietrznej jest zagadnieniem z dziedziny lokalizacji i śledzenia poruszających się obiekt&oacute;w. Opr&oacute;cz bardzo dobrze rozwiniętych metod aktywnej lokacji istnieją ogromne możliwość pasywnego określania położenia poruszających się obiekt&oacute;w. Szczeg&oacute;lna uwaga badaczy została skupiona na opisie zjawiska [3,4,6] i wykorzystaniu informacji o rozchodzeniu się powstających zmian ciśnienia w ośrodkach zaburzonych ruchem obiekt&oacute;w. Zagadnienia ta są przedmiotem badań eksperymentalno &ndash; doświadczalnych gł&oacute;wnie samolot&oacute;w [1,8,9,11], okręt&oacute;w nawodnych <br />i podwodnych [12] oraz wojskowych pojazd&oacute;w naziemnych [2, 11]. Og&oacute;lno dostępne są r&oacute;wnież modele [5,6] i wyniki badań [10] rozchodzenia się zmian ciśnienia w bardzo bliskiej odległości od źr&oacute;dła zaburzeń, kt&oacute;re wywołane są ruchem obiekt&oacute;w lecących z prędkością naddźwiękową. Brak jest natomiast szczeg&oacute;łowych modeli i og&oacute;lnodostępnych wynik&oacute;w badań zaburzeń ośrodka <br />w dalszej odległości do lecącego pocisku. Nad tej właśnie części zagadnień wykonawcy projektu zamierzają skupić gł&oacute;wny wysiłek badań. <br />",06/07/31,1,Analiza możliwości określania przelotu pocisku względem ruchomej tarczy powietrznej,"lokacja obiektów, drgania ośrodka, fala dźwiękowa",183
44,2286,opisProjektuStanWiedzy,"<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 26.95pt; TEXT-ALIGN: justify""><font size=""3""><span style=""FONT-FAMILY: &quot;Times New Roman&quot;"">Użytkownicy przeciwlotniczych zestaw&oacute;w rakietowych dążą do maksymalnego wykorzystania możliwości bojowych eksploatowanych przez siebie zestaw&oacute;w poprzez wprowadzanie r&oacute;żnych zmian i ich modernizację (np. zestawy NEWA, OSA, WEGA). <br />Z dostępnych informacji wynika, że jednym z parametr&oacute;w poprawianych w trakcie modernizacji jest zwiększenie strefy ognia PZR. Cel ten osiągany jest w rozmaity spos&oacute;b. Jako przykład można podać dane dotyczące PZR NEWA, kt&oacute;ry pod nazwą PECZORA-2M został zmodernizowany przez rosyjsko-białoruskie konsorcjum DEFENCE SYSTEMS. Zwiększenie zasięgu rakiet 5W27 tego zestawu uzyskano poprzez zastosowanie nowego typu paliwa oraz nowego typu silnika startowego o znacznie większej sile ciągu. Spos&oacute;b ten jest bardzo drogi i w warunkach krajowych raczej nie wchodzi w rachubę. Inny wariant pokazuje, że zwiększenie zasięgu rakiet można osiągnąć poprzez wprowadzenie nowych lub optymalizację już istniejących metod naprowadzania (poprzez zmianę algorytmu naprowadzania rakiet - np. firma TETRAEDR opracowała metody nazwane MTT i KDU). Z oczywistych względ&oacute;w dane dotyczące algorytm&oacute;w obr&oacute;bki sygnał&oacute;w oraz aparatury realizującej te metody są utajnione</span><span style=""mso-bidi-font-family: Arial"">.<span style=""COLOR: red""><o:p></o:p></span></span></font></p>",06/08/24,1,Analiza możliwości zwiększania zasięgu rakiety przeciwlotniczej poprzez zmianę algorytmu naprowadzania,"przeciwlotnicze zestawy rakietowe, metody naprowadzania, układy wyliczania komend",173
45,2296,opisProjektuStanWiedzy,"<span style=""FONT-SIZE: 12pt; COLOR: black; FONT-FAMILY: &quot;Times New Roman&quot;; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA; mso-no-proof: yes"">Na dzień dzisiejszy nie ma żadnych urządzeń tego typu do rejestracji sygnał&oacute;w zobrazowanych na wskaźnikach stacji naprowadzania rakiet. Nie ma r&oacute;wnież takich rozwiązań na rynku og&oacute;lnie dostępnych. Podyktowane jest to znacznym stopniem skomplikowania możliwych rozwiązań</span>",06/08/24,1,Opracowanie urządzenia automatycznej rejestracji sygnałów i informacji zobrazowanych na wskaźnikach stacji naprowadzania rakiet NEWA S.C.,"stacja radiolokacyjna, przeciwlotniczy zestaw rakietowy",173
46,2326,opisProjektuStanWiedzy,"<p align=""justify"">&nbsp;&nbsp;&nbsp; Istniejący stan wiedzy w zakresie tematu badań (jaki oryginalny wkład wniesie rozwiązanie postawionego problemu do dorobku danej dyscypliny naukowej w kraju i na świecie, czy w kraju i na świecie jest to problem nowy czy kontynuowany i w jakim zakresie weryfikuje utarte poglądy i dotychczasowy stan wiedzy) <br />&nbsp;&nbsp; Projekt stanowi kontynuację prowadzonych, szczeg&oacute;lnie po 2000 r., badań kierownika projektu w dotyczących pobytu wojsk radzieckich w Polsce. Wyraziło się to w postaci zrealizowania w Wyższej Szkole Oficerskiej w Toruniu dw&oacute;ch wewnętrznych temat&oacute;w badawczych pt.: Problemy stacjonowania wojsk radzieckich (rosyjskich) na terytorium Polski w latach 1944-1993 (część I), Toruń 2001, (WSO-JB Wewn. 52/2001), (część II), Toruń 2002, (WSO-JB Wewn. 38/2002). Zostały one przygotowane wraz Jerzym ZYGMUNTOWICZEM, kt&oacute;ry pod opieką naukową kierownika wniosku jesienią 2004 roku w Instytucie Historii, Wydziału Humanistycznego, Uniwersytetu Szczecińskiego obronił pracę doktorską na temat: Delegatura oraz Biuro Pełnomocnika Rządu PRL (RP) ds. pobytu Armii Radzieckiej (Wojsk Federacji Rosyjskiej) w Polsce. Opr&oacute;cz tego kierownik wniosku opublikował kilka artykuł&oacute;w związanych z problemami stacjonowania Armii Radzieckiej w poszczeg&oacute;lnych rejonach Polski. Uczestniczył r&oacute;wnież w konferencjach i sympozjach naukowych gdzie prezentował powyższą problematykę. <br />&nbsp;&nbsp;&nbsp; Autor projektu uważa, iż tak jak kwestia stacjonowania Wojsk Radzieckich w Polsce jest generalnie w naszym kraju znana, zbadana i opisana, tak problematyka związana z tym znamiennym zjawiskiem w krajach sąsiednich nie ma swojego odzwierciedlenia w polskiej literaturze naukowej. Istnieje więc potrzeba szerszego zaprezentowania tych kwestii polskiemu naukowemu środowisku historycznemu i politologicznemu oraz czytelnik&oacute;w zainteresowanych powyższą problematyką. <br /></p>",06/08/29,1,WOJSKA RADZIECKIE W PAŃSTWACH EUROPY ŚRODKOWO-WSCHODNIEJ PO II WOJNIE ŚWIATOWEJ,"podporządkowanie polityczne, militarne i gospodarcze, przemiany polityczne, procesy integracji i dezintegracji, stalinizacja, stosunki międzynarodowe, prawo międzynarodowe, polityka zagraniczna, ""zimna wojna"", opór społeczny, transformacja ustrojowa, Układ Warszawski, NATO, siły zbrojne, polska tożsamość narodowa, tożsamość narodów Europy Środkowo-Wschodniej, zanieczyszczenia ekologiczne, odszkodowania.",173
47,2328,opisProjektuStanWiedzy,"&nbsp;
<div>Prowadzone od wielu lat w Instytucie Optoelektroniki WAT prace badawczo - rozwojowe nad budową i&nbsp;optymalizacją urządzeń wykorzystujących promieniowanie podczerwone wchodzą obecnie w fazę opracowania nowoczesnych, matrycowych kamer termowizyjnych. W Przemysłowym Centrum Optyki w Warszawie oraz w Wojskowej Akademii Technicznej rozpoczęto prace nad skonstruowaniem polskich kamer termowizyjnych opartych o chłodzone i niechłodzone matryce detektor&oacute;w najnowszej generacji. Jakkolwiek sukces takiego przedsięwzięcia byłby milowym krokiem w kierunku wyposażenia polskiego wojska (oraz przemysłu) w modułowe kamery termowizyjne zbudowane w oparciu o możliwie jednorodne podzespoły (optyka, matryca FPA, układy odczytu i przetwarzania sygnału) jasne jest, że sukces rynkowy takiego sprzętu mogą zapewnić tylko konkurencyjne parametry techniczne, a te z kolei nie zależą jedynie od jakości detektora i optyki kamery. Technika przetwarzania obraz&oacute;w oraz rozw&oacute;j dedykowanych obr&oacute;bce obrazu system&oacute;w mikroprocesorowych daje obecnie możliwość implementacji skutecznych algorytm&oacute;w przetwarzania obrazu termowizyjnego w czasie rzeczywistym. Najnowsze osiągnięcia na tym polu jednego z największych producent&oacute;w kamer termowizyjnych, firmy FLIR (Szwecja) pokazują, że w konsekwencji, przy tym samym typie detektora, można otrzymać sprzęt o niepor&oacute;wnywalnie lepszych parametrach i funkcjonalności. Autorzy projektu widza możliwość praktycznego wykorzystania opracowanych w&nbsp;trakcie realizacji projektu algorytm&oacute;w we wspomnianych wyżej polskich, obserwacyjnych kamerach termowizyjnych i skompensowania w ten spos&oacute;b konieczności zastosowania komercyjnie dostępnych detektor&oacute;w IR o gorszych parametrach, ale znacznie niższej cenie.</div>
<div>Istotnym argumentem potwierdzającym celowość podjęcia proponowanych badań jest zgodność tej tematyki z programem Departamentu Polityki Zbrojeniowej MON dotyczącym 20. priorytetowych obszar&oacute;w badań i technologii w europejskich badaniach na rzecz bezpieczeństwa i walki z terroryzmem, w ramach projektu PASR [1] (Preparatory Action on Security Research) w punkcie 4: Pozyskiwanie danych dotyczących identyfikacji na polu walki oraz w punkcie 15: Czujniki i techniki do wykrywania i lokalizacji cel&oacute;w. Program badań jest r&oacute;wnież zgodny z opracowanymi przez MON prognozami rozwoju technologii na rzecz bezpieczeństwa oraz strategicznych cel&oacute;w państwa: w punkcie 5: Zaawansowane systemy optoelektroniczne.</div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></div>
<div><span>1.<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span>Istniejący stan wiedzy w zakresie tematu badań (jaki oryginalny wkład wniesie rozwiązanie postawionego problemu do dorobku danej dyscypliny naukowej w kraju i na świecie, czy w kraju i&nbsp;na&nbsp;świecie jest to problem nowy czy kontynuowany i w jakim zakresie weryfikuje utarte poglądy i&nbsp;dotychczasowy stan wiedzy)</div>
<div>&nbsp;</div>
<div>Obserwacyjne kamery termowizyjne są coraz częściej wykorzystywane w złożonych i rozbudowanych systemach, zar&oacute;wno w technice wojskowej jak i cywilnej. Do gł&oacute;wnych zastosowań tego typu system&oacute;w należy zaliczyć: rozpoznanie pola walki, kierowanie naprowadzaniem broni inteligentnej, zabezpieczanie obiekt&oacute;w, ochrona antyterrorystyczna, systemy wizyjne robot&oacute;w i&nbsp;automatycznej regulacji. Cechą wsp&oacute;lną tych system&oacute;w jest to, że wymagają one zastosowania zaawansowanych algorytm&oacute;w przetwarzania, analizy i&nbsp;rozpoznawania obraz&oacute;w. Zadaniem stosowanych algorytm&oacute;w...",06/08/31,1,Metoda poprawy rozpoznawania i identyfikacji obiektów w podczerwieni poprzez cyfrowe przetwarzanie termogramów,"przetwarzanie obrazu w podczerwieni, kamery termowizyjne, wykrywanie obiektów w podczerwieni",173
48,2332,opisProjektuStanWiedzy,"Problematyka stabilności broni z zamkiem swobodnym nie była dotąd dokładnie analizowana, ze względu na drugorzędne znaczenie broni wykorzystującej tę zasadę działania. Nie sformułowano zależności analitycznych określających kątowe przemieszczenie broni po powrocie zamka w przednie położenie. Przeprowadzone doświadczenia nie pozwalają na jednoznaczną ocenę wpływu parametr&oacute;w konstrukcyjnych broni. Analiza przemieszczeń kątowych broni, dokonana na podstawie rejestracji kamerą kolejności przestrzelin, wskazuje na znaczący wpływ postawy strzeleckiej oraz wyszkolenia strzelca na te przemieszczenia. Niemniej, na podstawie wielu pr&oacute;b można stwierdzić pewne skłonności danego rodzaju broni, kt&oacute;re są niezależne od strzelca. Realizowany w USA w latach osiemdziesiątych program przyszłościowego karabinka uwzględniał ograniczenia wpływu strzelca na położenie drugiej i trzeciej przestrzeliny poprzez oddanie serii strzał&oacute;w z szybkostrzelnością powyżej 2000 strz/min. Uzyskany tą drogą wzrost prawdopodobieństwa trafienia nie rekompensował jednak wzrostu koszt&oacute;w i komplikacji broni. <br />Badania przeprowadzone w trakcie realizowanej w WITU pracy nt.: &quot;Badanie stabilności broni strzeleckiej podczas ognia ciągłego&quot; nie potwierdzają w pełni utartego poglądu, że drugi i trzeci strzał w serii pada powyżej pierwszego. Widać tu wyraźny wpływ przyjętej postawy strzeleckiej podczas wykonywania strzelania. Brak jest wyjaśnień analitycznych tego zjawiska. <br />",06/08/31,1,Analiza sił i momentów wewnętrznych w broni z zamkiem swobodnym oraz doświadczalna weryfikacja sposobów ich redukcji,"odrzut broni, podrzut broni, narzut broni, zamek swobodny, stabilność broni",173
49,2338,opisProjektuStanWiedzy,"&nbsp;
<div>Pierwsze układy stabilizacji giroskopowej, w postaci wahadła giroskopowego zostały wykonane przez Otto Szlika w 1903r. i zastosowane do stabilizacji okręt&oacute;w wywołanych kołysaniem fal. Ale prawdziwe zainteresowanie układami stabilizacji kątowej zaczęło się w latach 50. w związku z burzliwym rozwojem techniki kosmicznej, gdzie były i nadal są stosowane do manewrowania promami kosmicznymi i bateriami słonecznymi sztucznych satelit&oacute;w.</div>
<div>Pierwsze modele matematyczne opisujące stabilizator giroskopowy wraz z koncepcją sterowania opublikował K. Magnus w 1971r. i W. Wrigley w 1972r. Wiele pozycji na ten temat zostało opublikowanych w&nbsp;dawnym ZSSR i&nbsp;publikowane są w krajach byłego Związku Radzieckiego, np. prace L. Kargu i M. Pawłowskiego. Warto podkreślić, iż prace te zawierają analizę wyłącznie modeli liniowych. Znikome informacje można tam znaleźć na temat sterowania tymi modelami.</div>
<div>W Polsce problemem sterowania giroskop&oacute;w zajmuje się gł&oacute;wnie prof. Jan W. Osiecki, przedstawiając w&nbsp;swych pracach badania nad pełnymi modelami matematycznymi oraz sterowania nimi metodą zagadnienia odwrotnego dynamiki. </div>
<div>W planowanym projekcie zakłada się opracowanie modelu matematycznego giroskopowego systemu naprowadzania i stabilizacji oraz wyznaczenie jego optymalnego sterowania. Model uwzględnia możliwość oddziaływań zakł&oacute;ceń zewnętrznych od pokładu obiektu latającego czy siły tarcia i niedokładności wykonania samego giroskopu.</div>",06/07/31,1,Analiza giroskopowego układu sterowania i stabilizacji bomby lotniczej,"naprowadzanie, giroskop, sterowanie, stabilizacja, bomba lotnicza",185
50,2344,opisProjektuStanWiedzy,"&nbsp;
<div>W Polsce brak jest praktycznie opracowań na ten temat. Wszelkie opracowania zagraniczne są bardzo og&oacute;lnikowe i nie mogą stanowić konkretnej informacji dotyczącej tematu projektu. Jest to w końcu zagadnienie objęte tajemnicą producent&oacute;w system&oacute;w rozpoznania obrazowego.</div>",06/08/31,1,Analiza możliwości wykorzystania systemów interpretacyjnych w obrazowym rozpoznaniu wojskowym,"rozpoznanie obrazowe, zdjęcia lotnicze i satelitarne, teledetekcja, filtracja, fotointerpretacja",173
51,2347,opisProjektuStanWiedzy,"&nbsp;
<div>
<div>Najczęściej opisywaną metodą wykrywania sygnał&oacute;w DS SS jest metoda polegająca na wyznaczaniu wsp&oacute;łczynnika korelacji między sygnałem odbieranym a sygnałem op&oacute;źnionym o <em><span>t </span>sygnałem odbieranym a sygnałem op&oacute;źnionym o <em><span>t.</span></em></em>[1], [2], [3], [4]. Metoda ta, polega na wielokrotnym (M- krotnym) wyznaczaniu korelacji miedzy </div>
<div><em>f)x(t-T)dt<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></em>(1)</div>
<div>gdzie: <em>x(t)<span>&nbsp;&nbsp; - </span></em>odebrany sygnał,</div>
<div><em>x(t-r) - </em>sygnał odebrany z op&oacute;źnieniem <em><span>t, </span><em>T -<span>&nbsp;&nbsp; </span></em>czas trwania okna czasowego.</em></div>
<div align=""center"">6</div>
<div>W&oacute;wczas, miarą fluktuacji korelacji jest moment drugiego rzędu</div>
<div>gdzie:<span>&nbsp;&nbsp; M -liczba okien.</span></div>
<div>Jak wykazano między innymi w pracach [1], [5], przy założeniu, że op&oacute;źnienie r jest r&oacute;wne czasowi trwania sekwencji rozpraszającej a każdy symbol rozpraszany jest tą samą sekwencją, w&oacute;wczas sumaryczna wartość średnia fluktuacji wywołanych sygnałem DS SS wynosi</div>
<div>&lt;=^<sup>4<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></sup>(3)<br />gdzie: <em>T<sub>s</sub><span>&nbsp;&nbsp;&nbsp; -&nbsp;&nbsp;&nbsp; </span></em>czas trwania symbolu,</div>
<div>&lt;t/<span>&nbsp;&nbsp;&nbsp;&nbsp; </span>-&nbsp;wariancja sygnału rozproszonego. Natomiast wartość odchylenia standardowego szumu jest dana zależnością</div>
<div></div>
<div>
<table height=""39"" cellspacing=""0"" cellpadding=""0"" width=""20"" hspace=""0"" vspace=""0"">
    <tbody>
        <tr>
            <td valign=""top"" align=""left"" height=""39"">
            <div><em><strong>(A\</strong></em></div>
            <div><strong><em><sup>(</sup></em><em>&nbsp;<sup>}</sup></em></strong></div>
            </td>
        </tr>
    </tbody>
</table>
<strong><em>&bdquo;</em><em><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _</span></em></strong></div>
<div><em><strong>~</strong></em></div>
<div><em>TWM</em></div>
<div>gdzie: <em>W - </em>szerokość pasma filtru środkowoprzepustowego (bliska szerokości pasma zajmowanego</div>
<div>przez sygnał DS S S), <em>a <sub>n</sub> </em><sup>2</sup><span>&nbsp;&nbsp; -&nbsp;wariancja szumu.</span></div>
<div>Stosunek tak wyznaczonego odchylenia standardowego fluktuacji wywołanego sygnałem D S SS i wartości średniej szumu jest określony wzorem</div>
<div>Z powyższego wynika, przy odpowiednio dużej liczbie sumowań <em>M </em>dla op&oacute;źnienia <em>r </em>będącego wielokrotnością <em>T<sub>s</sub>, </em>wartość fluktuacji wsp&oacute;łczynnika korelacji będzie większa niż dla op&oacute;źnień o innych wartościach. Pozwala to na stwierdzenie istnienia sygnału szerokopasmowego w paśmie wyznaczonym przez filtr pasmowoprzepustowy. Przykładowa krzywa fluktuacji korelacji wzajemnej sygnału odbieranego i sygnału op&oacute;źnionego przedstawiona jest na rys. 1. Dla op&oacute;źnień t <em>=T<...",06/09/01,1,Badania symulacyjne i analizowanie możłiwości detekcji sygnałów szerokopasmowych DS SS metodą sumowania widm,,173
52,2350,opisProjektuStanWiedzy,"Proponowane modelowe ujęcie problematyki ograniczania emisji spalin z silnik&oacute;w okrętowych, uwzgledniające zar&oacute;wno aspekty techniczno-eksploatacyjne, jak i ekonomiczne, jest nowatorskim podejściem do problemu. Wiąże się to przede wszystkim z wejściem w życie zaledwie przed rokiem Aneksu VI Konwencji MARPOL, a co za tym idzie problem ten będzie dopiero narastał wraz z koniecznością modernizacji coraz większej grupy jednostek pływających. <br />Dotyczczas problematyka techniczno-ekonomiczna związana z obniżaniem emisji związk&oacute;w toksycznych dotyczyła przede wszystkim samochod&oacute;w, zwłaszcza osobowych, gdyż wszelkie ich modernizacje w tym kierunku wiązały się ze wzrostem cen pojazd&oacute;w. W przypadku jednostek pływających powyższą problematyką zajmowali się jedynie armatorzy, kt&oacute;rych jednostki pływały w strefach wzmożonej kontroli emisji związk&oacute;w toksycznych w spalinach (np. strefa 100 mil od wybrzeży Kaliforni), co wiązało się z wysokimi opłatami za przepłynięcie jednostki. Obecnie przepisy dotyczące toksyczności spalin obejmują wszyskich armator&oacute;w, co zmusza ich do podjęcia odpowiednich działań. <br />Opracowane w ramach projektu modele pozwolą na ocenę przydatności znanych metod obniżania toksyczności spalin do zastosowania na danym silniku, ich efektywności w r&oacute;żnych warunkach eksploatacji silnika oraz jednostki pływającej, przy jednoczesnej ocenie aspekt&oacute;w ekonomicznych zar&oacute;wno doboru jak i eksploatacji jednostki z wybranym układem. <br />",06/09/11,1,Modele oceny efektywności techniczno-ekonomicznej systemu eksploatacji silników spalinowych statków morskich po wprowadzeniu Aneksu VI Konwencji MARPOL 73/78,"silnik okrętowy, emisja spalin, Konwencja MARPOL, efektywność techniczno-ekonomiczna",173
53,2354,opisProjektuStanWiedzy,"&nbsp;
<div>Dla oceny bezpieczeństwa zrzut&oacute;w podwieszeń podstawową metodą badawczą pozostają eksperymentalne badania tunelowe, choć coraz większą rolę odgrywają oblic<span>zenia numeryczne, zmniejszając koszty prac rozwojowych. Oryginalnym wkładem w badania zagadnień bezpieczeństwa zrzutu podwieszeń z nosiciela będzie zaproponowana metoda eksperymentalno-numeryczna, kt&oacute;ra przy stosunkowo niskich kosztach podniesie wiarygodność rezultat&oacute;w. Uzyskane rezultaty będą mogły być także wykorzystywane w obliczeniach całych trajektorii zrzucanych podwieszeń przy uwzględnieniu silnej interferencji pola przepływu wok&oacute;ł nosiciela w początkowej fazie i w jakim stopniu ona wpływa na tę trajektorię. Uzyskać będzie można odpowiedź na pytanie w jakich przypadkach może być pominięta a w jakich ta interferencja musi być uwzględniona. Rezultaty powyższego projektu uzupełnione będą w następnym projekcie o badania powyższych zagadnień w zakresie transonicznym i naddźwiękowym. Badania takie na modelu samolotu F-16 w skali około 1:20 mogą być przeprowadzone w trisonicznym tunelu N-3 I.Lot-u o zakresie M=0.3 - 2.3.</span></div>
<div>Badan<span>ia zagadnień aerodynamicznych samolot&oacute;w wysokomanewrowych i bezpieczeństwa zrzutu podwieszeń są w dalszym ciągu rozwijane w technice światowej. Problematyka ta stanowi także kontynuację prac realizowanych przez zesp&oacute;ł wykonawc&oacute;w projektu [2]</span></div>",06/07/31,1,Modelowe badania eksperymentalne w dużej skali własności aerodynamicznych samolotu F-16 z wybranymi podwieszeniami bojowymi i zagadnień bezpieczeństwa zrzutu podwieszeń,"lotnictwo, samoloty bojowe, aerodynamika, bezpieczeństwo lotu",183
54,3025,opisProjektuStanWiedzy,"<div style=""text-align: left;""><strong>OBECNY STAN WIEDZY</strong><br /></div>
<p align=""justify"">Scalone układy vision-chips stosowane są w komputerowym przetwarzaniu obrazu jako układy detekcji i wstępnej obróbki obrazu w czasie rzeczywistym [2]. Układy te pełnią rolę podobną do narządu wzrokowego ssaków (oka), gdzie obraz jest wstępnie przetwarzany przez siatkówkę, zanim zostanie poddany dokładnej obróbce w „jednostce centralnej”, czyli w korze mózgowej [3]. Obraz w siatkówce przetwarzany jest przez operującą w czasie ciągłym sieć neuronową, której bezpośrednie „odtworzenie” w układzie krzemowym nie jest zadaniem łatwym. Implementacja sieci neuronowych w technologii CMOS pociąga za sobą dużą złożoność układową, pobór mocy i dużą powierzchnię. Dodatkowo, kiedy sztuczna sieć neuronowa ma być programowalna, złożoność układowa i powierzchnia wrastają drastycznie. Z tych powodów sztuczne oko najczęściej implementuje się w technologii CMOS stosując tradycyjne techniki cyfrowe bądź analogowe. <br /></p>
<p align=""justify"">Jedną z możliwych struktur analogowego vision-chip pokazano na rys. 1. Jest to struktura w pełni równoległa, w której każdemu sensorowi optycznemu (pikselowi) przyporządkowany jest jeden prosty analogowy procesor piksela APE (od ang. analogue processing element). Zastosowanie architektury równoległej jest uzasadnione, gdyż układ vision-chip wykonuje niskopoziomowe funkcje przetwarzania obrazu (na przykład detekcja krawędzi, odszumianie itp.), które „posiadają” naturę równoległą. Oznacza to, że w układzie w układzie fizycznym, w trakcie wykonywania algorytmu, na każdym pikselu w danej chwili wykonywana jest dokładnie ta sama operacja. Dodatkową korzyścią z zastosowania architektury równoległej jest to, iż każdy procesor piksela posiada dostęp do danych ze wszystkich sąsiednich pikseli, nie ma więc potrzeby przesyłania tych danych na większe odległości, a liczba operacji przesyłania danych z sensorów optycznych (pikseli) do procesorów jest zredukowana do minimum. W konsekwencji równoległa architektura wydatnie zwiększa moc obliczeniową i umożliwia przetwarzanie obrazu w czasie rzeczywistym przy niskim poborze mocy i kosztach wytworzenia.</p>
<p align=""justify"">Czas przetwarzania matrycy procesorów wykonującej dany algorytm, w porównaniu do układu ściśle dedykowanego ASIC, jest zwykle dłuższy. Z drugiej jednak strony zaletą matrycy procesorów jest systematyczność i prostota budowy oraz – dzięki programowalności – uniwersalność. Większość komputerowych algorytmów przetwarzania obrazu wymaga wykonania wielu różnych niskopoziomowych operacji, zatem dla całego systemu przetwarzającego obraz korzystne jest zapewnienie jego pełnej programowalności, co pozwoli na implementację wielu różnych algorytmów bazując na tym samym sprzęcie. Matryca procesorów teoretycznie może wykonać nieskończoną liczbę algorytmów, w praktyce jest to ograniczone możliwościami sprzętowymi np. dostępną ilością pamięci. Wczesne analogowe vision-chips nie były uniwersalne i ograniczały się do implementacji konkretnej funkcji, na przykład wygładzanie [11], określanie położenia [12] oraz funkcji w przetwarzaniu stereoskopowym [13]. Implementację uniwersalnej matrycy procesorów o architekturze MIMD (ang. multiple instruction multiple data) można znaleźć w pracach [14] i [15]. Koncepcję analogowego przetwarzania w architekturze SIMD (ang. single instruction multiple data) i w czasie dyskretnym zastosowano po raz pierwszy w pracach [16] i [17] do realizacji uniwersalnej matrycy procesorów. W pełni programowalne realizacje równoległych analogowych vision-chips o architekturze SIMD opublikowano w pracach [19-23]. Podjęto również próby realizacji vision chips o architekturze SIMD wykorzystując mieszane techniki z układami analogowymi i sieciami neuronowymi [18]. <br /></p>
<p align=""justify"">W tym miejscu należy również wspomnieć o implementacjach cyfrowych vision chips. Spotyka się realizacje SIMD z procesorami jednobitowymi [4], [5], ale z powodu ograni...",07/01/29,23,Projekt i realizacja CMOS specjalizowanego analogowego procesora do wspomagania przetwarzania obrazów,"układy scalone, procesory analogowe, CMOS, przetwarzanie obrazów, układy niskomocowe",173
55,3053,opisProjektuStanWiedzy,"<p><font size=""3"">&nbsp; </font></p>
<div><font size=""3"">Rodzaj <em>Vaccinium</em> obejmuje 300 &ndash; 400 gatunk&oacute;w rosnących na całej p&oacute;łkuli p&oacute;łnocnej, od tropik&oacute;w po strefę polarną. Bor&oacute;wka wysoka (<em>Vaccinium x corymbosum</em> L.) obejmuje odmiany uprawne pochodzące od kilku gatunk&oacute;w z rodzaju <em>Vaccinium</em>. Początek odmianom szlachetnym dały międzygatunkowe krzyż&oacute;wki amerykańskich gatunk&oacute;w bor&oacute;wki wysokiej. Najważniejszą rolę w tworzeniu odmian uprawnych odegrały 4 gatunki bor&oacute;wki wysokiej: <em>Vaccinium corymbosum</em> L., <em>Vaccinium australe</em> Small., <em>Vaccinium arboreum</em> L., <em>Vaccinium angustifolium</em> Ait. <br /></font><span><font size=""3"">&nbsp;&nbsp; W przypadku intensywnej uprawy bor&oacute;wki&nbsp;priorytetem jest&nbsp;uzyskanie plonu wysokiej jakości, w&nbsp;związku z tym&nbsp;znajomość procesu kwitnienia&nbsp;jest&nbsp; niezmiernie istotna. Ważnym jest fakt, że na&nbsp; przebieg procesu kwitnienia i&nbsp;zapłodnienia&nbsp;mają wpływ warunki atmosferyczne, dlatego istotne jest zbadanie przebiegu kwitnienia&nbsp;w warunkach Polski ponieważ niekt&oacute;rych doniesień&nbsp; literaturowych nie można&nbsp; bezpośrednio przenosić na polski grunt.&nbsp; W przedstawionym projekcie grantu starano się określić skalę samopłodności najważniejszych uprawianych w Polsce odmian. <br />&nbsp;&nbsp; Na og&oacute;ł odmiany rosnące bardziej na p&oacute;łnocy <em>V. corymbosum</em> są samopłodne (Hermann i Plasner 2000). Pierwsze badania nad samopłodnością bor&oacute;wki wysokiej zaczęto prowadzić wkr&oacute;tce po wprowadzeniu tego gatunku do uprawy przez Coville (1921).&nbsp; Budowa kwiatu rodzaju <em>Vaccinium</em> świadczy o przystosowaniu gatunku do zapylenia krzyżowego np. kolor okwiatu, heterostylia, odległość pomiędzy&nbsp;otworami pylnik&oacute;w oraz kształt znamienia. W przypadku braku owad&oacute;w&nbsp; zapylających liczba ziaren pyłku padających na znamię jest niewielka. W przypadku <em>Vaccinium corymbosum</em>&nbsp;przy zapyleniu własnym pyłkiem obserwowano niezgodność polegającą na bardzo powolnym wzroście łagiewki pyłkowej pochodzącej z własnego pyłku. Jeśli w trakcie kwitnienia nie dojdzie do zapylenia obcym pyłkiem, w&oacute;wczas pod koniec okresu kwitnienia następuje zapłodnienie zalążka przez łagiewkę&nbsp;pochodzącą pyłku własnego (Ritzinger i Lyrene 1999). <br />&nbsp;&nbsp; Jest jednak wiele doniesień wskazujących, że obcozapylenie albo nie ma wpływu na zawiązanie owoc&oacute;w i ich wielkość albo, że poprawia te czynniki wpływające na wysokość plonu. Należy jednak w tym problemie rozdzielić pewne zagadnienia. Najczęściej stopień samopłodności u roślin jagodowych określany jest na podstawie liczby zawiązanych nasion przy samo- i obcozapyleniu. U bor&oacute;wek wykształcających dużą liczbę zalążk&oacute;w, maksymalna liczba zalążk&oacute;w mogących przekształcić się w prawidłowo rozwinięte nasiona jest genetycznie uwarunkowana i stała dla odmiany.&nbsp;Dla odmian, kt&oacute;re mają dużą liczbę źle zr&oacute;żnicowanych zalążk&oacute;w i są określane jako samopłonne,&nbsp;podniesienie plonu poprzez obcozapylenie praktycznie nie jest możliwe. Natomiast dla&nbsp;odmian o małej liczbie zalążk&oacute;w zdegenerowanych &ndash; w zalążni powstaje wtedy duża liczba&nbsp;prawidłowo wykształconych zalążk&oacute;w &ndash; rola obcego zapylenia ma duże znaczenie. Najczęściej odmiany te przy obcozapyleniu&nbsp;przyspieszają dojrzewanie owoc&oacute;w i poprawiają ich jakość poprzez zwiększenie udziału owoc&oacute;w większych i bardziej atrakcyjnych (Huang i in.1997). Obcozapylenie wpływa na zwiększenie o 43% udział owoc&oacute;w wcześniej dojrzewających, o 13 % udział owoc&oacute;w dużych oraz aż o 66% zmniejszenie udziału owoc&oacute;w małych. W efekcie, jak wyliczyli badacze, doch&oacute;d farmera amerykańskiego podnosi się o 5000 $ na 1 hektar (Lang i Parrie 1992). Według Ehlenfeldt (2001) obcozapylenie zwiększa masę owoc...",07/01/30,23,Biologia kwitnienia i zapylania borówki wysokiej oraz ocena zawirusowania roślin wybranych plantacji borówki w Polsce południowo - wschodniej,"borówka wysoka, zapylenie krzyżowe, zapylenie własnym pyłkiem, biologia kwitnienia, wirusy",173
56,3055,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""text-align: justify;""><span style=""font-size: 10pt; font-family: Arial;"">Dotychczas problem adaptacji metod zarządzania w naszym kraju nie stanowił przedmiotu kompleksowych badań. Powyższa uwaga odnosi się zwłaszcza do nowoczesnych metod organizacji i zarządzania, a za taką należy uznać metodę zarządzania strategicznego w systemach ukierunkowanych na jakość. Natomiast dużo miejsca w literaturze przedmiotu zajmują japońskie metody zarządzania produkcją (w tym: koncepcje, metody i techniki takie, jak: system kanban, jidoka, JiT, mapowanie strumienia wartości, poka yoke), co jest wynikiem popularności systemu Lean Management. W przypadku projektowanych badań uwaga zostanie zwrócona na metody, których konsekwencją jest dobór określonych metod zarządzania, w tym i zarządzania operacyjnego.<o:p></o:p></span></p>
<p class=""textglowny""><span style=""font-size: 10pt; font-family: Arial;""><span style=""""></span>Wyjściowa metoda zarządzania strategicznego w systemach zarządzania jakością – hoshin kanri – doczekała się jedynie krótkich prezentacji w formie artykułów lub rozdziałów w monografiach (publikacje prof. Z. Martyniaka, prof. R. Karaszewskiego, prof. M. Lisińskiego i wybranych artykułach dr Marka Ćwiklickiego (por. literatura przedmiotu umieszczona w punkcie 6)).<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style=""font-size: 10pt; font-family: Arial;""><o:p></o:p>Pełne opracowanie metody hoshin kanri pod względem metodycznym wzbogaci krajową literaturę przedmiotu z zakresu japońskich metod zarządzania, tym bardziej, iż jest ona protoplastą innej popularnej metody zarządzania zrównoważonej strategicznej karty wyników (Balanced Scorecard).<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style=""font-size: 10pt; font-family: Arial;""><o:p></o:p>Prezentowane badania ukażą sposób analizy adaptacji metod zarządzania (tu odniesiony do japońskich przedsiębiorstw) oraz staną się podstawą porównań z wynikami brytyjskimi badań z realizowanymi przez B. Witchera, co pozwoli określić różnicę pomiędzy polską, angielską a japońską postacią metody hoshin kanri. Identyfikacja pozostałych metod stosowanych przez firmy działające w Polsce, pozwoli na określenie pełnego ich zbioru, celów i sposobów zastosowania, a także różnic w stosunku do ich klasycznych japońskich wersji.<span style=""color: red;""> <o:p></o:p></span></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style=""font-size: 10pt; font-family: Arial;""><o:p></o:p>Proponowane badania są podobnymi do studiów przeprowadzonych w Wielkiej Brytanii przez dr B. Witchera z Norwich Business School. W przypadku angielskim badaniom poddano firmy stosujące metodę hoshin kanri i zaprezentowano różnicę wynikającą z ich specyfiki. Wyniki tych badań opublikowane w formie raportów i artykułów przez B. Witchera w latach 1999-2003, stanowiły jedną z przesłanek do zgłoszenia niniejszego projektu.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style=""font-size: 10pt; font-family: Arial;""><o:p></o:p>Uwzględniając ilość publikacji i ich temat w literaturze światowej należy stwierdzić, że metoda hoshin kanri jest powszechnie stosowana, nie mniej jednak występują różnice istotnie zmieniające jej pierwotny kształt. Zbadanie jej adaptacji w polskich warunkach pozwoli na wytyczenie dalszych kierunków studiów z tego tematu weryfikujących stosowanie zagranicznych metod. W przypadku kontynuowania badań nad innymi metodami możliwe stanie się dokładne określenie nie tylko zasad, toku postępowania, wad i zalet metod na podstawie zagranicznych studiów przypadków, ale także opracowanie wytycznych pasujących do specyfiki polskich firm. <o:p></o:p></span></p>",07/01/29,23,Adaptacja japońskich metod zarządzania w Polsce,"adaptacja metod zarządzania, hoshin kanri, japońskie metody zarządzania, ewolucja japońskich metod zarządzania, metody zarządzania jakością,  zarządzanie strategiczne w systemach zarządzania jakością",173
57,3083,opisProjektuStanWiedzy,"<p align=""justify"">W 1801 roku Johann Wilhelm Ritter odkrył promienie UV. W chwili obecnej promieniowanie w paśmie UV posiada szeroki zastosowanie: medycyna, przemysł spożywczy, farmaceutyka itp. Promieniowanie ultrafioletowe (nadfiołkowe, nadfioletowe) to promieniowanie elektromagnetyczne o długości fali od 40 do 400 nm (co odpowiada zakresowi energii 30 eV - 3 eV&nbsp;i umiejscawia je pomiędzy światłem widzialnym a promieniowaniem rentgenowskim). Zakres promieniowania ultrafioletowego dzieli się na UV pr&oacute;żni (długość fali 40-190 nm), dalekie promieniowanie ultrafioletowe (190-220 nm), UVC (220-290 nm), UVB (290-320 nm), oraz UVA (320-400 nm). Fale UVA podzielono dodatkowo na fale kr&oacute;tkie (320-340 nm)&nbsp;i długie(340-400 nm). Naturalne promieniowanie UVC praktycznie nie występuje na powierzchni Ziemi, ponieważ w całości jest pochłaniane jest przez atmosferę, podobnie jak promieniowanie pr&oacute;żni oraz dalekie UV. Sztuczne promieniowanie UVC jest wykorzystywane do odkażania (lampy bakteriob&oacute;jcze). Znaczne ilości UVC powstają r&oacute;wnież podczas spawania w łuku elektrycznym. Narażenie człowieka na znaczne dawki UVC może powodować stan zapalny sk&oacute;ry oraz uszkodzenia rog&oacute;wki. UVB jest uważane za najbardziej niszczycielską postać promieniowania ultrafioletowego ponieważ w tym zakresie długości fali nie dochodzi już do pełnej absorpcji promieniowania przez powietrze atmosferyczne, a zarazem energia foton&oacute;w jest wystarczająca do uszkodzenia kom&oacute;rkowego DNA. Niewielkie ilości UVB są niezbędne do syntezy witaminy D w organizmie ludzkim. Jednak większe dawki tego promieniowania powodują rumień sk&oacute;ry (oparzenie słoneczne), stymulują rozw&oacute;j raka sk&oacute;ry oraz zaćmy. Większa część pochodzącego ze słońca UVB jest pochłaniana przez ozon g&oacute;rnych warstw atmosfery. UVA jest odmianą promieniowania ultrafioletowego, na kt&oacute;rą organizmy żywe narażone są w największym stopniu. U człowieka UVA stymuluje opalanie sk&oacute;ry, czyli zwiększoną produkcję barwnika sk&oacute;ry (melanina). W przypadku nadmiernego wystawienia na UVA dochodzi ponadto do powstania rumienia sk&oacute;ry. Tylko niewielką część UVA pochłaniana jest przez ozon w atmosferze. Niewielkie dawki UVA są potrzebne do produkcji witaminy D w sk&oacute;rze. Nadmiar UVA powoduje grubienie i twardnienie sk&oacute;ry, osłabienie odporności immunologicznej organizmu oraz zaćmę. </p>
<p align=""justify"">Badania nad wpływem promieniowania świetlnego o r&oacute;żnych zakresach na organizmy żywe najczęściej są domeną lekarzy onkolog&oacute;w i fizykoterapeut&oacute;w. Uzyskane przez nich wyniki pozwoliły na rozw&oacute;j nowej medody leczenia - aktynoterapii. Dadej i in. [2003] w swojej pracy nt. roli UVA w patologii sk&oacute;ry&nbsp;przedstawiają poglądy na temat mechanizm&oacute;w oddziaływania promieniowania UVA na sk&oacute;rę, om&oacute;wiono udział UVA w procesie starzenia się sk&oacute;ry, kancerogenezie, immunosupresji i wywoływaniu fotodermatoz. Naukowcy zajmujący się sterylizacją produkt&oacute;w spożywczych wysoce rozwinęli gałąź nauki zwaną radiotechniką.&nbsp;Radioekolodzy prowadzą badania nad wpływem promieniowania świetlnego na rośliny. Rośliny gł&oacute;wnie absorbują część widma promieniowania biochemicznego (0,3-0,8 nm) niezbędnego do prowadzenie proces&oacute;w życiowych -&nbsp;promieniowanie FAR 0,4-0,7nm (Fotosyntetycznie Aktywna Radiacja).&nbsp;Przyswajanie przez rośliny światła ultrafioletowego jest utrudniona gdyż absorpcja w zakresie światła&nbsp;UV jest spowodowana przez drgania atom&oacute;w, związane z przeskokiem elektron&oacute;w z powłoki elektronowej o niższej energii na powłokę o wyższej energii. Podziemne części roślin praktycznie nie mają możliwości bezpośredniego i długotrwałego kontaktu z promieniami świetlnymi. </p>
<p align=""justify"">Dotychczasowy stan badań wskazuje, że poddanie nasion lub materiału sadzeniakowego stymulacji przy użyciu metod fizycznych (działanie p...",07/01/27,23,Stymulacja materiału sadzeniakowego ziemniaka promieniowaniem ultrafioletowym,"ziemniak, promieniowanie UV, rozwój, plonowanie",173
58,3107,opisProjektuStanWiedzy,"<p>Wytrzymałość oraz gabaryty części maszyn w dużym stopniu związane są z zastosowanymi w ich budowie materiałami. Konstrukcje powstałe dzięki zastosowaniu materiał&oacute;w kompozytowych takich jak tworzywa wzmacniane wł&oacute;knami szklanymi lub węglowymi,&nbsp;odznaczają się niewielką masą, odpornością na czynniki zewnętrzne i określonym przewodnictwem cieplnym i elektrycznym [6, 7]. <br />Wytrzymałość mechaniczna tworzyw wzmacnianych wł&oacute;knami szklanymi (TWW) zależna jest od właściwości mechanicznych materiał&oacute;w zbrojenia oraz osnowy, udział&oacute;w faz zbrojenia i osnowy, charakterystycznych wymiar&oacute;w zbrojenia (średnicy wł&oacute;kien) [8, 9]. Poza wymienionymi czynnikami, istotne znaczenie ma r&oacute;wnież zastosowany proces technologiczny, kt&oacute;ry wpływa na rozmieszczenie wł&oacute;kien zbrojenia w tworzywie osnowy [6, 10]. <br />Obecnie, w procesach technologicznych wytwarzania tworzyw wzmacnianych wł&oacute;knami, rozmieszczenie wł&oacute;kien nie jest uwzględnianie w ocenie jakości ich wykonania. Wynika to m.in. z braku badań zależności wytrzymałości mechanicznych takich tworzyw od jakości ich przygotowania, w szczeg&oacute;lności rozmieszczenia geometrycznego wł&oacute;kien zbrojenia. Jako wielkości charakteryzujące p&oacute;łprodukt podaje się zazwyczaj zawartość szkła oraz&nbsp;wytrzymałość mechaniczną na&nbsp;zginanie pr&oacute;bek-świadk&oacute;w.<br />Na podstawie przeprowadzonych badań literaturowych stwierdzono, że w ośrodkach naukowych zainteresowaniem objęte są zagadnienia związane z: delaminacją, propagacją pęknięcia, czy naprężeń występujących w materiale osnowy i wł&oacute;kien symulowanych z zastosowaniem metody element&oacute;w skończonych [2, 3]. W szczeg&oacute;lności doświadczenia dotyczące wpływu rozmieszczenia wł&oacute;kien na wytrzymałość materiału w kierunku poprzecznym są prowadzone w formie symulacji komputerowych [15, 16]. Testy wytrzymałościowe prowadzone na pr&oacute;bkach rzeczywistych dotyczą rozwoju pęknięć w kompozytach [3, 4, 5] lub materiale osłabionym otworami [17, 18].</p>
<p>Na podstawie przytoczonych&nbsp;fakt&oacute;w istotnym wydaje się przyjęcie kryteri&oacute;w, miar oraz metod oznaczania rozmieszczenia wł&oacute;kien zbrojenia i defekt&oacute;w, kt&oacute;re pozwolą na podjęcie pr&oacute;by rozwiązania tego problemu.</p>
<p><em><strong>Rezultatem projektu będzie opracowanie miary rozmieszczenia przydatnej w ocenie jakości materiał&oacute;w kompozytowych oraz prognozowaniu ich wytrzymałości mechanicznej na podstawie budowy mikrostrukturalnej.</strong></em></p>",07/01/25,23,Wpływ mikrostruktury geometrycznej kompozytów rowingowych na ich właściwości mechaniczne,"miara rozmieszczenia, kompozyty rowingowe, pomiary geometryczne mikrostruktur, komputerowa analiza obrazu",173
59,3131,opisProjektuStanWiedzy,"<p align=""justify"">Skutecznym sposobem oceny wydalania albumin z moczem jako wstępu do rozwoju nefropatii cukrzycowej, jest prowokacja albuminurii poprzez wysiłek fizyczny (8, 9, 10). Zasadne wydaje się więc przeprowadzenie analizy powysiłkowej albuminurii u dzieci w momencie klinicznego rozpoznania cukrzycy typu 1 oraz po p&oacute;łrocznym okresie jej trwania, kiedy choroba jest stabilna i można w pełni ocenić stopień jej wyr&oacute;wnania metabolicznego. W celu wystandaryzowania wynik&oacute;w albuminurii, stężenie albumin w moczu będzie odnoszone do stężenia kreatyniny w moczu, co posłuży do wyznaczenia wskaźnika albumina/kreatynina (ang. albumin/creatinin ratio, ACR) (1). Ponadto przy założeniu, że w analizie tej zostaną zaobserwowane r&oacute;żnice w poziomie albuminurii, możliwa będzie ocena wpływu stopnia wyr&oacute;wnania metabolicznego oraz stężenia peptydu C na poziom albuminurii u pacjent&oacute;w ze świeżo rozpoznaną cukrzycą typu 1. W ostatnich latach, dzięki zastosowaniu metod biologii molekularnej, ulega stopniowej zmianie podejście do mechanizm&oacute;w sprawczych proteinurii w nefropatii cukrzycowej. I tak, w kilku ośrodkach na świecie prowadzone są badania nad dwoma nowymi białkami: nefryną i podocyną, kt&oacute;rych ekspresja zachodzi w podocytach i kt&oacute;re mogą odgrywać kluczową rolę w uszczelnianiu błony filtracyjnej kłębuszk&oacute;w nerkowych (11, 12, 13). Uznaje się, że u chorych z nefropatią cukrzycową do ucieczki białka z moczem może doprowadzać zjawisko &bdquo;down-regulation&rdquo; przy zmniejszeniu ilości tych białek w kłębuszkach nerkowych (14). Uważa się obecnie, iż interakcje pomiędzy tymi białkami w podocytach, zar&oacute;wno w sensie ilościowym, jak r&oacute;wnież w ujęciu molekularnym mogą być odpowiedzialne za szczelność błony filtracyjnej (15). Interesującym z punktu widzenia mechanizm&oacute;w nefropatii cukrzycowej wydaje się dokonane ostatnio odkrycie, że ekspresja gen&oacute;w dla nefryny i podocyny zachodzi r&oacute;wnież na terenie trzustki. W przypadku nefryny udowodniono nawet, iż ma to miejsce bezpośrednio w kom&oacute;rkach wysp trzustkowych (16). W badaniach prowadzonych na modelu zwierzęcym, gdzie wywoływano cukrzycę eksperymentalnie, potwierdzono fakt obecności nefryny w moczu zwierząt i powiązano jej obecność ze wczesnymi zmianami o typie nefropatii cukrzycowej (17). Na modelu eksperymentalnym zaobserwowano r&oacute;wnież zmiany w poziomie tego białka w zależności od czasu trwania cukrzycy. I tak, ekspresja nefryny ulegała zwiększeniu w pierwszych 8 tygodniach trwania choroby, żeby zmniejszać się w dalszym jej przebiegu, r&oacute;wnolegle do narastania proteinurii (18). Badania prowadzone przez badaczy fińskich już u os&oacute;b chorych na cukrzycę typu 1 zdają się potwierdzać związek pomiędzy nefrynurią a poziomem albuminurii u tych pacjent&oacute;w. Autorzy ci sugerują, iż nefryna może okazać się prognostycznym wskaźnikiem powikłań nerkowych w cukrzycy (19). Istotnym zatem wydaje się dokonanie analizy stężenia nefryny i podocyny w moczu u dzieci chorych na cukrzycę typu 1 przed- i po pr&oacute;bie wysiłkowej w momencie rozpoznania cukrzycy oraz w okresie jej optymalnego wyr&oacute;wnania metabolicznego, a więc po 6 miesiącach trwania choroby. Poznanie tych fakt&oacute;w mogłoby przybliżyć odpowiedzi na pytania dotyczące odmienności przebiegu klinicznego cukrzycy typu 1 oraz uwarunkowań występowania nefropatii cukrzycowej. Celem pracy jest ocena wpływu wybranych czynnik&oacute;w na występowanie powysiłkowej albuminurii u dzieci ze świeżo rozpoznaną cukrzycą typu 1, ze szczeg&oacute;lnym uwzględnieniem stężenia nefryny i podocyny w moczu i ich roli jako potencjalnych marker&oacute;w ryzyka rozwoju nefropatii na dalszym etapie choroby. W celu sprowokowania albuminurii, pacjenci będą poddawani wystandaryzowanej pr&oacute;bie wysiłkowej na bieżni ruchomej wg protokołu Bruce&rsquo;a (20, 21). Ocena będzie dotyczyła stężenia albuminurii przed- i powysiłkowej w momencie klinicz...",07/01/24,23,Wpływ wybranych czynników na poziom albuminurii powysiłkowej u dzieci chorych na cukrzycę typu 1 - rola nefryny i podocyny jako potencjalnych markerów ryzyka rozwoju nefropatii cukrzycowej.,"cukrzyca typu 1, nefryna, podocyna, albuminuria powysiłkowa, C-peptyd, metabolizm",173
60,3153,opisProjektuStanWiedzy,"<p align=""justify""><font size=""4"">Jakość akustyczna pomieszczeń, ich komfort zawsze był jednym z najważniejszych parametr&oacute;w determinujących wszelkie działania akustyk&oacute;w wnętrz. Szczeg&oacute;lną uwagę poświęcano wnętrzom, takim jak sale koncertowe, teatralne, kina... W ostatnich latach uwaga akustyk&oacute;w zaczęła skupiać się na pomieszczeniach życia codziennego (sale biblioteki, czytelnie, pływalnie, biura, stacje kolejowe...) Niekt&oacute;re prace i artykuły dotyczą także akustyki pomieszczeń długich (stacje metra, korytarze, ulice i tunele). <br /></font></p>
<p align=""justify""><font size=""4"">Klimat akustyczny pomieszczeń, szczeg&oacute;lnie np. pomieszczeń sakralnych, zdeterminowany jest warunkami pogłosowymi, zrozumiałością mowy, hałasami zewnętrznymi oraz jakością dźwięk&oacute;w muzyki. Interesujące jest, czy podobne czynniki mają także wpływ na jakość akustyczną i klimat akustyczny pomieszczeń długich? <br /></font></p>
<p align=""justify""><font size=""4"">Kang [6] podaje definicję pomieszczenia długiego prostopadłościennego: długie pomieszczenie to pomieszczenie, kt&oacute;rego długość jest dużo większa od szerokości i wysokości pomieszczenia, gdzie jednocześnie te dwie wielkości są większe w por&oacute;wnaniu do długości fali dźwiękowej. Dobra jakość akustyczna jest ważnym czynnikiem pomieszczeń długich, w szczeg&oacute;lności w przypadku braku systemu nagłośnieniowego dla potrzeb mowy. Niestety w wielu tego typu pomieszczeniach zrozumiałość mowy mierzona poprzez STI nie jest satysfakcjonująca. Wiele badań akustycznych przeprowadzanych przez naukowc&oacute;w zakłada stałe liniowe warunki. Z tego względu nie mogą one być uwzględniane w rozważaniu rozwiązań architektonicznych. </font></p>
<p align=""justify""><font size=""4"">W&nbsp;artykule&nbsp;[6]&nbsp;podano także model obliczeniowy właściwości akustycznych systemu nagłośnieniowego, poprzez zastosowanie wynik&oacute;w badań z pojedynczym źr&oacute;dłem dźwięku. Podano także cztery r&oacute;żne sposoby poprawy wartości STI dla systemu nagłośnieniowego wielogłośnikowego. Długie pomieszczenia są przestrzeniami z niedyfuzyjnym polem akustycznym, dla kt&oacute;rego klasyczne teorie akustyczne nie zawsze mają zastosowanie. Z tego względu modelowanie pola akustycznego pomieszczeń długich jest r&oacute;żne od przewidywania zachowania się dźwięku w polu dyfuzyjnym. Rozpoczęto prace na rozwojem modeli bazujących na metodzie promieniowej do wizualizacji pola dźwiękowego w pomieszczeniach długich, w odniesieniu do takich przestrzeni jak stacje metra, kt&oacute;re są pomieszczeniami długimi o regularnym przekroju poprzecznym. Przewidywania r&oacute;żnych parametr&oacute;w wykazują wysoką zgodność z wynikami badań eksperymentalnych w cały zakresie częstotliwości, w szczeg&oacute;lności w polu dalekim. <br /></font></p>
<p align=""justify""><font size=""4"">W długich pomieszczeniach, takich jak stacje metra, klasyczne teorie akustyczne nie mają zastosowania, gdyż założenie pola dyfuzyjnego nie dotyczy pomieszczeń o tak ekstremalnych wymiarach. W ostatnich latach Kang i Orlowski [11] przeprowadzili badania właściwości akustycznych pomieszczeń długich i przedstawili podsumowanie swoich prac teoretycznych oraz poradnik projektowania oparty na projektach i badaniach naukowych stacji metra przeprowadzonych w Hong Kongu. Do przewidywania czasu pogłosu oraz zrozumiałości mowy zastosowali modele teoretyczne. Jednocześnie przeprowadzali badania nad modelami wykonanymi w skali. Badali efektywność rozwiązań akustyki architektonicznej. W pracy przedstawili p&oacute;ł-empiryczny wz&oacute;r na czas pogłosu Td w pomieszczeniach długich:</font><font size=""4""></font></p>
<p align=""center""><img style=""WIDTH: 403px; HEIGHT: 70px"" height=""125"" alt="""" width=""652"" src=""/OSFImageLoader.do?idImageDB=4575"" /></p>
<p><font size=""4"">gdzie:</font><font size=""4""></font> </p>
<ul type=""disc"">
    <li><font size=""4"">&nbsp;<em>d</em> &ndash; odległość źr&oacute;dła dźwięku od odbiornika, m,&nbsp;<...",07/01/31,23,Metody oceny jakości akustycznej pomieszczeń długich,"akustyka pomieszczeń, pomieszczenia długie",183
61,3163,opisProjektuStanWiedzy,"Do XX wieku akustyka uważana była bardziej za sztukę niż za naukę. Badania akustyczne prowadzone były metodą &ldquo;pr&oacute;b i błęd&oacute;w&rdquo;. W ubiegłych dw&oacute;ch wiekach wielu badaczy przyczyniło się do rozwoju badań nad akustyką, miedzy innymi Rayleigh czy Helmholtz. Na początku XX wieku akustyka stała się ważnym przedmiotem badań naukowych. Wraz z rozpoczęciem II wojny światowej rozwinięto metody ultrasoniczne w medycynie. Badania akustyczne nabrały znaczenia w Europie i USA. Powstały wielkie laboratoria po to, aby badać zjawiska takie jak wykrywanie łodzi podwodnych, albo komunikacja głosowa w warunkach hałasu w samolotach lub pojazdach wojskowych. Co więcej, zaczęto badać oddziaływania psychologiczne efektu hałasu. Po wojnie nacisk w badaniach akustycznych przesunął się w kierunku zastosowań w architekturze i przemyśle. <br />W latach 50-tych ubiegłego wieku samoloty z silnikami odrzutowymi, kt&oacute;rych gwałtowny rozw&oacute;j przypadał na okres II wojny i powojenny zaczęły wchodzić do użytku cywilnego. Na początku lat 60-tych, zaczęto używać powszechnie samolot&oacute;w z napędem odrzutowym do podr&oacute;ży długodystansowych. W związku z tym, natężenie hałasu w pobliżu lotnisk stało się poważnym problemem. Z powodu wysokiego hałasu silnik&oacute;w odrzutowych pod koniec lat 60-tych rządy wielu kraj&oacute;w wprowadziły regulacje prawne dotyczące dopuszczalnego natężenia hałasu. Od tego czasu ustawy rządowe stają się coraz bardziej ostre i mają coraz szersze zastosowanie. Dzisiaj regulacje dotyczące hałasu nie ograniczają się tylko do przemysłu lotniczego, ale także do innych gałęzi przemysłu, takich jak przemysł samochodowy czy energetyczny. <br />Hałas aerodynamiczny stał się źr&oacute;dłem zainteresowań wielu badaczy. Ligthtill zaproponował metodologię do określania tzw. hałasu &bdquo;odrzutowego&rdquo;. Zasugerował, że w sąsiedztwie silnik&oacute;w odrzutowych przestrzeń można podzielić na dwa obszary. Pierwszy obszar, tzw. źr&oacute;dło hałasu, znajduje się w najbliższym sąsiedztwie silnika i jest stosunkowo mały. Następnie hałas generowany w tym małym obszarze rozchodzi się liniowo w niezaburzonej przestrzeni (atmosferze). Ligthtill wprowadził ten podział przestrzeni do r&oacute;wnań dynamiki gaz&oacute;w oraz zaproponował spos&oacute;b opisu procesu rozprzestrzeniania się dźwięku, kt&oacute;ry nazywa się analogią lub metodą Ligthtilla. W analogii Ligthtilla przyjmuje się, że dźwięk generowany aerodynamicznie jest przetwarzany w oparciu o dane turbulentnego pola przepływu aerodynamicznego. Ponieważ analogia Ligthtilla oparta jest na prawach zachowania przepływu (r&oacute;wnaniach Naviera-Stokesa), dlatego ważna jest dla wszystkich rodzaj&oacute;w przepływu bez ograniczeń.. Ligthtill przekształcił gł&oacute;wne r&oacute;wnania zachowania (masy i pędu) eliminując w nich pewne człony. Dodał on r&oacute;wnania zachowania do siebie tak, aby otrzymać niehomogeniczne r&oacute;wnanie falowe dla fluktuacyjnej gęstości przepływu <br />Analogia Ligthtilla posłużyła jako punkt początkowy wielu badań naukowych w tej dziedzinie. Jego badania znajdują się wśr&oacute;d najczęściej wykorzystywanych oraz cytowanych jeśli chodzi o obliczenia hałasu aerodynamicznego. <br />Ponieważ analogia Ligthtilla została wyprowadzona dla hałasu generowanego przez strumień wypływający z silnika odrzutowego, gdzie efekt nieprzenikalności ścianek w przepływie można zaniedbać, nie można jej zastosować do hałasu generowanego przez śmigła. Curle wprowadził do r&oacute;wnania Ligthtilla źr&oacute;dła hałasu pochodzące od stacjonarnych, stałych brzeg&oacute;w. Curle uwzględnił wymianę pędu płynu ze źr&oacute;dłami umieszczonymi na powierzchni ciała stałego i przeprowadził podobne wyprowadzenie jak Ligthtill. W otrzymanym niehomogenicznym r&oacute;wnaniu falowym pojawił się w związku z tym dodatkowy człon. <br />P&oacute;źniej Ffowcs Williams i Hawkings, rozwinęli r&oacute;wnanie Curle dla ruchomych brzeg&oacute;w nieprzenikalnych (tzw. ś...",06/12/15,23,Numeryczne modelowanie aerodynamicznego hałasu w przepływach wewnętrznych metodami hybrydowymi uRANS(LES)/Euler,"aeroakustyka, uRANS, LES, Euler",173
62,3179,opisProjektuStanWiedzy,"<p align=""justify"">Badania nad zbiorowiskami pierwotniak&oacute;w Morza Bałtyckiego mają już pewną historię [Arndt, 1991; Bralewska i Witek, 1995; Witek, 1998; Set&auml;l&auml; i Kivi, 2003]. Zdecydowanie mniej jest natomiast prac na temat ekologii poszczeg&oacute;lnych gatunk&oacute;w pierwotniak&oacute;w [np. Johansson, 2002]. W efekcie analiza znaczenia ekologicznego zbiorowisk pierwotniak&oacute;w opiera się często na badaniach przeprowadzonych w innych częściach świata lub też z użyciem organizm&oacute;w przetrzymywanych w warunkach in vitro [np. Gismervik, 2006]. W efekcie takie wyniki mają tylko ograniczone zastosowanie dla oceny rzeczywistej roli ekologicznej pierwotniak&oacute;w w Morzu Bałtyckim. Dlatego też istnieje potrzeba przeprowadzenia badań nad odżywianiem się pierwotniak&oacute;w w warunkach <em>in situ</em> w Morzu Bałtyckim. Wstępne badania autora potwierdzają przewagę takich wynik&oacute;w nad tymi uzyskanymi podczas eksperyment&oacute;w <em>in vitro</em> [Rychert, praca złożona do&nbsp;publikacji w piśmie: Oceanologia].</p>
<p align=""justify""><br />Ponadto prowadzone dotychczas badania nad składem zbiorowisk pierwotniak&oacute;w w polskiej części Morza Bałtyckiego&nbsp;ograniczone były zasadniczo&nbsp;do Zatok Gdańskiej i Pomorskiej [Witek, 1995; Wrzesińska-Kwiecień i Mackiewicz, 1995]. Realizacja prezentowanego projektu pozwoliłaby dodatkowo uzupełnić obecny brak takich badań na Pomorzu Środkowym.<br /></p>",07/01/30,23,Ekologiczna charakterystyka morskich pierwotniaków pelagicznych strefy brzegowej Pomorza Środkowego.,"sieć mikrobiologiczna, orzęski, preferencje pokarmowe, wyżeranie",173
63,3189,opisProjektuStanWiedzy,"<p align=""justify"" style=""font-family: Times New Roman;""><font size=""3""> &nbsp;&nbsp;&nbsp; W dobie poszukiwań coraz doskonalszych materiał&oacute;w o własnościach i budowie precyzyjnie dostosowanej do wyspecjalizowanych potrzeb, polimery o stopniowej zmianie składu wzdłuż łańcucha (&bdquo;gradientowe&rdquo;, rys. 1 i 2) budzą coraz większe zainteresowanie. Obecne zainteresowanie tego typu polimerami wiąże się z potencjalnymi możliwościami kontroli mikrostruktury (&bdquo;gradientu&rdquo; składu) łańcucha i z obserwacjami wskazującymi na specyficzne własności tych polimer&oacute;w, odr&oacute;żniające je od kopolimer&oacute;w blokowych i kopolimer&oacute;w o r&oacute;wnomiernym rozkładzie jednostek w łańcuchu [1-4]. Warunkiem otrzymania kopolimer&oacute;w o zadanym rozkładzie mer&oacute;w wzdłuż łańcucha i jednorodnych pod względem budowy wszystkich łańcuch&oacute;w jest opanowanie metod kontrolowanej polimeryzacji. Opracowanie żyjących i pseudożyjących system&oacute;w polimeryzacyjnych otworzyło możliwość syntezy kopolimer&oacute;w o wąskim rozrzucie ciężar&oacute;w cząsteczkowych i wąskim rozrzucie kompozycji mer&oacute;w w makrocząsteczkach. W warunkach szybkiego inicjowania i braku proces&oacute;w zakończenia oraz przenoszenia łańcucha wszystkie makrocząsteczki w środowisku polimeryzacyjnym rosną r&oacute;wnocześnie. Koniec wzrostu makrocząsteczek następuje w momencie celowego przerwania polimeryzacji poprzez dodanie odczynnika zakańczającego. W tych więc warunkach skład makrocząsteczek i ich struktura są jednolite. R&oacute;żnice w reaktywności monomer&oacute;w prowadzą do gradientowego rozkładu mer&oacute;w w łańcuchach, gdyż monomer o większej reaktywności preferencyjnie wbudowuje się w łańcuch. Jego &bdquo;stężenie&rdquo; w łańcuchu maleje w miarę wzrostu makrocząsteczki, gdyż szybciej wyczerpuje się on w układzie. Wielkość gradientu zależy od wsp&oacute;łczynnik&oacute;w reaktywności, kt&oacute;re są funkcją temperatury i budowy aktywnego centrum. </font></p>
<div align=""justify"" style=""font-family: Times New Roman;""><font size=""3"">&nbsp;&nbsp;&nbsp; Literatura na temat tego typu kopolimer&oacute;w jest stosunkowo uboga. Większość prac dotyczy gradientowych kopolimer&oacute;w organicznych, otrzymywanych gł&oacute;wnie metodami kontrolowanej polimeryzacji rodnikowej [1,5-9]. Są także nieliczne doniesienia o kopolimerach gradientowych otrzymanych metodą polimeryzacji anionowej [10,11] i kationowej [12]. </font></div>
<p align=""center"" style=""font-family: Times New Roman;""><font size=""3""><img alt="""" src=""/OSFImageLoader.do?idImageDB=8200"" /><br /></font></p>
<p align=""center"" style=""font-family: Times New Roman;""><font size=""3"">Rys. 1. Schematyczne przedstawienie łańcucha kopolimeru gradientowego; czarne k&oacute;łka symbolizują monomer A, białe - monomer B.</font></p>
<p align=""center"" style=""font-family: Times New Roman;""><font size=""3""><img src=""/OSFImageLoader.do?idImageDB=8082"" alt="""" /></font></p>
<p align=""center"" style=""font-family: Times New Roman;""><font size=""3""></font></p>
<div align=""justify"" style=""font-family: Times New Roman;""><font size=""3"">Rys. 2. Przykładowe rozkłady gęstości grup funkcyjnych wzdłuż łańcucha</font></div>
<div align=""justify"" style=""font-family: Times New Roman;""><font size=""3"">&nbsp;</font></div>
<div align=""justify"" style=""font-family: Times New Roman;""><font size=""3"">&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; W dziedzinie polisiloksan&oacute;w jedyne prace na temat kopolimer&oacute;w o gradientowej budowie łańcucha pochodzą z naszego zespołu [13-16]. Jedyną znaną dotychczas metodą syntezy kopolimer&oacute;w siloksanowych o kontrolowanym ciężarze cząsteczkowym i niskim rozrzucie ciężar&oacute;w cząsteczkowych jest anionowa polimeryzacja z otwarciem pierścienia cyklotrisiloksan&oacute;w. Wykorzystaliśmy tę metodę do syntezy polisiloksan&oacute;w gradientowych. We wcześniejszych pracach kopolimery te były wykorzystane jako reaktywne bloki do syntezy złożonych architektur makrocząstecz...",07/01/29,23,Siloksanowe kopolimery gradientowe,"kopolimery gradientowe, polisiloksany, polimeryzacja anionowa, kinetyka kopolimeryzacji",183
64,3240,opisProjektuStanWiedzy,"<br />Otworowe wymienniki ciepła i kolektory słoneczne należą do nowoczesnych technologii umożliwiających pozyskiwanie energii cieplnej. Ich rozw&oacute;j jest jak najbardziej wskazany z ekologicznego punktu widzenia i jednocześnie bardzo dobrze wpisuje się w politykę energetyczną Polski do roku 2020 w zakresie uzyskiwania 20% energii z odnawialnych źr&oacute;deł. Obecna sytuacja Polski, wobec zobowiązań międzynarodowych, nie jest korzystna i należy zdecydowanie powiększyć możliwości otrzymywania dodatkowej energii z rozwiązań alternatywnych do typowej energetyki opartej o źr&oacute;dła klasyczne. Zaproponowane rozwiązanie, zdaniem autor&oacute;w wniosku, znajdzie szerokie zastosowanie w praktyce, i to w&nbsp; obiektach użyteczności publicznej jak i w obiektach prywatnych.<br />Wnioskodawcy od wielu lat zajmują się problematyką pozyskiwania ciepła Ziemi. Zajmują się także badaniami nad wykonywaniem i eksploatacją otworowych wymiennik&oacute;w ciepła w układach z pompami ciepła.<br />Dzięki wdrożeniu automatycznych układ&oacute;w uzyska się inteligentny system zarządzania i sterowania pracą układu.<br />Na podstawie praktycznej instalacji otworowych wymiennik&oacute;w ciepła wraz z pompami ciepła i kolektorami słonecznymi oraz modelowania układ&oacute;w zostanie opracowana metodyka doboru techniki i technologii wykonywania otworowych wymiennik&oacute;w ciepła uzupełnionych o kolektory słoneczne w zależności od charakterystyki energetycznej odbiorcy.<br />Wykonywane w praktyce instalacje są to instalacje pomp ciepła z wymiennikami otworowymi pracującymi w trybie pozyskiwania ciepła oraz instalacje kolektor&oacute;w słonecznych wykonywane rozdzielnie. Połączenie obydwu typ&oacute;w źr&oacute;deł ciepła z wykorzystaniem magazynowania ciepła (odbudowywania zasob&oacute;w) w g&oacute;rotworze umożliwi zmniejszenie rozmiar&oacute;w kosztownych instalacji.<br />Wykonanie projektu zweryfikuje utarte poglądy, że instalacje grzewcze z pompami ciepła z wymiennikami otworowymi są nieopłacalne nawet w długim okresie czasu.",09/07/31,45,Opracowanie zintegrowanego systemu otworowych wymienników ciepła i kolektorów słonecznych w aspekcie poprawy efektywności gospodarowania ciepłem w górotworze,"górotwór, otwory wiertnicze, otworowe wymienniki ciepła, odwierty naftowe, czysta energia, pompy ciepła, magazynowanie energii, kolektory słoneczne",200
65,3251,opisProjektuStanWiedzy,"<p>Renta gruntowa jest pojęciem ekonomicznym powszechnie znanym w teorii ekonomii i ekonomice gospodarstw rolnych. Znany jest fakt uzyskiwania wyższych dochod&oacute;w przez rolnik&oacute;w posiadających lepszej jakości ziemię przy zastosowaniu takiego samego poziomu nakład&oacute;w. Jest to doch&oacute;d związany z posiadaniem ziemi o r&oacute;żnej jakości. Renta gruntowa, z powodu sztywności podaży ziemi, jest zależna od wartości produkt&oacute;w, jakie wytwarza. Na jej wysokość wpływa m.in. atrakcyjność położenia, proporcja pomiędzy popytem i podażą itp. Renta gruntowa jest dochodem, kt&oacute;ry otrzymuje właściciel ziemski, a wynika ona z r&oacute;żnic w urodzajności gleby i położenia do rynk&oacute;w zbytu. Ceny płod&oacute;w rolnych wyznaczają społeczne koszty produkcji, w skład kt&oacute;rych wchodzi przeciętna stopa zysku na najmniej urodzajnych gruntach. Podstawą rozszerzenia upraw na ziemie najmniej urodzajne jest działanie prawa malejącej urodzajności gleby, a także ograniczonej ilości urodzajnych gleb. W normalnych warunkach popyt na produkty rolne rośnie stosunkowo szybko (wzrost liczby ludności, wzrost dochod&oacute;w), a postęp techniczny może tylko zahamować tempo działania prawa malejącej użyteczności gleby, ale nie może go wyeliminować. Przedmiotem zainteresowania niniejszego projektu jest zasadniczo renta r&oacute;żniczkowa I pozostająca w związku z r&oacute;żną urodzajnością ziemi i r&oacute;żnym położeniem działek rolnych w stosunku do ośrodk&oacute;w konsumpcji. Renta r&oacute;żniczkowa II związana z r&oacute;żną wydajnością kolejnych porcji nakład&oacute;w kapitałowych na tej samej powierzchni ziemi, ze względu na statyczny charakter badań nie będzie w obszarze zainteresowań autora.</p>",07/01/29,23,Badania nad rentą gruntową w Małopolsce,"renta gruntowa, rolnictwo, Małopolska",173
66,3275,opisProjektuStanWiedzy,"<font size=""3""><span style=""font-family: Times New Roman;"">&nbsp;&nbsp;&nbsp; Wiedza i literatura fachowa dotycząca makroskopowych pr&oacute;bek ciekłokrystalicznych (mezogennych) jest obecnie bardzo rozległa. Poważniejsze badania makroskopowych materiał&oacute;w ciekłokrystalicznych prowadzone są bowiem od lat dwudziestych ubiegłego wieku. Intensywne badanie własności warstw ciekłokrystalicznych, o grubości od milimetr&oacute;w do mikron&oacute;w (lata osiemdziesiąte/dziewięćdziesiąte XX w.) doprowadziły do spektakularnych zastosowań tych materiał&oacute;w (LCD, HDTV,....).&nbsp; Ostatnia dekada to rozpoczęcie prac doświadczalnych, teoretycznych i modelowania komputerowego własności ultracienkich (nanoskala) warstw mezogennych. Warstwy takie mogą znaleźć zastosowania w nanotechnologii, np. przy nowej generacji wyświetlaczy o minimalnym poborze mocy i niewielkiej wadze (nanorurki bardzo lekkie). Nasz projekt wpisuje się w ten nowy nurt badań nanowarstw ciekłokrystalicznych. Projekt dostarczy nowej, nieznanej jeszcze wiedzy.</span></font>",07/01/31,23,Badanie ultracienkich warstw ciekłokrystalicznych otaczających nanorurki węglowe - symulacje komputerowe,"warstwy ciekłokrystaliczne, nanorurki węglowe, symulacje komputerowe MD i MC, parametry porządku, absorpcja i rozpraszanie światła",173
67,3281,opisProjektuStanWiedzy,"<div style=""text-align: justify;"">
<div style=""text-align: justify;"">  </div>
<div style=""text-align: justify;""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">  <font size=""3""><span style=""font-family: Arial;""><span style=""font-family: Times New Roman;"">Budowa muszli mięczaków była badana początkowo mikroskopem optycznym (Bøggild 1930). Dopiero jednak użycie mikroskopu elektronowego, dyfrakcji rentgenowskiej i cyklotronu umożliwiło znaczący postęp w tym zakresie (Hedegaard, Wenk 1998,</span></span></font></span><font size=""3""><span style=""font-size: 12pt; font-family: Times New Roman;""> Chateigner, Hedegaardb, Wenk 2000,<span style=""""> </span>DiMasi 2003).<span style=""""> I tak na przykład </span>mikroteksturalna analiza muszli niektórych mięczaków wykazała, że orientacja osi krystalograficznych<span style="""">  </span>kryształów węglanowych tworzących poszczególne warstwy nie jest przypadkowa. Kryształy tworzące poszczególne warstewki wykazują określone uporządkowanie wyrażające się jednym z kilku rodzajów symetrii opisujących to uporządkowanie (fig. 1); bywa ono charakterystyczne dla określonego rodzaju lub nawet gatunku mięczaków (Kocks et al. 1998, Chateigner, Hedegaardb, Wenk 2000).</span></font><br /><font size=""3""><span style=""font-size: 12pt; font-family: Times New Roman;""></span></font></div>
<p style=""text-align: center;""><font size=""3""><span style=""font-size: 12pt; font-family: Times New Roman;""><img alt="""" src=""/OSFImageLoader.do?idImageDB=3629"" /><br /></span></font></p>
<p style=""font-family: Times New Roman;""><span style=""font-size: 12pt;""></span></p>
<p style=""text-align: justify; font-family: Times New Roman;"" class=""MsoNormal""><font size=""3"">Fig. 1. Ułożenie kryształów aragonitu<span style="""">  </span>masy perłowej niektórych mięczaków na podstawie analizy położenia osi krystalograficznych: Z (lewa kolumna) i X (prawa kolumna) w: a – <em>Pinctada maxima</em>, b - <em><span style="""">Nerita polita</span></em><span style="""">, c - <em>Fragum fragum</em>, d - <em>Cypraea testudinaria</em>, e – <em>Helix pomatia</em>, f - <em>Conus leopardus</em>, g - <em>Nautilus pompilius, </em>h<em> - Haliotis cracherodi. </em>Kolumna: stopień koncentracji wychodni osi krystalograficznych od zera (barwa biała) do stu procent (barwa czarna) (</span><span style="""">Chateigner</span><span style="""">,</span><span style=""""> Hedegaard</span><span style="""">b</span><span style="""">, Wenk</span><span style=""""> 2000).</span></font></p>
<p style=""text-align: justify; font-family: Times New Roman;"" class=""MsoNormal"">  </p>
<p style=""text-align: justify; font-family: Times New Roman;"" class=""MsoNormal"">  </p>
<p style=""text-align: justify; font-family: Times New Roman;"" class=""MsoNormal"">  </p>
<p style=""text-align: justify;"" class=""MsoNormal"">  </p>
<p class=""MsoBodyText3"" style=""font-family: Times New Roman;""><font size=""3""><span style=""font-family: Times New Roman;"">   Prac referujących szczegóły mikrobudowy masy perłowej muszli mięczaków jest dotychczas stosunkowo niewiele, żeby przytoczyć ważniejsze, np. Bøggild 1930, Wenk 1965, Wilmot et al. 1992, Chateigner, Hedegaard, Wenk 2000, <span style=""""> </span>DiMasi 2003. W jednej z nich (Mutvei 1978, 1980) próbowano zaproponować nawet teorię wyjaśniającą powstawanie i rozmieszczenie płytek masy perłowej, ale jej konkluzja została całkiem niedawno całkowicie podważona (Hedegaard, Wenk 1998, Chateigner, Hedegaardb, Wenk 2000, </span></font><span style=""""><font size=""3""><span style=""font-family: Times New Roman;""> DiMasi 2003).</span><span style=""""><span style=""font-family: Times New Roman;""> Istniejące na ten temat opracowania rozpatrują mikroarchitekturę masy perłowej głównie pod kątem właściwości mechanicznych; problematyka właściwości optycznych uwarunkowanych mikrostrukturalnie podejmowana była dotychczas fragmentarycznie, np. </span></span></font></span><font size=""3""><span style=""color: rgb(35, 31, 32);"">S</span><span style=""color: rgb(35, 31, 32);"">now</s...",07/01/25,23,Efekt perłowy  w świetle badań masy perłowej uchowców,"gemmologia, efekt perłowy, masa perłowa, nanobiokompozyty, uchowce, mięczaki",173
68,3339,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""margin: 6pt 0cm 12pt 3.7pt; text-align: justify; text-indent: 32.3pt; line-height: 150%; font-family: Times New Roman;""><font size=""4"">Metoda czynnikowa wykorzystywana jest do tworzenia większości współczesnych projekcji demograficznych. Cechą charakterystyczną tego sposobu prognozowania stanu i struktury ludności jest odrębne ustalenie (prognozowanie) przebiegu zmian w rozrodczości, umieralności i migracjach. W tym celu wykorzystane mogą zostać różne metody – analogowa, heurystyczna, w oparciu o trend, czy modele przyczynowo-skutkowe. Niezależnie od wybranego narzędzia niezbędna jest znajomość dotychczasowego przebiegu badanych zjawisk, jak i znajomości ich uwarunkowań. Mnogość czynników oddziałujących na przebieg zmian w analizowanych procesach prowadzi do uzyskiwania przez różne ośrodki badawcze, czy naukowe, różnych rezultatów rachunków prognostycznych wykonywanych dla tych samych obszarów w analogicznych okresach (por. prognozy ludności dla Polski Głównego Urzędu Statystycznego [GUS 2004], ONZ [UN 2003] i <span style=""color: black;"">Środkowoeuropejskiego Forum Badań Migracyjnych [Bijak, Kupiszewski, Kicinger 2004]</span>, prognozy dla Europy ONZ [UN 2003], Eurostat, European Association for Population Studies [1999]). Dokładność prognoz zależy także od jakości danych o stanie i strukturze ludności w momencie wyjściowym i mimo że w tym względzie uzyskano znaczącą poprawę, to jakość informacji np. o odnotowywanych ruchach migracyjnych bywa niewystarczająca [Clarke 1998]. </font></p>
<p class=""MsoNormal"" style=""margin: 6pt 0cm 0.0001pt 3.6pt; text-align: justify; text-indent: 32.4pt; line-height: 150%; font-family: Times New Roman;""><font size=""4""><span style=""""> </span>Najczęściej dokładność prognoz demograficznych weryfikowana jest ze względu na zgodność przewidywanego i rzeczywistego stanu liczebnego [por. Gawryszewski 2005]. W wyniku nałożenia się błędów w określaniu przebiegu poszczególnych procesów demograficznych może wystąpić sytuacja, iż prognozowana liczba ludności pokryje się z rzeczywistą. Wówczas o poprawności wykonanych rachunków prognostycznych świadczyć będzie zgodność rzeczywistej i prognozowanej struktury ludności według płci i wieku oraz innych cech, na których należy się opierać przy ocenach projekcji. Przydatność badań nad przyczynami błędów popełnianych w projekcjach demograficznych w dowolnej skali nie budzi wątpliwości.</font></p>",07/01/24,23,Prognozowanie demograficzne - historia i metodologia polskich prognoz demograficznych,"demografia, prognozowanie demograficzne",173
69,3351,opisProjektuStanWiedzy,"<p align=""justify"">Rozw&oacute;j informatyki kwantowej zaowocował wieloma zaskakującymi wynikami, Z pewności można do nich&nbsp;powstanie kwantowej &nbsp;teorii gier [1-4]. &nbsp;Szybko zrozumiano,&nbsp;że kwantowa teoria gier jest istotnym składnikiem&nbsp;kwantowej kryptografii i kwantowej informatyki. Podano r&oacute;wnież&nbsp; argumenty świadczące za tym, że strategie kwantowe mogą być wykorzystywane przez&nbsp; niekt&oacute;re organizmy żywe [5], ludzi, a nawet grupy społeczne [6-7]. Z punktu widzenia fizyka, kwantowa rozgrywka, jeżeli kiedykolwiek ma przestać być &nbsp;problemem czysto akademickim, wymaga rozwiązania szeregu zagadnień &nbsp;technicznych i technologicznych niezbędnych dla jej implementacji.&nbsp; Bezpieczeństwa rozgrywki, korygowania błęd&oacute;w oraz problemem przygotowania i transmisji układ&oacute;w fizycznych tworzących ,,kwantową planszę do gry&rsquo;&rsquo;, implementacji startegii&nbsp;oraz odpowiednich procedur pomiarowych niezbędnych do oceny stanu gry i jej wyniku są typowymi i trudnymi problemami kwantowego przetwarzania informacji. Przy analizie tych zagadnień pomocna będzie tu analiza już istniejących protokoł&oacute;w korygujących błędy, wymiany i uzgadniania informacji, roli ,,kwantowej pamięci&rsquo;&rsquo; oraz metod dystrybucji i przechowywania stan&oacute;w kwantowych. Zdefiniowanie oraz efektywny opis najprostszych, możliwie uniwersalnych składnik&oacute;w (ang. universal primitives), niezbędnych w implementacji będzie jednym z gł&oacute;wnych cel&oacute;w projektu.<br />Teoria gier jest ważnym działem matematyki stosowanej, mającym liczne zastosowania w często odległych od siebie dziedzinach nauki i techniki (ekonomii, biologii, ekologii oraz psychologii). Klasyczna (niekwantowa) matematyczna teoria gier rozwijana jest od wielu lat, a literatura dotycząca tego zagadnienia w postaci monografii oraz artykuł&oacute;w naukowych jest wyjąkowo bogata, za jednego z jej tw&oacute;rc&oacute;w uważany jest J. von Neumann, kt&oacute;ry jest także tw&oacute;rcą matematycznego formalizmu teorii kwant&oacute;w. <br />W ostatnim okresie, w związku z rozwojem informatyki kwantowej, pojawiło się mn&oacute;stwo &nbsp;prac dotyczących opisu gier kwantowych. W najprostszym przypadku dwu-osobowej statycznej gry kwantowej dwaj gracze&nbsp; wykorzystując&nbsp; operacje kwantowe&nbsp; na pojedynczych qubitach zmieniają stan kwantowy przyporządkowanych im&nbsp; qubit&oacute;w w celu maksymalizacji odpowiednio zdefiniowanych miernik&oacute;w użyteczności gry. W każdej chwili stan gry kwantowej jest stanem kwantowym. Zwykle zakłada się, że&nbsp; gracze znają zbi&oacute;r strategii dopuszczalnych i w nie znają aktualnie wybranej przez niego strategii (taktyki). <br />W opisie matematycznym statycznej gry kwantowej wykorzystuje się oznaczenia i pojęcia związane bezpośrednio z mechaniką kwantową oraz rachunkiem macierzowym i teorią przestrzeni hilberta, a w szczeg&oacute;lności z operatoramii unitarnymi [17]. <br />Najważniejszymi osiągnięciami algorytmiki kwantowej są algorytm poszukiwań Grovera [18]&nbsp;i algorytm faktoryzacji Shora [19]. Na świecie trwają poszukiwania nowych metod projektowania algorytm&oacute;w kwantowych. Jedną z takich metod może być zastosowanie kwantowej teorii gier. Zostało pokazane, że algorytm Grovera może być interpretowany jako szczeg&oacute;lny przypadek układu dw&oacute;ch gier kwantowych [20]. Można zatem podejrzewać, że uog&oacute;lnienie pewnych gier kwantowych doprowadzi do stworzenia nowych użytecznych algorytm&oacute;w kwantowych. Jednym z cel&oacute;w niniejszego projektu jest stworzenie zaplecza informatycznego i fizycznego&nbsp;do takich poszukiwań oraz ich rozpoczęcie [21-22]. Nie mniej ważne wydają się być potencjalne komercyjne zastosowania - wg raportu Commercial Prospects for Quantum Information Processing (<a target=""_blank"" href=""http://www.qipirc.org/files/Commercial%20Prospects%20for%20QIP%20v1.pdf"">http://www.qipirc.org/files/Commercial%20Prospects%20for%20QIP%...",07/01/22,23,Implementacja i potencjalne zastosowania gier kwantowych,"gry kwantowe, informatyka kwantowa, przetwarzanie informacji, splątanie, pomiar",173
70,3356,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.4pt; LINE-HEIGHT: 200%""><font size=""2"">W 1996 roku w Journal of Periodontology prof. Genco opublikował pracę podsumowującą wiedzę na temat czynnik&oacute;w ryzyka w periodontologii. Tezą artykułu podsumowującego wyniki badań w&oacute;wczas przeprowadzanych było stwierdzenie, iż w zapoczątkowaniu i rozwoju choroby przyzębia niebagatelną rolę pełni zakł&oacute;cenie prawidłowej odpowiedzi odpornościowej. Cechy modyfikujące potencjał immunologiczny gospodarza nazwano czynnikami ryzyka. Zalicza się do nich m.in. czynnik genetyczny. Pełni on podstawową rolę w regulowaniu odpowiedzi immunologicznej, jako że poprzez ekspresję fenotypową działa od momentu połączenia gamet rodzic&oacute;w aż do śmierci. </font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; LINE-HEIGHT: 200%""><font size=""2"">Badania przeprowadzone w ostatnich latach dowiodły genetycznego uwarunkowania pacjent&oacute;w do wystąpienia choroby przyzębia. Przełomem okazały się przeprowadzane w ostatnich dw&oacute;ch dekadach badania nad ludzkim genomem (Human Genome Project).</font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; LINE-HEIGHT: 200%""><font size=""2"">Czynnikiem poddanym szczeg&oacute;lnej uwadze była jedna z cytokin wydzielana przez makrofagi - IL-1. Działanie jej związane jest z zapoczątkowaniem i podtrzymaniem odpowiedzi zapalnej. Stan zapalny i destrukcja tkanki łącznej to podstawowe składowe patogenezy chor&oacute;b przyzębia, a IL-1 pełni w tych procesach kluczową rolę. Z tego powodu położono szczeg&oacute;lny nacisk na zbadanie polimorfizmu genowego IL-1. Stwierdzono zależność między genami IL-1A i IL-1B (kodującymi odpowiednio IL-1<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">a</span></span> i IL-1<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">b</span></span>), a stopniem zaawansowania zapalenia przyzębia. Wynika to z polimorfizmu genowego. Czynnik kodujący (allel) najczęściej występujący w populacji oznacza się jako allel 1, pozostałe (w kolejności): allel 2, allel 3 itd. Badania wykazały, że allele 2 kodujące IL-1<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">a</span></span> i IL-1<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">b</span></span> są &ndash; w przypadku ich wsp&oacute;lnego występowania, czyli u homozygot względem allelu 2 &ndash; powiązane ze znamiennie wyższą produkcją tych cytokin w odpowiedzi na czynnik bakteryjny. Wg Kornmana i wsp. niepalące osoby w wieku 40-60 lat z takim genotypem miały 18,9 &ndash; krotnie wyższe prawdopodobieństwo wystąpienia utraty kości, co tożsame jest z zaawansowaną chorobą przyzębia. DiGiovine i wsp. badali zależność między genotypem IL-1<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">b</span></span>, a poziomem produkcji tej cytokiny. Wykazano, że średnia ilość produkowanej przez makrofagi interleukiny 1 w przypadku, gdy pacjenci są homozygotami względem allelu 1, wynosi 5,2 ng/ml. Jeśli pacjenci są heterozygotami (r&oacute;żne allele w obu lokacjach), średnia produkcja IL-1<span style...",07/01/29,23,Wpływ genotypu i składu bakteryjnego kieszonki na stan kliniczny przyzębia,"genetyka, bakterie, choroba przyzębia",173
71,3417,opisProjektuStanWiedzy,"<p align=""left""><font size=""2"">Obecnie do detekcji i pomiaru koncentracji gaz&oacute;w zawartych w powietrzu stosowane są metody działania miejscowego, tzw. in situ oraz metody detekcji zdalnej. Z dostępnej literatury wynika, że spośr&oacute;d metod punktowych stosowanych bezpośrednio w miejscu pomiaru, często stosowane są: <br />- metody chemiczne,<br /></font><font size=""2"">- metody biochemiczne,<br /></font><font size=""2"">- oraz metody fizyczne i fizykochemiczne, do kt&oacute;rych można zaliczyć między innymi metody jonizacyjne, spektrometrii masowej, chromatografii, spektrofotometryczne zar&oacute;wno emisyjne jak i absorpcyjne, fotoakustyczne, elektrochemiczne, oraz biosensory itd.</font></p>
<p align=""left""><font size=""2"">Natomiast wśr&oacute;d metod&nbsp;zdalnych można wyr&oacute;żnić:<br /></font><font size=""2"">- metody aktywne &ndash; LIDAR (ang.:<em> Light Detection and Ranging</em>) &ndash; analizowany jest sygnał laserowy po jego oddziaływaniu z atmosferą (rozproszeniowy, absorpcyjny &ndash; np. DIAL, DOAS (ang.: <em>Differential Optical Absorption Systems</em>), fluoroscencyjny),<br /></font><font size=""2"">- metody pasywne &ndash; LOPAIR (ang.: <em>Long Path Infrared</em>) &ndash; analiza widma promieniowania w podczerwieni emitowanego przez otoczenie.</font></p>
<p align=""justify""><font size=""2"">W dostępnych na rynku przenośnych miernikach i sensorach NO<font size=""1"">2</font> stosuje się czujniki elektrochemiczne, kt&oacute;re umożliwiają pomiar stężeń na poziomie kilkuset ppb. [4 &ndash; 11]. Większą czułość posiadają mierniki stacjonarne [12 &ndash; 14] jednak ich wadą, poza gabarytami, jest ich wysoka cena. Ponadto w przypadku tego typu urządzeń należy wziąć pod uwagę znaczne koszty eksploatacji, zwłaszcza przy zastosowaniu laser&oacute;w innych niż p&oacute;łprzewodnikowe. </font></p>
<p align=""justify""><font size=""2""><font size=""2"">Jedną z najnowszych i najczulszych metod detekcji gaz&oacute;w jest spektroskopia strat we wnęce optycznej, określana akronimem CRDS (ang. <em>Cavity Ring Down Spectroscopy</em>). Metoda ta należy do&nbsp;najmłodszych absorpcyjnych metod spektroskopowych. Została ona zaproponowana przez J.M. Herbelina na początku lat 80 &ndash; tych XX w. do wyznaczania wsp&oacute;łczynnik&oacute;w odbicia zwierciadeł [15,16], natomiast w 1988 roku O&rsquo;Keefe i Deacon [17] przeprowadzili doświadczenie, w kt&oacute;rym zastosowali metodę CRDS do wyznaczenia wsp&oacute;łczynnika absorpcji gazu wypełniającego wnękę. Był to pierwszy eksperyment przeprowadzony techniką CRDS z zastosowaniem lasera impulsowego.</font></font><font size=""2""><font size=""2"">&nbsp;</font></font></p>
<p align=""center""><img style=""WIDTH: 574px; HEIGHT: 203px"" height=""616"" alt="""" width=""1815"" src=""/OSFImageLoader.do?idImageDB=4555"" /></p>
<p align=""center""><font size=""2""><font size=""2"">Rys. 1. Idea pracy metody CRDS</font></font></p>
<p align=""justify""><font size=""2""><font size=""2"">W metodzie&nbsp;CRDS impuls światła zostaje wprowadzony do wnęki optycznej, zbudowanej z dw&oacute;ch zwierciadeł o dużym wsp&oacute;łczynniku odbicia R (rys. 1.). Impuls świetlny wprowadzony do wnęki przez jedno ze zwierciadeł, ulega w jej wnętrzu wielokrotnemu odbiciu. Po każdym odbiciu część światła opuszczającego wnękę jest rejestrowana przez detektor. Amplitudy kolejnych impuls&oacute;w opuszczających wnękę ulegają zmniejszeniu. Szybkość zaniku natężenia światła we wnęce zależy od wsp&oacute;łczynnika odbicia zwierciadeł, strat dyfrakcyjnych oraz od ekstynkcji, czyli rozpraszania i absorpcji światła zachodzącej w gazie wypełniającym wnękę. Wyznaczając szybkość zaniku promieniowania &tau; we wnęce wypełnionej absoberem&nbsp;oraz bez absorbera - &tau;<sub>o</sub>&nbsp;można określić stężenie gazu N,</font></font></p>
<p align=""center""><img style=""WIDTH: 141px; HEIGHT: 57px"" height=""68"" alt="""" width=""164"" src=""/OSFImageLoader.do?idImageDB=4557"" />,</p>
<p align=""left""><font size=""2"">gdzie c oznacza prędkość światła, natomiast &sigma; - p...",07/01/25,23,Zwiększenie czułości czujnika NO2 działającego w oparciu o metodę CEAS,"detekcja gazów, czujnik NO2, CRDS, CEAS",183
72,3430,opisProjektuStanWiedzy,"<p style=""text-align: justify; text-indent: 18pt;"" class=""MsoNormal""><span style=""font-size: 11pt; font-family: Arial;"">Na podstawie przeprowadzonej analizy literatury oraz zgodnie z dostępną wiedzą z tego zakresu można stwierdzić, iż podjęty problem, proponowane metody jego rozwiązania oraz możliwe zastosowania są całkowicie nowe zarówno w kraju jak i na świecie. Jest to związane z istniejącą dopiero od niedawna dostępnością nowoczesnego sprzętu badawczego do akwizycji, przetwarzania danych oraz weryfikacji otrzymanych wyników. <o:p></o:p></span></p>
<p style=""text-align: justify; text-indent: 18pt;"" class=""MsoNormal""><span style=""font-size: 11pt; font-family: Arial;"">Praca będzie stanowić kontynuację oraz praktyczne zastosowanie przeprowadzonych badań własnych z wybranej dziedziny.<o:p></o:p></span></p>",07/01/03,23,"Numeryczna i elastooptyczna metoda wyznaczania i weryfikacji naprężeń przy uszkodzeniach LeFort’a typu I,II,III","złamania LeFort’a, badania elastooptyczne, metody pomiarowe, CAD/CAM, Reverse Engineering, Rapid Prototyping",173
73,3443,opisProjektuStanWiedzy,"<p>Dotychczas nie powstało żadne opracowanie naukowe, kt&oacute;re miało na celu sklasyfikowanie system&oacute;w e-learning. Z punktu widzenia historycznego, tematyka kształcenia na odległość nie jest dyscypliną nową.&nbsp;Patrząc jednakże na e-kształcenie, kt&oacute;re wykorzystuje wszystkie dostępne media elektroniczne, a gł&oacute;wnie Internet - mamy przykład nowego trendu w nauczaniu, kt&oacute;ry pojawił się z chwilą popularyzacji komputer&oacute;w i sieci teleinformatycznych. Dostępność literatury, badań naukowych, jak i&nbsp;powstawanie ośrodk&oacute;w edukacyjnych (czy też samodzielnych jednostek)&nbsp;prowadzących zajęcia&nbsp;i szkolenia dydaktyczne w tej formie nauczania, pokazuje szerokie zainteresowanie tą problematyką.&nbsp;E-learning staje się&nbsp;dyscypliną naukową, kt&oacute;ra stawia przed nauczycielami, studentami, uczelniami, ośrodkami edukacyjnymi, a nawet przedsiębiorstwami nowe perspektywy&nbsp;kształcenia jak i zagrożenia (chociażby problemy z odpowiednim przygotowaniem pod względem metodycznym - kurs&oacute;w online; czy bezpieczeństwo zarządzania samą platformą). </p>
<p>Opracowanie klasyfikacji istniejących system&oacute;w e-learningowych pozwoli na usystematyzowanie i przedstawienie w czytelnej formie dostępnych platform e-nauczania, ich narzędzi, zgodności ze standardami, element&oacute;w służących do zarządzania treścią, użytkownikiem czy metod komunikacji (a-)synchronicznej w badanych projektach. Otrzymana klasyfikacja pozwoli na stworzenie Zintegrowanego Modelu E-learning,&nbsp;mogącym stać się wzorcowym modelem, kt&oacute;ry byłby odniesieniem do system&oacute;w klasy LMS/LCMS.</p>
<p>Analiza stanowić będzie źr&oacute;dło wiedzy dla wszelkich instytucji edukacyjnych, podmiot&oacute;w gospodarczych chcących wdrożyć optymalne dla swoich potrzeb, zasob&oacute;w i możliwości rozwiązanie e-learning, kt&oacute;re będzie stanowiło pewien standard reprezentacji zawartości dydaktycznej. </p>
<p>Dotychczas w Polsce nie przeprowadzono takich badań związanych z platformami e-learningowymi. Dotychczasowy stan wiedzy przedstawia formy, zalety, korzyści czy możliwości jakie stwarzają platformy e-learning&nbsp;w edukacji, nie przedstawiają&nbsp; jednak dokładnie samych platform. Nie istnieje analiza por&oacute;wnawcza ani klasyfikacja system&oacute;w e-learning (komercyjnych oraz open source).</p>",07/01/29,23,Klasyfikacja porównawcza systemów e-learningowych,"e-learning, systemy CMS, LMS, LCMS, nauczanie na odległość, szkolenia blended learning",173
74,3451,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"" size=""4"">Ponieważ terapia narracyjna jest stosunkowo nowym podejściem terapeutycznym, istnieje niewiele opracowań literatury pięknej wykonanych według tej teorii. Dodatkowo, większość prac dotyczących związk&oacute;w między szaleństwem a literaturą opiera się na teoriach krytycznych wywodzących się z humanistyki, nie z psychiatrii i nauk jej pokrewnych. Tak więc, badacze traktują pojęcie choroby psychicznej jako konstrukt społeczny, a nie obiektywnie istniejące zjawisko medyczne. W znacznej większości, krytycy i biografowie Frame i Woolf, &nbsp;powołując się na teksty anty-psychiatr&oacute;w (Foucaulta, Lainga i Szasza) oraz feministycznych historyk&oacute;w czy teoretyk&oacute;w psychiatrii (Showalter, Chestler, Ussher), negują fakt, iż autorki były w og&oacute;le chore. Uważają oni, iż przyznanie autorkom statusu os&oacute;b chorych psychicznie może zagrozić prestiżowi ich pracy. M&oacute;j projekt badawczy opiera się&nbsp;epistemologicznie zar&oacute;wno na teoriach krytycznych jak i wiedzy medycznej, dzięki czemu cechuje go większy obiektywizm. </font></p>
<p align=""justify""><font face=""Times New Roman"" size=""4"">Dodatkowo, mimo, iż wiele osob zauważało zbieżność między Woolf a Frame, nie istnieje żadna praca naukowa dokładnie analizująca podobieństwa w ich stylu, obrazowaniu i tematyce. Pod tym względem projekt jest całkowicie nowatorski.</font></p>",07/01/11,23,The Search for the Self: writings of Virginia Woolf and Janet Frame.   W poszukiwaniu własnego Ja: twórczość Virginii Woolf i Janet Frame,"madness, self, identity, narrative therapy (szaleństwo, Ja, tożsamość, terapia narracyjna)",173
75,3513,opisProjektuStanWiedzy,"Nieunikniony kryzys światowej energetyki opartej na surowcach konwencjonalnych, takich jak ropa naftowa, gaz ziemny czy węgiel, wynikający ze skończoności ich zasob&oacute;w w skorupie ziemskiej jak i negatywnych skutk&oacute;w dla środowiska z powodu ich nieodwracalnego przetwarzania, wymusza poszukiwanie alternatywnych rozwiązań. Klasyczne źr&oacute;dła winny zatem być zastąpione takimi, kt&oacute;re zapewniają bezpieczeństwo dla otoczenia, a ich zasoby wystarczą na długi okres. Takim zasobnikiem energii z pewnością jest Słońce. Skutki emisji energii słonecznej w przestrzeń kosmiczną na Ziemi przejawiają się światłem, ciepłem, wiatrem, pływami oceanicznymi, czy też jej kumulacją w roślinach bioenergetycznych. <br />W chwili obecnej na świecie najbardziej dynamicznie rozwija się energetyka związana z ruchami mas powietrza, ale tuż za nią podąża konwersja fotowoltaiczna i fototermiczna [1]. W dobie przystąpienia naszego kraju do Wsp&oacute;lnoty Państw Europejskich, ważną rzeczą jest nadążenie za dynamiką wdrażania rozwiązań alternatywnej energetyki r&oacute;wnież w Polsce. Problem badawczy zgłaszanego projektu skupiać się będzie zatem na opracowaniu technologii otrzymywania cienkowarstwowych ogniw słonecznych na bazie warstw lateralnych Si w oparciu o metodę epitaksji z fazy ciekłej (LPE), a w szczeg&oacute;lności techniki epitaksjalnego wzrostu lateralnego (ELO &ndash; epitaxial lateral overgrowth) przy wykorzystaniu krzemowych podłoży słabszej jakości ( a tym samym tańszych). <br />Technologia LPE jest najtańszym sposobem wytwarzania cienkich warstw p&oacute;łprzewodnik&oacute;w. Jej gł&oacute;wną zaletą jest prostota aparaturowa, pełny recykling używanych materiał&oacute;w oraz możliwość prowadzenia proces&oacute;w wzrostu przy obniżonych temperaturach w por&oacute;wnaniu do temperatury topnienia krzemu [2-6]. Dodatkowo przez zastosowanie selektywnego pokrycia krzemowych podłoży bazowych warstwą dielektryka (SiO2 lub Si3N4) możliwe jest uzyskanie cienkich warstw lateralnych o zdecydowanie lepszej jakości i morfologii, dzięki zmniejszonej generacji i propagacji defekt&oacute;w w ich strukturze krystalicznej. Uzyskiwanie warstw metodą epitaksjalnego wzrostu lateralnego (ELO) uniezależnia ich parametry od parametr&oacute;w podłoża, co daje możliwość wykorzystania w procesie technologicznym podłoży bazowych gorszej jakości, a więc uczynić proces znacznie tańszym [7-14]. Czynnik ekonomiczny w technologiach wytwarzania struktur ogniw słonecznych, obok ich efektywności, jest najważniejszym elementem utrzymującym silną konkurencję na rynku produkt&oacute;w fotowoltaicznych. Wstępne prace Zespołu dowodzą, ze cechy strukturalne i jakościowe p&oacute;łprzewodnikowych warstw lateralnych są silnie uwarunkowane parametrami technicznymi charakteryzującymi sam proces wzrostu. Istotną rzeczą jest nie tylko dobranie odpowiedniej szybkości chłodzenia roztwor&oacute;w, temperatury początkowej czy wreszcie czasu trwania procesu ELO, ale r&oacute;wnież rodzaj i spos&oacute;b przygotowania i geometrii otwartych okien krzemowych na powierzchni pokrytego dielektrykiem podłoża bazowego. <br /><br /><br /><br />Literatura: <br />[1] S.Pietruszko, Status of Photovoltaics (2004) and Recommendations for PV Development in EU N M&amp;CS. <br />[2] R. Kopecek, K. Peter, J. Hotzel, E. Bucher, Structural and electrical properties of silicon epitaxial layers grown by LPE on highly resistive monocrystalline substrates, Journal of Crystal Growth 208 (2000) 289-296. <br />[3] M.J. McCann, K.J. Weber, M. Petravic, A.W. Blakers, Boron doping of silicon layers grown by liquid phase epitaxy, Journal of Crystal Growth 241 (2002) 45-50. <br />[4] T. Ujihara, E. Kanda, K. Fujiwara, N. Usami, G. Sazaki, K. Nakajima, Effect of growth temperature on surface morphology and crystal quality of Si thin-film by liquid phase epitaxial growth technique, Proceedings of 17th European Photovoltaic Solar Energy Conference, Rome, Italy, 7-11 October 2002, vol. I, p. 408. <br />[5] T...",07/01/30,23,Badania procesów usprawniania wydajności konwersji fotowoltaicznej cienkowarstwowych baterii słonecznych z krzemu,"inżynieria środowiska, odnawialne źródła energii",173
76,3536,opisProjektuStanWiedzy,"<p align=""justify""><font size=""4"">Wsp&oacute;łczesne&nbsp;badania nad ceramiką naczyniową kultury bogaczewskiej koncentrują się wok&oacute;ł trzech zagadnień dotyczących: wypracowania metod opisu tej kategorii zabytk&oacute;w, podział&oacute;w typologicznych ceramiki odkrywanej na osadach i cmentarzyskach oraz datowania wydzielonych form. Konieczność intensyfikacji prac nad tą problematyką, była kilkakrotnie poruszana w literaturze, stanowiła też przedmiot dyskusji oraz konkluzję obrad trzech konferencji poświęconych wytw&oacute;rczości ceramicznej zachodniobałtyjskiego kręgu kulturowego: Ceramika zachodniobałtyjska od wczesnej epoki żelaza do początku ery nowożytnej (Białystok, maj 1997) oraz Ceramika zachodniobałtyjska. Nowe źr&oacute;dła i interpretacje (Białystok, wrzesień 2002) oraz Ceramika bałtyjska. Tradycje i wpływy (Białystok, wrzesień 2005). <br /><br />Brak jednoznaczności w określaniu poszczeg&oacute;lnych form naczyń, a także w wydzielaniu oraz nazywaniu poszczeg&oacute;lnych partii naczynia, oraz potrzeba uporządkowania tej terminologii widoczne są już po pobieżnym zapoznaniu się z literaturą przedmiotu. Autorzy opracowań materiałowych oraz syntez posługują się zr&oacute;żnicowanymi określeniami dotyczącymi tych samych części naczyń. Fakt ten oraz brak precyzyjnego zdefiniowania partii naczyń powoduje sytuację w kt&oacute;rej czytelnicy intuicyjnie, a przez to często mylnie interpretują intencję autora. Mniejszą uwagę zwracano dotychczas na inny problem opisu ceramiki. Jest nim przenoszenie nazw funkcjonalnych konkretnych form z zestawu naczyń osadowych na naczynia z cmentarzysk. Omawiane zagadnienia stały się tematem dyskusji zainicjowanej podczas obrad konferencji &bdquo;Ceramika zachodniobałtyjska od wczesnej epoki żelaza do początku ery nowożytnej&rdquo; (Białystok, 1997). Zwr&oacute;cono w&oacute;wczas uwagę, że panująca w literaturze &bdquo;dowolność terminologiczna&rdquo; stanowi poważne utrudnienie w wykorzystywaniu opracowań źr&oacute;dłowych. </font></p>
<p align=""justify""><font size=""4"">Analiza formalna była podstawą roboczych typologii naczyń zestawianych na potrzeby opracowania materiał&oacute;w ceramicznych z konkretnego stanowiska. Opracowanie typologii, kt&oacute;ra mogłaby stać się punktem odniesienia dla ceramiki z całego obszaru kultury bogaczewskiej wymaga dysponowania większym liczebnie zbiorem naczyń. Dla ceramiki pochodzącej z cmentarzysk tej kultury, pr&oacute;bę taką podjął P. Szymański. Omawiana typologia zawiera klasyfikację naczyń na grupy wydzielone na podstawie podobieństwa morfologicznego. Kolejnymi stopniami jest podział na odmiany i warianty. Za mankament tej klasyfikacji należy uznać: występujący w niekt&oacute;rych przypadkach brak czytelnych przesłanek do przyporządkowania naczyń do konkretnych odmian, używanie nazw przynależnych ceramice z osad (garnek, naczynie wazowate, misowate) w odniesieniu do naczyń pochodzących z cmentarzysk oraz niekonsekwentne stosowanie pojęć &bdquo;grupa&rdquo;, &bdquo;odmiana&rdquo;, &bdquo;wariant&rdquo;. Przedmiotem odrębnej analizy typologicznej była ornamentyka naczyń odkrywanych na cmentarzyskach kultury bogaczewskiej. Autor podjął też pr&oacute;bę ustalenia chronologii poszczeg&oacute;lnych grup zdobniczych. </font></p>
<p align=""justify""><font size=""4"">Istotnym elementem badań nad wytw&oacute;rczością ceramiczną są prace dotyczące konkretnych form naczyń lub określonych cech stylistycznych. W przypadku kultury bogaczewskiej jedyne tego typu opracowania poświęcone zostały kubkom typu Szwajcaria, tzw. kubasom. Do zagadnień związanych ze stylistyką naczyń kultury bogaczewskiej z p&oacute;źnego podokresu wpływ&oacute;w rzymskich i wczesnej fazy okresu wędr&oacute;wek lud&oacute;w nawiązał natomiast M. Karczewski formułując uwagi dotyczące rozprzestrzenienia się ornamentu palcowego. <br /><br />Znaczny wzrost liczby naczyń zar&oacute;wno z osad jak i cmentarzysk przyniosły badania wykopaliskowe prowadzone na stanowiskach kultury bogaczewskiej ...",07/01/25,23,"Styl ceramiczny kultury bogaczewskiej - geneza, typologia, oddziaływania",ceramika - typologia - stylistyka,173
77,3569,opisProjektuStanWiedzy,"<p style=""line-height: 150%; text-align: justify;"" class=""MsoNormal""><span lang=""PL"" style=""font-size: 11pt; line-height: 150%;""><o:p> </o:p></span></p>
<p style=""text-indent: 0.25in; line-height: 150%;"" class=""MsoNormal""><span lang=""PL"" style=""font-size: 11pt; line-height: 150%;"">W chwili obecnej rozpoznawanie i ocena ciężkości POChP opiera się w głównej mierze na spirometrycznych wskaźnikach FEV 1%FVC i FEV 1 oraz subiektywnych objawach klinicznych (GOLD, NICE). Według międzynarodowych towarzystw POChP jest ogólnoustrojową chorobą zapalną, która manifestuje się głównie w układzie oddechowym. Dominującymi komórkami zapalnymi w stabilnym okresie POChP są: makrofagi, limfocyty T z przewagą CD8<sup>+</sup> i neutrofile wraz z wydzielanymi przez siebie mediatorami: LTB<sub>4,</sub> IL-8, TNF-alfa oraz innymi, które biorą udział w zapaleniu neutrofilowym. Przyczynia się to do braku wskazań do stosowania glikokortykosteroidów w stabilnym okresie choroby. Zaostrzenia choroby różnią się profilem limfocytarno-cytokinowym [3]. Warunkuje to być może efekt kliniczny stosowania hormonów kory nadnerczy podczas zaostrzenia choroby. Ważnym dopełnieniem reakcji zapalnej jest zachwianie równowagi między aktywnością proteinaz i antyproteinaz. Jak pokazuje doświadczenie kliniczne dane uzyskiwane na podstawie w/w zmiennych są niewystarczające do sprawnego monitorowania pacjentów podczas zaostrzenia choroby. Dopiero od kilku lat możemy odnaleźć dowody mówiące o wpływie hyperinflacji na sprawność fizyczną pacjentów, chorujących na umiarkowane i ciężkie POChP [6]. Ze względu na wieloczynnikowy charakter zaburzeń oraz znaczną niejednorodność, osoby te stanowią trudny materiał badawczy. Jest to źródłem rozbieżności w wynikach wielu dobrze przeprowadzonych badaniach naukowych. W praktyce wykorzystywane są różne metody oceny sprawności wysiłkowej przez pacjentów z POChP np. 6 minutowy test chodu. Obserwujemy w nim poprawę uzyskiwanych wyników po zastosowaniu bromku tiotropium [7]. Z drugiej strony ze względu na dużą zależność od woli pacjenta i brak możliwości oceny innych parametrów wentylacji często wykorzystuje się testy wydolnościowe z użyciem cykloergometru rowerowego [5]. Dowodem na wieloczynnikowy charakter zaburzeń wentylacji u pacjentów z POChP są doniesienia <strong><span style=""font-weight: normal;"">Vogiatzis</span></strong> i wsp. [10], w swojej pracy autorzy wykazują lepszą tolerancję wysiłku przerywanego w porównaniu do ciągłego, u osób z POChP przy zbliżonym nasileniu hiperinflacji podczas odmiennych schematów treningowych. Udowodniono też, że nie każdy chory z POChP podczas wysiłku prezentuje hiperinflację [11]. Przemawia to za istnieniem innych czynników wpływających na stan kliniczny badanych.<o:p></o:p></span></p>
<p style=""line-height: 150%;"" class=""MsoNormal""><span lang=""PL"" style=""font-size: 11pt; line-height: 150%;"">Solidnie potwierdzony, choć niezbyt popularny wydaje się być fakt, że monitorowanie wskaźników hiperinflacji jest bardziej miarodajne i wiarygodne niż lansowana od lat wartość FEV 1 w stabilnym POChP [1]. Potwierdzają to również liczne dowody naukowe obrazujące wpływ leków rozkurczających oskrzela (B mimetyków i cholinolityków) na poprawę wskaźników dynamicznej hiperinflacji u pacjentów z POChP w stabilnym okresie choroby [1,4,5]. Nie tylko leki redukują dynamiczną hiperinflację, lecz również tlenoterapia ze wzrastającym FiO2 do 50% przyczynia się do zmniejszenia opisywanego zjawiska, przez zmniejszenie częstości oddychania, a w konsekwencji wydłużenie fazy wydechowej i poprawę wentylacyjnych właściwości płuc [8]. Z tym korzystnym z punktu widzenia klinicysty zjawiskiem mamy do czynienia również u pacjentów nie kwalifikowanych na podstawie ogólnie zaakceptowanych do tlenoterapii kryteriów gazometrycznych. Kolejnym, nie mniej ważnym sposobem walki z narastającym kalectwem wywołanym postępującą hiperinflacją pogarszającą sprawność w kolejnych stadiach choroby są specjalne intensywne programy treningowe. Dzięki temu ogr...",07/01/25,23,Dynamiczna hyperinflacja i nadciśnienie płucne jako modyfikowalne leczeniem wskaźniki warunkujące poprawę kliniczną u pacjentów z zaostrzeniem POChP,"POChP, zaostrzenie, dynamiczna hiperinflacja, nadciśnienie płucne, zapalenie, testy czynnościowe",183
78,3575,opisProjektuStanWiedzy,"<font size=""3""><span lang=""PL"" style=""mso-bidi-font-size: 12.0pt; mso-bidi-font-family: Arial""><span style=""mso-tab-count: 1"">
<p class=""MsoBodyText"" style=""MARGIN: 6pt 0in 0pt""><span lang=""PL"" style=""FONT-FAMILY: Arial; mso-ansi-language: PL""><span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Wieloetiologiczny charakter, mnogość mechanizm&oacute;w patofizjologicznych a także r&oacute;żny stopień zaawansowania niewydolności serca stanowią barierę postępowania leczniczego doprowadzając do&nbsp;progresji choroby (1,2). Jest ona wynikiem przede wszystkim postępującej przebudowy tkanek mięśnia sercowego (3). Na szczeg&oacute;lną uwagę w tym procesie zasługuje obecność przewlekłej dysfunkcji lokalnego układu krzepnięcia i fibrynolizy. Znajduje to swoje odzwierciedlenie w obecności stanu prokoagulacyjnego u chorych z niewydolnością serca (4). W badaniach własnych zaobserwowano między innymi podwyższone poziomy kompleksu T-ATIII w tej grupie chorych (5). Obecność zaburzeń układu krzepnięcia koreluje ściśle z obecnością pobudzenia immunologicznego na poziomie tkankowym oraz cechami immunologicznego uszkadzania kom&oacute;rek mięśniowych. Potwierdzeniem tego są wyniki badań własnych, w kt&oacute;rych stwierdzono ujemną korelację pomiędzy niską liczbą zaktywowanych makrofag&oacute;w a obecnością podwyższonych poziom&oacute;w marker&oacute;w uszkadzania kom&oacute;rek mięśniowych (6). Jednym z podstawowych mechanizm&oacute;w łączących procesy krzepnięcia i zapalenia jest zapalna indukcja trombina oraz czynnika tkankowego (7,8). Oba czynniki są przewlekle aktywowane między innymi w następstwie spadku aktywności trombomoduliny oraz przetrwałego wzrostu inhibitora plazminogenu (9). W konsekwencji, wzrost zar&oacute;wno naczyniowego jak i pozanaczyniowego poziomu fibryny doprowadza do następowego wł&oacute;knienia tkanek mięśnia sercowego. Ponadto, fibryna bezpośrednio stymuluje uwalnianie cytokin (IL-1&beta; oraz TNF&alpha;) przez kom&oacute;rki jednojądrzaste, w szczeg&oacute;lności szeregu monocyto-makrofagalnego (10). Ten mechanizm jest jednym z wielu wzajemnie się amplifikujących w przebiegu przewlekłej, patologicznej przebudowy mięśnia sercowego u chorych z niewydolnością serca (11,12). <o:p></o:p></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 6pt 0in 0pt; TEXT-INDENT: 35.3pt""><span lang=""PL"" style=""FONT-FAMILY: Arial; mso-ansi-language: PL"">Zmiany patologiczne w obrębie wsierdzia i warstwy mięśniowej podwsierdziowej są powszechną cechą morfologiczną obecną w przebiegu schorzeń układu krążenia (13). I tak przykładowo w materiale doświadczalnym wykazano, iż pod wpływem przewlekłego przeciążenia objętościowego zar&oacute;wno błona podstawna jak i warstwa łącznotkankowa ulega pogrubieniu, zawierając większą ilość wł&oacute;kien elastynowych w por&oacute;wnaniu z prawidłowym wsierdziem (14). Jednakże udział wsierdzia w rozwoju i progresji niewydolności serca pozostaje w sferze spekulacji i hipotez. Badania eksperymentalne z wykorzystaniem mikroskopii konfokalnej wykazały obecność zmienionego morfologicznie śr&oacute;dbłonka wsierdzia w przebiegu zastoinowej niewydolności serca (15). Zmianom tym towarzyszyły zmiany w warstwie łącznotkankowej pod postacią hiperplazji kom&oacute;rek o morfologii miofibroblast&oacute;w jaki i kom&oacute;rek gładkich, zawierających liczne wł&oacute;kna aktynowe. Rola tych kom&oacute;rek w progresji niewydolności serca pozostaje nieznana. Brak jest także danych dotyczących udziału samego śr&oacute;dbłonka wsierdzia w przebudowie warstwy łacznotkankowej w skład kt&oacute;rej wchodzi błona podstawna oraz warstwa wł&oacute;knisto-elastyczna wsierdzia. Szczątkowe są r&oacute;wnież dane dotyczące zaburzeń lokalnej hemostazy w obrębie zar&oacute;wno wsierdzia jak i warstwy podwsierdziowej. Zaburzenia te zostały najlepiej udokumentowane w przebiegu eozynofilowego zapalenia wsierdzia. Wykazano bowiem, liczne, drobne śr&oacute;dścienne skrzepliny w miejscach utra...",07/01/25,23,Rola śródbłonka wsierdzia i warstwy podwsierdziowej mięśnia sercowego w rozwoju skurczowej niewydolności serca,"wsierdzie, śródbłonek wsierdzia, niewydolność serca, kardiomiopatia rozstrzeniowa, biopsja endomiokardialna, histochemia, immunohistochemia, mikroskopia elektronowa, biologia molekularna",173
79,3585,opisProjektuStanWiedzy,"<p align=""justify"">Dotychczas ukazało się zaledwie kilka publikacji podejmujących problem porządk&oacute;w intranzytywnych w ramach kwantowej teorii gier. Ilościowa analiza porządk&oacute;w intranzytywnych stanowi oryginalne podejście w badaniach nad właściwościami gier kwantowych. Projekt stanowi pierwszą pr&oacute;bę opisu wpływu kwantyzacji gry na porządki intranzytywne. Wykazanie istnienia modelu, w kt&oacute;rym optymalne efekty mogą być uzyskane jedynie dzięki strategiom intranzytywnym stanowi wyrazisty przykład na potrzebę dalszych badań w tym kierunku i weryfikacji dotychczasowych pogląd&oacute;w uznających porządki intranzytywne za niepożądane.</p>",07/01/19,23,Wpływ kwantyzacji gry na porządki intranzytywne.,"teoria kwantów, gry kwantowe, kwantyzacja, strategie kwantowe, preferencje",173
80,3591,opisProjektuStanWiedzy,"<p align=""left"">Do chwili obecnej zostały zebrane i zbadane dość szeroko, zwłaszcza w publikacjach przedwojennych, dane statystyczne dotyczące składu narodowościowego mieszkańc&oacute;w Polski okresu międzywojennego. Opublikowane są r&oacute;wnież og&oacute;lne informacje analityczne na podstawie wynik&oacute;w ostatniego spisu ludności z roku 2002. <br />Proponowany projekt jest pionierskim. W zamierzeniu autora ujęcie problemu winno być szersze i bardziej pogłębione w stosunku do dotychczasowych publikacji zajmujących się zbliżoną problematyką. Opracowanie to miałoby przyczynić się do uporządkowania dotychczasowego stanu wiedzy w zakresie wyjaśnianych definicji oraz badań demograficznych, narodowościowych i historycznych, a także rozszerzenia go o ocenę przemian w strukturze narodowościowej mieszkańc&oacute;w naszego kraju i wpływu tychże na procesy demograficzne w długim okresie. Warto dodać, iż punktem wyjścia dla objaśnienia stosowanych pojęć będzie ich&nbsp;rozumienie w polskich spisach zajmujących się problematyką narodowościową oraz w literaturze przedmiotu. Dość oryginalna będzie analiza identyfikacji narodowościowo-religijnej. W pracy, zostanie podjęta pr&oacute;ba odpowiedzi na pytanie, w jakim stopniu wyznanie i język ojczysty determinują przynależność do danej narodowości. Projekt zakłada także przeprowadzenie analizy por&oacute;wnawczej składu narodowościowego mieszkańc&oacute;w Polski w okresie międzywojennym i na początku XXI stulecia w skali makro (wojew&oacute;dztwa) i mikro (powiaty lub miejscowości - na przykładzie Łodzi, Przemyśla, Poznania, Białegostoku i Biłgoraju). Przestawione zostaną tutaj występujące r&oacute;żnice pod względem narodowościowym. Z danych uzyskanych ze spis&oacute;w powszechnych oraz innych oszacowań można wnioskować, jak wielki wpływ na strukturę narodowościową mają migracje wywołane przez wojnę i jej polityczne następstwa. W pracy zostanie także podjęta pr&oacute;ba por&oacute;wnania składu narodowościowego ludności Polski oraz innych kraj&oacute;w europejskich w okresie międzywojennym i na początku XXI wieku. <br /></p>",07/01/10,23,Zmiany w strukturze narodowościowej mieszkańców Polski w XX wieku i ich konsekwencje dla procesów demograficznych,"naród, narodowość, wyznanie, przejście demograficzne, ruch naturalny, mniejszość narodowa",183
81,3713,opisProjektuStanWiedzy,"<div align=""justify""><font face=""Times New Roman"" size=""3"">Jak wspomniano w poprzednim punkcie, w Polsce problematyką związaną z reologią&nbsp;płyn&oacute;w&nbsp;zajmuje&nbsp;się zesp&oacute;ł prof. Dziubińskiego. Jednak realizowane przez ten zesp&oacute;ł prace badawcze dotyczyły badań eksperymentalnych wykonanych dla płyn&oacute;w nienewtonowskich. Badania numeryczne CFD dla płyn&oacute;w nienewtonowskich przeprowadził między innymi zesp&oacute;ł prof. Jaworskiego [5-9]. Przykładowo Adamiak [6] wykonał modelowanie numeryczne CFD dla przepływu płynu nienewtonowskiego przez mieszalniki statyczne Kenics oraz SMX. Płynem nienewtonowskim był wodny roztw&oacute;r karboksymetylocelulozy (CMC), o stężeniu 0,3% wagowych. Na podstawie przeprowadzonych pomiar&oacute;w anemometrycznych LDA (ang. Laser Doppler Anemometry) oraz wynik&oacute;w symulacji CFD, Adamiak [6] stwierdził, że wyniki otrzymane na drodze modelowania numerycznego CFD&nbsp;były najbardziej zbliżone do danych eksperymentalnych dla przepływu&nbsp;laminarnego&nbsp;płynu nienewtonowskiego (CMC) przy najniższych wartościach liczby Reynoldsa (to jest Re = 10; 40). Wraz ze wzrostem wartości Reynoldsa, jakość przewidywania p&oacute;l prędkości uległa znacznemu pogorszeniu, a profile prędkości otrzymane w ramach modelowania CFD nie oddawały w pełni rzeczywistego poziomu gradient&oacute;w prędkości, choć zgodność jakościowa była zachowana . </font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">Problematyka związana z zagadnieniami przepływowymi płyn&oacute;w nienewtonowskich była r&oacute;wnież realizowana w ramach projektu badawczego pt. &bdquo;Badania anemometryczne i modelowanie proces&oacute;w przenoszenia w złożonych przepływach przyściennych&rdquo; (KBN 7 T09C 059 21). Badania wykonane w ramach grantu pozwoliły na wyznaczenie charakterystyki własności sprężystych i lepkich przy ścinaniu rozcieńczonych roztwor&oacute;w CMC (stężenie wynosiło 0,3%). Stwierdzono, że poziom naprężeń normalnych tych cieczy w badanych przepływach był&nbsp;por&oacute;wnywalny z naprężeniami stycznymi.&nbsp;Wykonano r&oacute;wnież eksperymentalne pomiary anemometryczne LDA&nbsp; dla składowej osiowej i stycznej prędkości dla przepływu Couette&rsquo;a-Poiseuille&rsquo;a, bez wir&oacute;w Taylora. Otrzymane rozkłady wykazały istotne odstępstwa od teoretycznych rozkład&oacute;w dla płynu czystolepkiego rozrzedzanego ścinaniem. W ramach realizacji grantu nie wykonano symulacji numerycznych CFD dla przepływu Couette&rsquo;a-Poiseuille&rsquo;a, dlatego symulacje tego typu będą przeprowadzone w ramach niniejszego projektu.</font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">Jeżeli chodzi o zastosowanie metod dynamiki molekularnej do przewidywania lepkości płynu na podstawie jego budowy, to należy podkreślić, że problem ten jest analizowany od nie dawna. Jako przykład zastosowania metody dynamiki molekularnej do prognozowania lepkości płynu można podać pracę Galliero i wsp. [10].&nbsp;Dagr&eacute;ou [11] i wsp. prognozowała własności reologiczne dla układu dwufazowego wody i oleju. Zaprezentowane wyniki były pionierskie &ndash; wcześniej w literaturze przedmiotu nie zostały opisane matematycznie własności reologiczne dla tej emulsji. W pracy [11] zaproponowano model dla tego przypadku i stwierdzono jego dobrą zgodność z wynikami badań eksperymentalnych. <br /></font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">Podsumowanie <br />Zacytowane prace są przykładowymi pracami z zakresu reologii. W dostępnej literaturze przedmiotu nie znaleziono prac poświęconych r&oacute;wnocześnie problematyce modelowania przy zastosowaniu dynamiki molekularnej oraz CFD. Niewątpliwie metoda CFD stosowana do modelowania przepływu płyn&oacute;w nienewtonowskich wymaga znacznego dopracowania, co zostało wykazane przez Adamiaka [5-6]. Dlatego celem niniejszego projektu jest polepszenie jakości modelowania ruchu&nbsp;płyn&oacute;w nienewtonowskich przez CFD i&nbsp;zast...",07/01/31,23,Zastosowanie metod numerycznych CFD oraz dynamiki molekularnej do modelowania przepływów płynów nienewtonowskich,"symulacje numeryczne CFD, dynamika molekularna, płyny nienewtonowskie",173
82,3771,opisProjektuStanWiedzy,"<div style=""text-align: justify;""><font size=""2""><span style=""font-size: 12pt; font-family: Arial;"">            Najbardziej znane, klasyczne prace dotyczące detekcji QTL pochodzą już z lat 50-tych (Cockerham 1954; Hayman and Mather 1955). Jednak metody ich lokalizacji oraz estymacji efektów są nieustannie udoskonalane. Opublikowano i wciąż publikuje się bardzo wiele prac poświęconych metodom detekcji QTL u ludzi, zwierząt i roślin. Historycznie pierwszym, a więc i najlepiej poznanym zagadnieniem jest lokalizacja QTL o addytywnym wpływie na cechę. Aktualnie jednym z najintensywniej rozwijanych zagadnień jest poszukiwanie efektów epistatycznych (np. </span></font><font size=""2""><span lang=""NO-BOK"" style=""font-size: 12pt; font-family: Arial;"">Cheverud 2000; Kao and Zeng 2002; Lou et al. 2003; Yang 2004; Jung et al. 2005; Malmberg et al. 2005; Zeng et al. 2005). </span><span style=""font-size: 12pt; font-family: Arial;"">W lokalizacji wykorzystywane są zarówno testy oparte o istotność efektów modelu liniowego jak i o miarę zaburzenia równowagi Hardyego-Weinberga. Wadą większości z istniejących metod jest przyjmowanie założeń odległych od rzeczywistości, takich jak: (i) bialleliczne markery rozłożone w jednakowych odległościach, (ii) kompletna informacja o genotypie wszystkich markerów dla każdego osobnika, (iii) QTLe zlokalizowane bezpośrednio w markerach oraz (iv) efekty epistatyczne dotyczące tylko par genów. Do najistotniejszych problemów utrudniających zastosowanie zaawansowanych metod zaliczają się: (i) duża złożoność i czasochłonność obliczeniowa, (ii) problemy z dokładną estymacją dużej liczby efektów modelu przy ograniczonej liczebności próby danych oraz (iii) wybór najlepszego z pośród wielu potencjalnie możliwych modeli (liczba możliwych modeli częstokroć przekracza milion).</span></font></div>
<span style=""font-size: 12pt; font-family: Arial;""> </span>",06/12/17,23,"Zastosowanie narzędzi bioinformatycznych do modelowania efektów epsitatycznych, na przykładzie genomów kury i świni","bioinformatyka, detekcja genów, epistaza, testowanie wielokrotne",173
83,3839,opisProjektuStanWiedzy,"<p>  </p>
<p style=""font-family: Arial;""><font size=""3"">W niewyselekcjonowanych populacjach częstość występowania mutacji genów <span style=""font-style: italic;"">BRCA1 </span>i <span style=""font-style: italic;"">BRCA2 </span>wynosi około 13%, jednak geny te charakteryzują się niepełną penetracją i warunkują jedynie kilka procent zachorowań na raka piersi [12, 23]. W regionie pomorskim najczęstszymi odpowiedzialnymi za występowanie raka piersi mutacjami germinalnymi w genie <span style=""font-style: italic;"">BRCA1 </span>są mutacje typu 185delAG, 300T&gt;G, 3819del5, 4153delA, 5382insC, a w genie <span style=""font-style: italic;"">BRCA2 </span>- 6174delT [25].</font></p>
<p style=""font-family: Arial;""><font size=""3"">Istnieje szereg opracowań opisujących cechy morfologiczne guzów i podstawową charakterystykę molekularną raków piersi u nosicielek mutacji genów <span style=""font-style: italic;"">BRCA1 </span>i <span style=""font-style: italic;"">BRCA2 </span>[2-5, 8, 9, 11-13, 15-20, 22-24]. Raki uwarunkowane mutacją <span style=""font-style: italic;"">BRCA1 </span>charakteryzują się zwykle niskim stopniem zróżnicowania; najczęściej występuje typ przewodowy, ale wysoki odsetek stanowią typowe lub atypowe raki rdzeniaste. Częściej niż w przypadkach sporadycznych obserwuje się brak ekspresji receptorów steroidowych, receptora HER2, ekspresję białka P53 oraz antygenów podstawnych (mioepitelialnych): cytokeratyn 5/6, 14, 17, P-kadheryny, a także odmienną ekspresję części białek związanych z procesami proliferacji, apoptozy i kontroli cyklu komórkowego (BCL2, BAX, kaspaza 3, CCND1, CCNE, CCNA, p27Kip1, SKP2, CCNB1, Ki67, EGFR) [2, 5, 8, 9, 11-13, 15-18, 22-24]. Taki profil immunohistochemiczny odpowiada typowi podstawnemu (basal type) raka piersi, zidentyfikowanemu na podstawie badań profili ekspresji genów metodą mikromacierzy DNA [16].</font><font size=""3""><br /></font></p>
<p style=""font-family: Arial;""><font size=""3"">Ponadto, stopień ekspresji białka BRCA1 (będący wyrazem mutacji terminalnych lub somatycznych) wydaje się być jednym z najważniejszych czynników warunkujących odpowiedź na leczenie poszczególnymi grupami leków cytostatycznych: wysoka ekspresja prawidłowego białka BRCA1 jest związana z dużą wrażliwością na leki działające na wrzeciono mitotyczne (taksany, alkaloidy <em style="""">Vinca</em>), podczas gdy brak ekspresji BRCA1 wiąże się z nadwrażliwością na leki uszkadzające DNA (pochodne platyny, mitomycyna C) [20].</font><strong style=""""><span style=""background: yellow none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;""><o:p></o:p></span></strong><font size=""3""><br /></font></p>
<p style=""font-family: Arial;""><font size=""3"">W przypadku nosicielstwa mutacji genu <span style=""font-style: italic;"">BRCA2 </span>zwykle występują średnio- lub nisko-zróżnicowane raki przewodowe, ale obserwuje się zwiększoną częstość innych typów: zrazikowego (pleomorficznego), cewkowego i sitowatego. Ekspresja receptorów steroidowych w tych nowotworach nie różni się od ekspresji w przypadkach raków sporadycznych, ale podobnie jak w przypadku mutacji <span style=""font-style: italic;"">BRCA1</span>, rzadko występuje nadekspresja HER2 [3, 8, 9, 12, 15, 17, 19, 22, 24]. Dane dotyczące ekspresji innych antygenów są mniej jednoznaczne niż w przypadku <span style=""font-style: italic;"">BRCA1 </span>i często przyjmują wartości pośrednie pomiędzy charakterystycznymi dla przypadków sporadycznych i uwarunkowanych mutacją <span style=""font-style: italic;"">BRCA1</span>. </font></p>
<p style=""font-family: Arial;""><font size=""3"">Mutacje <span style=""font-style: italic;"">BRCA1 </span>i <span style=""font-style: italic;"">BRCA2 </span>warunkują jednak jedynie niewielki odsetek ogółu raków piersi i około 25-40% wszystkich przypadków uwarunkowanych genetycznie [12, 23]. Charakterystyka morfologiczna innych uwarunkowanych genetycznie raków piersi jest o wiele słabiej poznana. Ch...",07/01/24,23,Porównanie ekspresji wybranych białek procesu onkogenezy w guzach piersi u chorych na obustronnego raka piersi lub raka piersi i jajnika oraz chorych na jednostronnego raka piersi o podobnych cechach demograficznych - badanie typu pair-matching,"rak piersi, BRCA1, BRCA2, nowotwory dziedziczne, onkogeneza",173
84,3841,opisProjektuStanWiedzy,"<p align=""justify"">Zapobieganie osteoporozie stanowi wyzwanie dla wsp&oacute;łczesnej nauki polskiej i światowej. Według klasycznej definicji wypracowanej przez zesp&oacute;ł ekspert&oacute;w w roku 1993 w Hong Kongu (WHO 1994) osteoporoza jest to &bdquo;<em>układowa choroba szkieletu, charakteryzująca się niską masą kości, upośledzoną mikroarchitekturą tkanki kostnej i w konsekwencji &ndash; zwiększoną jej łamliwością i podatnością na złamania</em>&rdquo;. Stopniowe wydłużanie życia ludzi i starzenie społeczeństw sprzyjają ujawnianiu się przewlekłych problem&oacute;w zdrowotnych uwarunkowanych dietą, w tym osteoporozie. W zależności od przyjętych kryteri&oacute;w diagnostycznych rozpowszechnienie osteoporozy u kobiet po 50. roku życia jest oceniane na 15%, jeśli wynika z pomiaru szyjki kości udowej oraz na 27%, jeśli pomiar wykonano w odcinku lędźwiowym kręgosłupa (Badurski 2003). Wraz z wiekiem (od 20 do 79 lat) w spos&oacute;b zbliżony do liniowego maleje odsetek os&oacute;b o prawidłowej masie kostnej i prawidłowych wartościach BMD (&plusmn;1 SD), a liniowo wzrasta występowanie osteoporozy. Zaburzenia mikroarchitektury tkanki kostnej i niska masa kostna nasilają ryzyko złamań niskoenergetyczych i mogą mieć poważne konsekwencje (Lorenc, Kaczmarewicz 2006). Złamania w wieku dojrzałym i podeszłym często przebiegają z komplikacjami, generując koszty społeczne związane z absencją chorobową, leczeniem, hospitalizacją i inwalidztwem. U 1/3 pacjent&oacute;w w starszym wieku powikłania pojawiające się po złamaniu szyjki kości udowej kończą się śmiercią. Powyższe przesłanki uzasadaniają zainteresowanie środowisk naukowych wczesną diagnostyką osteoporozy, poznawaniem jej czynnik&oacute;w ryzyka i podejmowaniem działań prewencyjnych.</p>
<p align=""justify"">&nbsp;W ostatnich latach wyłoniło się nowe podejście w dostrzeganiu problemu. Zainteresowanie &bdquo;złymi wynikami densytometrycznymi&rdquo;, tj. niską masą kostną, zastąpiono oceną ryzyka złamań, kt&oacute;re w największym stopniu odpowiadają za pogorszenie komforu życia ludzi i zgony os&oacute;b, kt&oacute;re doznały złamania osteoporotycznego (Kanis, Gluer, 2000). Ta niewielka, ale istotna zmiana, oznacza wzrost zainteresowania czynnikami ryzyka złamania kości, zamiast czynnikami ryzyka osteoporozy, kt&oacute;ra nie jest r&oacute;wnoznaczna z obecnością złamania (Badurski 2003). Opisywana w literaturze medycznej liczba czynnik&oacute;w ryzyka złamania kości przekracza 30 i jest nadal dyskutowana i weryfikowana. Wśr&oacute;d pozażywieniowych czynnik&oacute;w ryzyka złamania kości wymienia się m.in. płeć żeńską, rasę białą, wiek (65+ lat u kobiet i 70+ lat u mężczyzn), pierwotne lub wt&oacute;rne zaburzenia miesiączkowania, wczesną menopauzę (przed 45. rokiem życia), występowanie złamań osteoporotycznych w rodzinie, małą aktywność fizyczną, niską masę ciała (&lt;57 kg) lub niskie BMI (&lt;19 kg/m<sup>2</sup>), niedożywienie, palenie tytoniu, zły stan zdrowia w ocenie własnej pacjenta (Badurski 2003; Brown, Josse, 2002; Kanis, Gluer, 2000; Lorenc, Kaczmarewicz 2006; NIH 2001; WHO 1994). Nie wszystkie czynniki mają takie samo znaczenie. Przykładowo, masa ciała &lt;58 kg zwiększa względne ryzyko złamania kości 1,8 razy w por&oacute;wnaniu do os&oacute;b o prawidłowej masie ciała, nadużywanie alkoholu lub aktualne palenie papieros&oacute;w zwiększa ryzyko 1,7 razy (każdy z czynnik&oacute;w niezależnie) w por&oacute;wnaniu do os&oacute;b nie nadużywających alkoholu lub niepalących, a niezdolność do samodzielnego wstania z fotela zwiększa ryzyko złamania kości aż 2,5 razy w por&oacute;wnaniu z osobami mobilnymi. </p>
<p align=""justify"">Niezmienne zainteresowanie skupiają żywieniowe czynniki ryzyka złamań kości, w przekonaniu, że skierowane na nie działania prewencyjne mogą być najłatwiejszą drogą do zapobiegania skutkom złamań kości. Brown i Josse (2002) zaliczyli małe spożycie wapnia, nadużywanie alkoholu i duże spożycie kawy do tzw. &bdquo;małych&rdquo; czynnik&oacute;w ryzyka złama...",07/01/25,23,Badanie wzorów spożycia produktów mlecznych przez matki i ich córki w relacji do statusu tkanki kostnej i zagrożenia kobiet niedoborami wapnia i osteoporozą. Projekt MODAF,"aktywność fizyczna, córki, matki, osteoporoza, produkty mleczne, spożycie, stan odżywienia, status tkanki kostnej, środowisko rodzinne, wapń, wzory spożycia",183
85,3851,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify; tab-stops: list 27.0pt"" align=""justify""><font face=""Times New Roman""><font size=""3"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wielowarstwowe filmy polielektrolit&oacute;w są złożonymi materiałami o interesujących właściwościach, od skali nano, po własności makroskopowe. Postępujące zainteresowanie materiałami wytworzonymi techniką sekwencyjnej adsorpcji polielektrolit&oacute;w, zwanej też adsorpcją layer-by-layer (LbL), notuje się od połowy lat dziewięćdziesiątych, zwłaszcza po publikacji Dechera i Honga z 1992&nbsp;r. [1]. Łatwo można to zaobserwować śledząc ilość pojawiających się publikacji dotyczących tego zagadnienia [7]. Wykorzystując metodę naprzemiennej adsorpcji polielektrolit&oacute;w można wytwarzać powłoki zar&oacute;wno na powierzchniach makroskopowych jak i na powierzchniach mikrocząstek o rozmiarach koloidalnych, mikrokropelek emulsji lub hydrożelu, kt&oacute;re zawierając substancję aktywną stanowią rdzeń wytworzonej w ten spos&oacute;b mikrokapsułki <strong>[</strong>8].<span style=""FONT-SIZE: 11pt; mso-bidi-font-size: 12.0pt""> </span>Zastosowanie techniki sekwencyjnej adsorpcji do wytwarzania powłok mikrokapsułek na rdzeniach koloidalnych zostało zapoczątkowane przez Sukhorukova i innych [9]. Rdzenie te można rozpuścić otrzymując pustą powłokę polielektrolitową o przepuszczalności, kt&oacute;ra może być regulowana przez zmianę pH lub temperatury. Powłoki takie można ponownie napełnić substancją aktywną, otrzymując mikrokapsułkę o pożądanej funkcji. <span lang=""EN-US"" style=""FONT-SIZE: 11pt; mso-bidi-font-size: 12.0pt; mso-ansi-language: EN-US""><o:p></o:p></span></font></font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify; tab-stops: 27.0pt"" align=""justify""><font face=""Times New Roman""><font size=""3""><span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Pomimo istotnego znaczenia poznawczego i wielu perspektywicznych zastosowań praktycznych mikrokapsułek rozw&oacute;j efektywnych technologii ich wytwarzania jest w znacznym stopniu ograniczony niedostateczną znajomością podstaw fizykochemicznych tych proces&oacute;w.<span style=""mso-spacerun: yes"">&nbsp; </span>W szczeg&oacute;lności brak jest systematycznych prac opisujących mechanizmy i kinetykę proces&oacute;w tworzenia powłok mikrokapsułek zbudowanych z film&oacute;w wielowarstwowych oraz wpływ takich parametr&oacute;w jak pH i siła jonowa roztwor&oacute;w, z kt&oacute;rych filmy te są nanoszone. Brak jest r&oacute;wnież w literaturze danych odnośnie kinetyki adsorpcji (osadzania) mikrokapsułek na stałych powierzchniach w warunkach<span style=""mso-spacerun: yes"">&nbsp; </span>wymuszonej konwekcji.</font></font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify; tab-stops: 27.0pt"" align=""justify""><font face=""Times New Roman"" size=""3"">Dane te są niezbędne dla określenia aspekt&oacute;w kinetycznych proces&oacute;w osadzania mikrokapsułek, ich adhezji, kinetyki desorpcji, itp. Jest to spowodowane w gł&oacute;wnej mierze brakiem pomiar&oacute;w prowadzonych przy pomocy bezpośrednich metodyk badawczych. Zazwyczaj kinetykę proces&oacute;w osadzania nano/mikrocząstek wyznacza się przy użyciu metod pośrednich, polegających na pomiarze zmian stężenia suspensji przed i po kontakcie z powierzchnią [10]. Do często używanych metod pośrednich należą metody radioizotopowe pozwalające na wyznaczenie kinetyki adsorpcji protein [11-13], a także metody optyczne np. elipsometria i reflektometria, stosowane do polimer&oacute;w i cząstek koloidalnych [14-15]. Metody te wymagają stosowania skomplikowanych procedur kalibracyjnych. Innym sposobem pośredniego wyznaczenia pokryć powierzchniowych jest pomiar zmian potencjału przepływu w skutek adsorpcji cząstek [16]. Wymienione metody dostarczają informacji uśrednionych z dużych obszar&oacute;w powierzchni, nie pozwalając na badanie efekt&oacute;w związanych z mikr...",07/01/29,23,Osadzanie modelowych mikrokapsułek na powierzchniach metalicznych modyfikowanych przez adsorpcję multiwarstw polielektrolitów,"mikrokapsułki, osadzanie, polielektrolity, dyfuzja, konwekcja, adsorpcja",183
86,3866,opisProjektuStanWiedzy,"<blockquote dir=""ltr"" style=""MARGIN-RIGHT: 0px"">
<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; </font><font face=""Times New Roman"" size=""3"">Ograniczenie emisji SO<sub>2 </sub>prowadzone jest gł&oacute;wnie poprzez odsiarczanie paliw, zgazowanie paliw, a także odsiarczanie otrzymanego gazu oraz usuwanie ditlenku siarki z gaz&oacute;w spalinowych. Metody odsiarczania gaz&oacute;w odlotowych można podzielić według:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font></p>
<p align=""left""><font face=""Times New Roman"" size=""3"">a) rodzaju procesu: - adsorpcyjne, absorpcyjne i katalityczne, <br />b) charakteru procesu: - mokre, suche, p&oacute;łsuche, regeneracyjne i nieregeneracyjne (odpadowe), <br />c) rodzaju zastosowanego sorbentu: - wapniowe, sodowe, amoniakalne, magnezowe, <br />d) otrzymywanego produktu utylizacji: - szlam posorpcyjny, gips, ditlenek siarki, kwas siarkowego (VI), siarczan (VI) amonu, siarka elementarna.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Najbardziej rozpowszechnionymi metodami odsiarczania spalin są metody wapienne. Jako sorbentu używa się wapna palonego (CaO), węglanu wapnia lub wapna gaszonego (Ca(OH)<sub>2</sub>). Metoda wapniowa mokra jest stosunkowo prosta i tania, i dlatego, pomimo swoich wad (stopień odsiarczenia spalin rzędu 80-90%, trudności techniczne związane z zarastaniem urządzeń stałymi osadami soli wapnia, kłopotliwy odpad), jest najczęściej stosowna [14]. Istnieje szereg modyfikacji tej metody, pozwalających na efektywniejsze odsiarczanie gaz&oacute;w [9]. Efektywność stosowanej metody odsiarczania zależy m.in. od rodzaju kotła, w kt&oacute;rym prowadzony jest proces spalania. Zastosowanie stałego sorbenta jest efektywne na przykład w kotłach fluidalnych. Sucha metoda wapniowa jest najbardziej odpowiednia dla małych i&nbsp;średniej wielkości źr&oacute;deł emisji [8]. Znane są r&oacute;wnież katalityczne metody odsiarczania spalin. Należą do nich [15]: metoda Cat+OX firmy Monsanto, polegająca na utlenianiu SO<sub>2</sub> do SO<sub>3</sub> na katalizatorze wanadowym, a następnie na&nbsp;absorpcji w kwasie siarkowym (VI), kt&oacute;ry jest produktem, metoda katalitycznego utleniania SO<sub>2 </sub>w rozcieńczonym H<sub>2</sub>SO<sub>4</sub> w obecności soli żelaza lub manganu (proces Chiyoda Throughbred 101). Produktem odsiarczania w tej metodzie jest gips, katalityczny proces odsiarczania i odazotowania gaz&oacute;w spalinowych, opracowany przez firmę Haldor Topsoe, gdzie poprzez utlenienie SO<sub>2</sub> do SO<sub>3</sub> otrzymywany jest kwas siarkowy (jako produkt finalny usuwania SO<sub>2</sub> z&nbsp;gaz&oacute;w spalinowych), a tlenki azotu redukowane do N<sub>2</sub> na monolitycznym katalizatorze opartym na TiO<sub>2</sub> w reakcji z&nbsp;amoniakiem jako reduktorem.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zmniejszenie emisji tlenk&oacute;w azotu pochodzących z proces&oacute;w spalania jest możliwe przez modyfikacje samego procesu spalania paliw lub ich usunięcie z gaz&oacute;w odlotowych. W pierwszym przypadku ilość emitowanych tlenk&oacute;w maleje dzięki: - modyfikacji konstrukcji kotł&oacute;w, stosowaniu palnik&oacute;w o specjalnej konstrukcji (palniki niskotoksyczne, dwuzasuwowe, ze stopniowaniem paliwa, z recyrkulacją gaz&oacute;w itp.), minimalizacji wsp&oacute;łczynnika nadmiaru powietrza, doprowadzenia wody i pary do strefy spalania, recyrkulacji spalin oraz zmianom w&nbsp;sposobach spalania (dwustrefowe, katalityczne, w złożu fluidalnym, rusztowe) [16]. Realizacja tych sposob&oacute;w jest możliwa w&nbsp;niejednakowym stopniu i nie we&nbsp;wszystkich wypadkach. Stopień redukcji ilości tlenk&oacute;w...",06/12/21,23,Wysokoaktywne zeolitowe i montmorillonitowe katalizatory odsiarczania i odazotowania gazów spalinowych z procesu spalania węgla kamiennego,"spalanie, węgiel kamienny, montmorillonit, zeolit, katalizator, odsiarczanie, odazotowanie, DESONOX, metoda DIM, metoda CIM, metoda CIM/AS",173
87,3926,opisProjektuStanWiedzy,,07/01/30,23,System podwodnego monitoringu antyterrorystycznego portów morskich,"kartografia, monitoring podwodny, hydrolokacja, antyterroryzm",173
88,3971,opisProjektuStanWiedzy,"<p style=""text-align: justify;""><font size=""2""><span style=""font-family: Arial;"">    </span><span style=""font-size: 10pt; font-family: Arial;"">Koncepcja hierarchizacji topologii sieci ad hoc nie jest nowa. Jej stosowanie wynika m.in. z potrzeby zapewnienia skalowalności sieci, organizacji warstwy dostępu do medium, a przede wszystkim z konieczności ustabilizowania dynamicznie zmiennej topologii, wynikającej z przemieszczania się węzłów. Ruch węzłów jest główną przyczyną upadków bezprzewodowych łączy i w konsekwencji źródłem ruchu kontrolnego, generowanego przez protokoły poszczególnych warstw sieci. <o:p></o:p></span><span style=""font-family: Arial;"">  </span><span style=""font-size: 10pt; font-family: Arial;""></span></font></p>
<div style=""text-align: justify;""> </div>
<p style=""text-align: justify;""><font size=""2""><span style=""font-size: 10pt; font-family: Arial;"">    Od kilku lat obserwuje się rozwój algorytmów zarządzania topologią w sieciach ad hoc. Oferują one coraz bardziej efektywne rozwiązania w zarządzaniu topologią sieci. Jednak analiza ich funkcjonowania ogranicza się do najprostszego, podstawowego podziału hierarchicznego, wyróżniającego dwa poziomy: zwykłych węzłów oraz zarządców podsieci. Celem niniejszego projektu jest zbadanie celowości i efektywności wprowadzania kolejnych poziomów w hierarchii podziałów węzłów sieci.</span></font></p>",07/01/22,23,"Analiza wpływu głębokości struktury hierarchicznej sieci na stabilność topologii połączeń w wysoko mobilnych, bezprzewodowych sieciach ad hoc.","sieci ad hoc, kontrola topologii, routing, inżynieria ruchu",173
89,3973,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN-RIGHT: 1.15pt; TEXT-ALIGN: justify"">Szybkie procesy termiczne (RTP) stosowane są w badaniach p&oacute;łprzewodnik&oacute;w i przemyśle od wielu lat [20]. Zalety, przede wszystkim optymalny bilans energetyczny i czasowy, predysponują RTP do wykorzystania na etapie selenizacji w przemysłowej produkcji ogniw CIS. W&nbsp;ciągu ostatniej dekady procesy RT znajdują coraz szersze zastosowanie w wytwarzaniu ogniw słonecznych pozwalając na dogodne prowadzenie proces&oacute;w (SEL-RTP). Selenizacja termiczna RTP jest aktualnie przedstawiana w szeregu publikacjach [16,17,23]. Proces ten często prowadzony jest w kwarcowych piecach pr&oacute;żniowych z halogenowymi źr&oacute;dłami promieniowania pozwalającymi na wyzyskanie prędkość przyrostu temperatury procesu powyżej 10<span style=""FONT-FAMILY: Arial;""><span>&deg;</span></span>C/s [23]. <br /></p>
<p class=""MsoNormal"" style=""MARGIN-RIGHT: 1.15pt; TEXT-ALIGN: justify"">Metoda RTP rozwijana jest r&oacute;wnież w Katedrze Elektroniki AGH [7]. Prace badawcze koncentrują się na doborze składu i struktury warstw prekursor&oacute;w oraz określeniu parametr&oacute;w technologicznych selenizacji [3,17], gł&oacute;wnie przebieg&oacute;w temperaturowych [2,4,5]. Planowane do realizacji w ramach projektu badanie in-situ rezystancji pr&oacute;bek w trakcie obr&oacute;bki termicznej stwarza interesującą możliwość monitorowania procesu selenizacji. Obserwacja rezystancji pr&oacute;bki pozwala śledzić poszczeg&oacute;lne stadia formowania się p&oacute;łprzewodnikowej warstwy absorbera CIS z metalicznych prekursor&oacute;w [16,21,24].</p>
<div style=""TEXT-ALIGN: justify""><span style=""FONT-SIZE: 12pt; FONT-FAMILY: &quot;Times New Roman&quot;"">W procesie RT kluczowe znaczenie ma uzyskanie odpowiedniego profilu temperatury. Podstawowym problemem jest rozwiązanie zagadnienia efektywnego sterowania radiacyjnym transportem energii w urządzeniu laboratoryjnym do selenizacji materiał&oacute;w fotowoltaicznych. Ponieważ w procesach RT w bardzo kr&oacute;tkich przedziałach czasu zachodzą złożone zjawiska termiczne do sterowania procesem stosowane są zaawansowane, stale rozwijane procedury regulacji [18,19]. Problem planuje się rozwiązać wyznaczając sterowanie w oparciu o zbudowany matematyczny model procesu. Bazując na analizie zjawisk fizycznych (r&oacute;wnania radiacyjnego i przewodzonego transportu ciepła) opracowano procedurę tworzenia modelu we wcześniejszych pracach [4]. Dla zamodelowania zjawisk termicznych piec podzielono na elementy pomiędzy kt&oacute;rymi opisano oddziaływania termiczne. Każdy element składowy opisywany jest przez temperaturę (T<sub>n</sub>) kt&oacute;rej zmianę powoduje padający na dany element strumień ciepła (zależność1) drogą pr zewodzenia i promieniowania (zależność 2).</span><br /><span style=""FONT-SIZE: 12pt; FONT-FAMILY: &quot;Times New Roman&quot;""></span></div>
<span style=""FONT-SIZE: 12pt; FONT-FAMILY: &quot;Times New Roman&quot;""><img style=""WIDTH: 622px; HEIGHT: 44px"" alt="""" src=""/OSFImageLoader.do?idImageDB=5801"" /><br /></span>
<p class=""MsoNormal"" style=""MARGIN-RIGHT: 1.15pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 10pt"">gdzie: C<sub>n</sub>-pojemność termiczna elementu n; P<sub>n</sub>-moc dostarczana do elementu n w kt&oacute;rej skład wchodzi moc radiacyjna (P<sub>Rn</sub>) i przewodzenie ciepła (P<sub>Cn</sub>); P<sub>Ri-&gt;n</sub>-moc radiacyjna dostarcznana z elementu i do elementu n; P<sub>Ci-&gt;n</sub>-strumień ciepła przewodzonego z elementu i do elementu n.<o:p></o:p></span></p>
<span style=""FONT-SIZE: 12pt; FONT-FAMILY: &quot;Times New Roman&quot;"">Zjawiska termiczne układu element&oacute;w oipsuje macierzowE r&oacute;wnanie stanu (3) [4,22].<br /><img style=""WIDTH: 589px; HEIGHT: 123px"" alt="""" src=""/OSFImageLoader.do?idImageDB=5802"" /><br /></span>
<p class=""MsoNormal"" style=""MARGIN-RIGHT: 1.15pt; TEXT-ALIGN: justify"">Pierwsza macierz (A) grupuje oddziaływania radiacyjne, druga macierz (B) oddziaływania przez prz...",07/01/30,23,Opracowanie modelu wspomagającego proces obróbki termicznej materiałów fotowoltaicznych,"Szybkie Procesy Termiczne (RTP), Selenizacja, Siarczki i selenki miedziowo-indowe (CIS), Sterowanie w oparciu o model (MBS), Sterowanie predykcyjne (MPC), Fotowoltaika.",183
90,4008,opisProjektuStanWiedzy,"<p align=""justify""><font size=""4"">Obiektem badań jest sześć związków będących pochodnymi daunorubicyny, doksorubicyny oraz epidoksorubicyny, które zsyntezowano w Instytucie Biotechnologii i Antybiotyków w Warszawie. W pozycji 3’ pierścienia daunozaminy nowe analogi zawierają grupę morfolinową lub heksametylenoiminową. Związki oznaczono następująco: daunorubicyna (DRB), doksorubicyna (DOX), epidoksorubicyna (EDOX), ich pochodne morfolinowe odpowiednio DRBM, DOXM, EDOXM), a pochodne heksametylenoiminowe (DRBH, DOXH, EDOXH).</font></p>
<p align=""justify""><font size=""4""><br /></font></p>
<p align=""justify""><font size=""4""></font></p>
<p style=""text-align: center;""><font size=""4""><img style=""width: 603px; height: 310px;"" alt="""" src=""/OSFImageLoader.do?idImageDB=3854"" /></font></p>
<p align=""justify""><font size=""4""></font></p>
<p align=""center""><font size=""4""><br /></font></p>
<p align=""center""><font size=""4"">Rysunek 1. Wzory badanych antracyklin</font></p>
<p align=""justify""><font size=""4"">Ta nowa grupa pochodnych antracyklinowych jest od kilku lat intensywnie badana przez polskie zespoły badawcze, które wykazały, że nowe analogi posiadają kilka interesujących właściwości takich jak: wysoka aktywność cytotoksyczna [1, 2, 4] , zdolność do tworzenia kowalencyjnych wiązań z DNA [2, 3], aktywność przeciwnowotworowa <span style=""font-style: italic;"">in vivo</span> [5], zdolność do indukcji różnicowania komórek białaczki K562 [6], obniżona w stosunku do związków macierzystych kardiotoksyczność [7], zdolność do przełamywania oporności wielolekowej [8]. Opisane w wymienionych odnośnikach, korzystne z punktu widzenia chemioterapii nowotworów, cechy nowych analogów nie zostały jeszcze wyjaśnione z biochemicznego punktu widzenia i nie jest wiadome jak obecność morfolinowego i heksametylenoiminowego podstawnika w strukturze antracyklin wpływa na mechanizm cytotoksycznego działania tych związków.</font></p>
<p align=""justify""><font size=""4"">Praca doktorska, o której finansowanie wnioskuję, została już rozpoczęta (przewód doktorski otworzono w lipcu 2006). Uzyskane do tej pory wyniki dotyczącą mechanizmu zabijania szybko proliferujących komórek białaczki L1210 przez nowe analogi doksorubicyny i daunorubicyny. Stwierdzono, że obecność heterocyklicznych pierścieni w pozycji 3’ daunozaminy obniża aktywność cytotoksyczną, co wydaje się mieć związek ze zmianą powinowactwa nowych analogów do &quot;rozszczepialnego kompleksu&quot; DNA-topoizomeraza II. Jednocześnie wykazano, że nowe analogi mają, w porównaniu ze związkami macierzystymi, bardziej skomplikowany mechanizm cytotoksycznego działania. Związki te, okazały się słabszymi inhibitorami DNA topoizomerazy II, lecz posiadają zdolność do powodowania degradacji DNA za pomocą innego mechanizmu, w którym istotną role odgrywa proces tworzenia wiązań kowalencyjnych. Ta część doktoratu wymaga jeszcze uzupełnienia odnośnie dwóch innych związków: EDOXM i EDOXH. Reszta planowanych we wniosku badań będzie zmierzać do wyjaśnienia dlaczego analogi z pierścieniami heterocyklicznymi wykazały znaczną aktywność cytotoksyczną wobec komórek charakteryzujących się opornością wielolekową [8] oraz obniżoną kardiotoksycznością <span style=""font-style: italic;"">in vivo</span> [7].</font></p>
<p align=""justify""><font size=""4"">Oporność wielolekowa jest jednym z czynników, które ograniczają kliniczną skuteczność większości leków przeciwnowotworowych. Obecność grupy morfolinowej lub heksametylenomiminowej w cząsteczce antracykliny okazała się być korzystną modyfikacją strukturalną [8], lecz nie jest jeszcze jasne, czy przełamywanie oporności wielolekowej przez formamidynowe pochodne antracykliny wynikają z szybszego wnikania tych związków do komórek, czy może z wolniejszego ich usuwania przez komórki oporne na doksorubicynę. Możliwe jest również, że powodem takiej aktywności analogów jest ich inna, niż w przypadku klinicznie stosowanych antracyklin wewnątrzkomórkowa lokalizacja. Proponowane badania mają na celu wyjaśni...",07/01/25,23,Biochemiczne podstawy cytotoksycznego i kardiotoksycznego działania formamidynowych pochodnych antracykliny,"antracykliny, cytotoksyczność, uszkodzenia DNA, topoizomeraza II, reduktywna aktywacja, kardiotoksyczność, glikozoaminoglikany",183
91,4057,opisProjektuStanWiedzy,"<p>Projektowanie złożonych nieliniowych układ&oacute;w technicznych, czego przykładem są niewrażliwe układy sterowania napędem elektrycznym, wymaga doboru wartości zmiennych decyzyjnych&nbsp;x<sub>i</sub> z&nbsp;uwzględnieniem ograniczeń, tak aby zoptymalizować więcej niż jedno kryterium y<sub>i</sub>. Przedmiotem badań jest zadanie dla kt&oacute;rego kryteria&nbsp; są sprzeczne. Oznacza to, że nie istnieje pojedynczy wektor decyzyjny <strong>x</strong>, należący do zbiory dopuszczalnego X, optymalizujący wektor rozwiązań <strong>y</strong>.</p>
<div>W literaturze poświęconej metodom optymalizacji wielokryterialnej dominuje podejście określane jako kompromisowe optimum Pareto. Poszukiwanie zbioru rozwiązań paretooptymalnych (zwanego frontem Pareto) jest coraz częściej realizowane z metodami ewolucyjnymi. Mianem tym określamy algorytmy heurystyczne inspirowane spostrzeżeniami z analizy ewolucji naturalnej. W ostatnich 20&nbsp;latach opracowano wiele algorytm&oacute;w ewolucyjnej optymalizacji wielokryterialnej (wyczerpująca lista jest dostępna jest on-line w&nbsp;uaktualnianym na bieżąco repozytorium [1]). Algorytmy są analizowane i por&oacute;wnywane z&nbsp;wykorzystaniem funkcji testowych (wyczerpujące zestawienia zawierają prace [2], [3]). Z&nbsp;reguły badane są zadania optymalizacji dwu- lub tr&oacute;jwymiarowej, co potwierdza zestawienie liczby cytowań pogrupowanych w zależności od ilości kryteri&oacute;w (Rys.1) [7].</div>
<div></div>
<p align=""center""><img alt="""" src=""/OSFImageLoader.do?idImageDB=9072"" /></p>
<div></div>
<div>Rys.1 Ilość cytowań prac poświęconych optymalizacji wielokryterialnej w zależności od ilości kryteri&oacute;w w zadaniu [7].</div>
<div>&nbsp;</div>
<div>Większość obecnie stosowanych algorytm&oacute;w staje się mało skuteczna wraz ze wzrostem liczby kryteri&oacute;w na co wykazują wyniki analizy [7], [8], [9]. W ostatnich latach prowadzone są intensywne prace mające wyeliminować te ograniczenia. Przykładem są poszukiwania metody por&oacute;wnywania rozwiązań alternatywnej do podejścia Pareto [10]. Według najlepszej wiedzy autora tego wniosku, w literaturze przedmiotu brak doniesień o efektywnej obliczeniowo realizacji algorytm&oacute;w implementujących nowe miary .</div>
<div>Kolejnym ograniczeniem w zastosowaniach wymagających optymalizacji większej niż trzech kryteri&oacute;w, są dodatkowe trudności z efektywną reprezentacją rozwiązań paretooptymalnych. Wynikają one z faktu że wynik obliczeń ewolucyjnych stanowi zbi&oacute;r rozwiązań <strong>y</strong> aproksymujących frontu Pareto. Związany z tą aproksymacją zbi&oacute;r wektor&oacute;w zmiennych decyzyjnych <strong>x </strong>musi być oceniony przez projektanta (jest nim korzystający z algorytmu, <em>decision maker</em>) poszukującego jednego rozwiązania, kt&oacute;re ma być przyjęte do realizacji. Ocenia on z reguły wyniki zgodnie ze swoimi preferencjami, kt&oacute;re są trudne do sformalizowania. Przy&nbsp;wyborze takiego preferowanego rozwiązania możliwe jest zastosowanie podejście <em>a&nbsp;priori</em> lub <em>a posteriori</em> w zależności od etapu realizacji obliczeń w kt&oacute;rym <em>decision maker </em>formułuje swe preferencje. </div>
<div><u>Uwzględnienie preferencji określonych <em>a priori</em></u>. Klasycznym przykładem jest przypisanie poszczeg&oacute;lnym kryteriom wag i zastąpienie wektora funkcją skalarną. W analizowanym podejściu Pareto z zastosowaniem algorytm&oacute;w ewolucyjnych możliwe jest ukierunkowanie poszukiwań poprzez wskazanie w zbiorze dopuszczalnym X pewnego obszaru preferowanego. Koncepcja została sformułowana w latach siedemdziesiątych [11], a wyniki jej zastosowania zostały opublikowane w ubiegłym roku. Obszar jest określany nie w postaci ograniczeń, lecz przez wskazanie punkt&oacute;w odniesienia,. Przykład zastosowania tego podejścia do zadania ewolucyjnej optymalizacji dwu-kryterialnej z wykorzystaniem skutecznego algorytmu NSGA II przedstawiono na Rys.2 [13]. Zwraca uwagę możliwość skupienia...",07/01/30,23,Uwzględnienie preferencji w projektowaniu układów regulacji metodami ewolucyjnej optymalizacji wielokryterialnej,"synteza układów regulacji, optymalizacja wielokryterialna, algorytmy ewolucyjne, preferencje projektanta",173
92,4059,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.45pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">Badania dotyczące selekcji szkolnych, tj. proces&oacute;w r&oacute;żnicowania się dr&oacute;g szkolnych i szans edukacyjnych dzieci i młodzieży na r&oacute;żnych szczeblach szkolnictwa, mają w naszym kraju, a także za granicą długą tradycję. Od dziesiątek lat problematyka ta wzbudza żywe zainteresowanie socjolog&oacute;w, psycholog&oacute;w i pedagog&oacute;w. Wielość prowadzonych badań obejmujących r&oacute;żnorodne aspekty selekcji szkolnych dowodzi, iż jest to zagadnienie wieloaspektowe, z trudem poddające się jednoznacznej interpretacji. W literaturze przedmiotu można znaleźć analizy tego pola problemowego, przede wszystkim w skali makro, obrazujące rozmiary, podłoże, przejawy zjawiska, jego skutki społeczne <span style=""mso-spacerun: yes"">&nbsp;</span>(Kozakiewicz 1973, Kwieciński 1980, 1995; Borowicz 1983, 2000; Frąckowiak 1986; Szymański 1988, 1996 i in.), jak też w skali mikro, odniesione do opisu wybranego, pojedynczego przypadku wsi czy szkoły (np. Wincławski 1971, Kwieciński 1972, Mikiewicz 2005). </font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.45pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">Liczne badania selekcji szkolnej dowodzą, że jest to proces warunkowany zespołem kompleksowo i długotrwało oddziałujących, wielorakich czynnik&oacute;w. Do najważniejszych z nich należą: przyczyny osadnicze i demograficzne, strukturalno-polityczne, gospodarcze i bytowe, socjalizacyjne i inkulturacyjne, pedagogiczne i edukacyjne oraz biopsychiczne (Kwieciński 2000), przy czym ich oddziaływanie na rozw&oacute;j dziecka rozpoczyna się już od momentu jego urodzenia</font><a title="""" style=""mso-footnote-id: ftn1"" target=""_blank"" name=""_ftnref1""><span class=""MsoFootnoteReference""><span style=""mso-special-character: footnote""><span class=""MsoFootnoteReference""><span style=""FONT-SIZE: 12pt; FONT-FAMILY: &quot;Times New Roman&quot;; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: PL; mso-bidi-language: AR-SA"">[1]</span></span></span></span></a><font face=""Times New Roman"" size=""3"">. Badania te wskazują przede wszystkim, że r&oacute;żnice szans edukacyjnych dzieci ze wsi, z<span style=""mso-spacerun: yes"">&nbsp; </span>rodzin o niskim statusie społeczno-ekonomicznym w por&oacute;wnaniu z dziećmi z rodzin wielkomiejskich z warstw średnich są ogromne, gł&oacute;wnie w zakresie poziomu rozwoju na progu szkoły podstawowej, wynik&oacute;w w nauce, motywacji, aspiracji, los&oacute;w edukacyjnych i innych (np. Kwieciński 1980, 1995, 2000, 2002; <span style=""mso-spacerun: yes"">&nbsp;</span>Borowicz 1983, 2000, Konarzewski 1991).</font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.45pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">Prawie wszystkie badania fenomenu r&oacute;żnicowania się pozycji szkolnych dzieci i młodzieży i ich szans edukacyjnych przyznawały gł&oacute;wną rolę pochodzeniu społecznemu (Coleman 1966; Jencks 1972; Kwieciński 1973, 1975, 1995, 2002; Bernstein 1990; Konarzewski 1991; Hurrelmann 1994; Bourdieu 2005, 2006 i in.).Wynika z nich, że status społeczny rodziny pochodzenia ucznia i jej &bdquo;habitus&rdquo;, czyli og&oacute;ł wzor&oacute;w życiowej aktywności, w decydującym stopniu r&oacute;żnicują status szkolny i karierę edukacyjną ucznia.</font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.45pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">Wsp&oacute;łczesne badania nad procesem selekcji szkolnych &ndash; między innymi brytyjskie, holenderskie, francuskie, a także i polskie - dowodzą, że w procesie wyznaczania ścieżek edukacyjnych dzieci coraz większy udział mają rodzice, zwłaszcza zaś rodzice z klasy średniej. Zdają sobie oni doskonale sprawę z wagi rodzaju ...",07/01/29,23,Wczesne selekcje społeczno-szkolne. Studium porównawcze dwóch pierwszych klas," wczesne selekcje społeczno-szkolne, start szkolny, nierówności społeczne, nierówności edukacjne, szanse edukacyjne, różnicowanie się dróg szkolnych, kapitał społeczno-kulturowy, rodzice, wybory edukacyjne, sytuacja rodzinna, sytuacja szkolna, wzory zachowań, klasa szkolna, skład społeczny klasy, nauczyciel, dzieci, pierwszoklasiści, studium porównawcze",173
93,4115,opisProjektuStanWiedzy,beznadziejny,,27,Słownik,idiomy,1
94,4120,opisProjektuStanWiedzy,"<p style=""margin-bottom: 0cm;"">Badaniami rozsiewaczy, siewników do nasion i nawozu, a także przesiewaczy i innych urządzeń, w których wspólną cechą jest to, że obiektem ich pracy są materiały ziarniste podlegające transportowi i rozdzielaniu oraz dystrybucji powierzchniowej  zajmowano się od dawna, tzn. od czasu wynalezienia tych maszyn. Ze względu na złożoność tych procesów, były to głównie badania empiryczne, wyjaśniające w dostępnym stopniu wpływ wybranych parametrów konstrukcyjnych na uzyskiwane efekty technologiczne, o bardzo ograniczonej ogólności, mało przydatne dla obliczeń optymalizacyjnych. I tak, w przypadku rozsiewaczy tarczowych badano wpływ położenia otworu dozującego, kątów i ustawienia i wymiarów  łopatek, składu granulometrycznego materiału, na poprzeczny rozkład materiału na powierzchni pola. W przypadku przesiewaczy bada się wpływ wymiarów otworów sita, kata ich nachylenia, parametrów ruchu i natężenia zasilania oraz składu granulometycznego materiału na intensywność przesiewania i wydajność tego procesu. Bliskie podejmowanym tu zagadnieniom   zjawiska transportu pneumatycznego w kanałach opisywano metodami półempirycznymi, z wykorzystaniem  równania Bernoulliego. Mieszaninę wielofazową traktuje się jako jednorodny płyn z ogólnym uwzględnieniem transportowanego materiału poprzez wprowadzenie do tych równań dodatkowych członów i współczynnika koncentracji określającego udział transportowanego materiału w stosunku do medium transportującego. Współczynniki wchodzące do tych równań ustala się empirycznie, zaś takie zjawiska jak nierównomierność koncentracji na przekroju poprzecznym kanału, rozkład cząstek w strumieniu determinujący ich podział w kanałach rozdzielczych był poza zasięgiem obliczeń teoretycznych i mógł być badany jedynie na drodze empirycznej. Przykładem takich badań były również te w przeszłości prowadzone również przez autora i wykonawców  projektu.  Nowy kierunek badań podejmowany przez autorów projektu jest ogólnie zarysowany w wielu publikacjach dotyczących i rozsiewania nawozów przez rozsiewacze tarczowe, procesu przesiewania  materiałów granularnych na sitach [ 1,2,3,4,5  ].   W wielu publikacjach,  takie zjawiska jak:  ruch granul na tarczy wysiewającej, sicie, czy kanale pneumatycznym opisywano budując model matematyczny ruchu pojedynczej cząstki. Taki model pozwalał nawet wyznaczyć na drodze symulacji komputerowej dystrybucję  nawozu na powierzchni pola, prędkość i trajektorie ruchu materiału na sicie itp.  Niestety, uproszenia tego modelu polegające na pominięciu istotnych oddziaływań  między cząstkami materiału na tarczy i w powietrzu spowodowały znaczną rozbieżność uzyskanych wyników z danymi eksperymentalnymi, co wymagało sztucznej korekcji danych wejściowych do obliczeń, wyznaczanej z danych empirycznych. Obecnie, obiecującym kierunkiem badań są modele uwzględniające wzajemne oddziaływania nawet tysięcy ciał o złożonej geometrii i właściwościach mechanicznych oraz Są to jednak dopiero  prace początkowe. Przyjmuje się, że pionierskimi pracami w tym zakresie były prace Cundalla i Strucka [ 5 ]. W zakresie wyjaśniania ruchu materiału w szczelinie roboczej zespołu młócącego  procesie omłotu ziarna takie podejście zastosował również autor projektu w pracach[ 6,11 ].  Tkwiący w tej metodzie potencjał  wymaga jednak intensywnego rozwijania i może przynieść ważne rezultaty w zastosowaniach praktycznych.  Wydaje się, że przeniesienie tego uniwersalnego podejścia do wyjaśnienia zjawisk dystrybucji materiałów w kanałach rozdzielczych siewników pneumatycznych,  oraz dystrybucji materiału przez  tarczowe elementy  rozsiewające oraz ruchu materiału na sitach przesiewaczy  może wyprowadzić z impasu zastosowania mechaniki teoretycznej w konstruowaniu maszyn w tej  dziedzinie techniki.  Jak dotąd, prace tego rodzaju przynoszą ciekawe rezultaty teoretyczne, jednak do wykorzystania praktycznego uzyskiwanym wynikom brakuje dostatecznej precyzji, pewności i niezawodności. W pionierskiej pracy Li i innych...",07/01/29,23,"Zastosowanie symulacji komputerowej metodą elementów dyskretnych do badania ruchu materiałów granularnych w maszynach, w aspekcie optymalizacji konstrukcji i regulacji zespołów roboczych maszyn do sortowania i powierzchniowej  dystrybucji kruszyw, nasion roślinnych i nawozów","modelowanie, ziarno, przesiewanie, rozsiewanie",173
95,4129,opisProjektuStanWiedzy,"<p align=""justify""><font size=""4"">&nbsp;&nbsp; W literaturze krajowej i zagranicznej można znaleźć wiele książek i publikacji na temat żeglugi i transportu morskiego. Dotyczą one przede wszystkim postępu technicznego budowy statk&oacute;w, polityki morskiej państwa, restrukturyzacji przedsiębiorstw oraz aspektu budowy autostrad &ndash; jako jednego z czynnik&oacute;w rozwoju. Niewiele prac natomiast poświęconych zostało działalności marketingowej w żegludze morskiej, w tym promowej. Jak dotąd nie przeprowadzono badań dotyczących wielkości i struktury segment&oacute;w rynku pasażer&oacute;w podr&oacute;żujących drogą morską na poszczeg&oacute;lnych rynkach w obrębie Unii Europejskiej. Nie przeprowadzono r&oacute;wnież badań marketingowych armator&oacute;w na tak szeroką skalę.&nbsp;Rozwiązanie postawionego w projekcie problemu wzbogaci dyscyplinę marketingu w kraju i na świecie o dodatkową wiedzę o nabywcach usług promowych i aktywności operator&oacute;w świadczących takie usługi. Por&oacute;wnanie zjawisk i proces&oacute;w mających miejsce na poszczeg&oacute;lnych rynkach Europy pozwoli ponadto zweryfikować relacje panujące na tych rynkach. </font></p>",07/01/29,23,Perspektywy rozwoju pasażerskiej żeglugi promowej w Europie,"żegluga promowa, przewozy pasażerskie, transport morski",173
96,4165,opisProjektuStanWiedzy,"<p align=""justify"">Obecnie na rynku materiałów budowlanych wiele firm prezentuje różnorodne rozwiązania konstrukcyjne i technologiczne z zakresu termoizolacji budynków mieszkalnych i przemysłowych [1,7]. W związku ze zwiększonym zapotrzebowaniem na materiały termoizolacyjne, wiele zakładów podjęło produkcję różnego rodzaju płyt styropianowych, bądź mat z wełny mineralnej. Jakość tych materiałów warunkują odpowiednie normy, wciąż uaktualniane wraz z postępem technologicznym występującym w tej dziedzinie. W celu wyznaczenia przewodności cieplnej danego materiału stosuje się aparaty dwupłytowe wykorzystujące metodę stanu ustalonego przepływu ciepła [3,4,10,11,16,20,21,25]. Są to aparaty rozbudowane, a sam pomiar zajmuje dużo czasu. Nie ma natomiast praktycznych rozwiązań przenośnego systemu pomiarowego, który w krótkim czasie pozwoliłby stwierdzić spełnianie lub nie przez materiał dostarczony na plac budowy, bądź opuszczający linię produkcyjną wymagań normy pod względem wartości współczynnika przewodności cieplnej. Dlatego nasz zespół badawczy podjął prace nad rozwinięciem metod dynamicznych wyznaczania parametrów cieplnych materiałów termoizolacyjnych. Prowadzone badania obejmują także możliwości wykorzystania sztucznych sieci neuronowych do rozwiązania współczynnikowego zagadnienia odwrotnego dla procesów dyfuzji ciepła w wybranych modelach stanowisk pomiarowych, które wykorzystywałby teorię stanu nieustalonego przepływu ciepła [15,16]. <br />W laboratoriach najczęściej stosowana jest metoda stykowa, oparta na ustalonych warunkach wymiany ciepła, wykorzystująca aparaty płytowe [10,17,20]. Metoda ta pozwala wyznaczyć jedynie współczynnik przewodzenia ciepła badanego materiału, wymaga użycia dużych i ciężkich systemów pomiarowych oraz długiego odcinka czasu na dokonanie pomiaru. Dlatego też, podejmowane są pracę nad stworzeniem koncepcji systemów pomiarowych, nie posiadających takich ograniczeń jak obecnie wykorzystywane aparaty do wyznaczania parametrów cieplnych materiałów termoizolacyjnych. <br /></p>",07/01/10,23,Określenie możliwości zastosowania sztucznych sieci neuronowych do wyznaczania parametrów cieplnych materiałów termoizolacyjnych w metodzie pomiarowej wykorzystującej sondę cieplną,"współczynnik przewodzenia ciepła, współczynnik wyrównywania temperatury, sieci neuronowe, materiały termoizolacyjne",183
97,4183,opisProjektuStanWiedzy,"<font size=""3""><font face=""Times New Roman"">
<p class=""MsoNormal"" style=""TEXT-JUSTIFY: inter-ideograph; MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.4pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify"">Od odkrycia w badaniach transmisyjnej mikroskopii elektronowej (TEM) przez I. Ijimę w 1991 roku wielościennych nanorurek węglowych (MWCNT) i dwa lata p&oacute;źniej jednościennych nanorurek węglowych (SWCNT), struktury te skupiły żywą uwagę naukowc&oacute;w na całym świecie. Posiadają one unikalne właściwości mechaniczne, elektryczne, czy optyczne. Bardzo szybko nanorurki węglowe stały się multidyscyplinarnym obiektem badań chemik&oacute;w, fizyk&oacute;w, biochemik&oacute;w, biofizyk&oacute;w i specjalist&oacute;w z zakresu inżynierii materiałowej. Obecnie przewiduje się bardzo szerokie zastosowanie nanorurek węglowych, w szczeg&oacute;lności w takich dziedzinach jak nanoelektronika, czy medycyna. Obecne zastosowania nanorurek węglowych do produkcji sprzętu sportowego czy opon, wskazują, że wiele z potencjalnych zastosowań może być wdrożonych do przemysłu do produkcji na skalę masową. Budowane są prototypy urządzeń, np. baterie o kilkakrotnie dłuższym czasie życia niż konwencjonalne, przenośne rentgenografy promieni rentgenowskich, lampy, ekrany (technologia SED). Kolejną, bardzo ciekawą i obiecującą dziedziną jest medycyna, gł&oacute;wnie w zakresie traktowania nanorurek węglowych jako nanokontener&oacute;w, kt&oacute;re wypełnione terapeutykiem mogą dostarczać leki do ściśle określonego miejsca w organizmie. Stabilna dyspersja CNT w wodnych roztworach stanowi jednak ważną barierę&nbsp;do ich biomedycznych zastosowań. CNT są praktycznie nierozpuszczalne w żadnym rozpuszczalniku i rozw&oacute;j ostatnich badań nad funkcjonalizacją nanorurek otworzył możliwość do badania ich potencjalnego zastosowania w medycynie. Możliwość tworzenia kompleks&oacute;w pomiędzy nanorurkami, a r&oacute;żnego typu polimerami lub modyfikacja ścian nanorurek poprzez organiczną funkcjonalizację drastyczne poprawiły rozpuszczalność nanorurek. W zależności od sposobu funkcjonalizacji (kowalencyjnego bądź niekowalencyjnego) oraz grup funkcyjnych dołączonych do lub oddziałujących z nanorurkami, może być modelowana rozpuszczalność w r&oacute;żnych rozpuszczalnikach. Jako konsekwencja tego działania może być rozważany szeroki wachlarz potencjalnych zastosowań , a w tym używanie nanorurek jako nośnik&oacute;w do wzrostu neuronowego [1-3], nośnik&oacute;w do adhezji liposacharyd&oacute;w [4], do naśladowania kom&oacute;rek membranowych [5], blokad kanał&oacute;w jonowych i układ&oacute;w transportujących np. leki [6,7]. <o:p></o:p></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.4pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify"">Ze względu na fakt, że nanorurki są praktycznie nierozpuszczalne lub rozpuszczalne w niskim stopniu, celowym dla integracji technologii nanorurek z aplikacjami biologicznymi niezbędne jest prowadzenie badań nad wzrostem ich rozpuszczalności (dyspergowalności) szczeg&oacute;lnie w roztworze wodnym. Prowadzi się to dzisiaj kilkoma metodami dyspersji i oczyszczania.&nbsp;Techniki te można podzielić na dwie grupy. Do pierwszej grupy należy procedura polegająca na niekowalencyjnej funkcjonalizacji nanorurek z surfaktantami, kwasami nukleinowymi, peptydami, polimerami i oligomerami [11-15]. Poważną zaletą tego procesu jest zachowanie struktury elektronicznej nanorurek,&nbsp;co jest ważne do ich użycia jako biosensor&oacute;w. Do drugiej grupy natomiast zaliczamy procedurę polegającą na kowalencyjnej funkcjonalizacji nanorurek [16]. Wstępnie, nanorurki są tutaj utlenione, by wygenerować pewną ilość grup karboksylowych, a następnie tworzy się poprzez nie wiązania z innymi cząsteczkami. Alternatywnie ściany nanorurek mogą być bezpośrednio sfunkcjonalizowane za pomocą reakcji addycji. </p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.4pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify"">Niekowalencyjna dyspersja nanorurek w roztwor...",07/01/30,23,Opracowanie technologii syntezy i funkcjonalizacji nanorurek węglowych do zastosowań biomedycznych,"nanobiotechnologia, zastosowania biomedyczne nanorurki węglowe stabilna dyspersja, funkcjonalizacja nanorurek węglowych",173
98,4203,opisProjektuStanWiedzy,"<p align=""justify""><strong><em><u>Wprowadzenie.</u></em></strong></p>
<div align=""justify"">Fenomenem <strong><u>plastyczności neuronalnej</u></strong> określa się og&oacute;ł zmian zachodzących w kom&oacute;rkach nerwowych wspomagających adaptację do nowych (najczęściej niekorzystnych) warunk&oacute;w środowiskowych. Będące swoistą &bdquo;linią obronną&rdquo; neuronalne procesy przystosowawcze mogą zachodzić fizjologicznie (wzrost, dojrzewanie i starzenie się neuron&oacute;w) jak i w trakcie niekt&oacute;rych stan&oacute;w patologicznych (takich jak np. uszkodzenie wypustek neuron&oacute;w czy też procesy zapalne). Chociaż charakter zmian zachodzących w neuronach wydaje się być kompleksowy, dotychczas za najistotniejsze uznano zmiany ich morfologii, cech elektrofizjologicznych oraz zmiany w zawartości neurotransmiter&oacute;w i neuromodulator&oacute;w (tzw. kodowaniu chemicznym). Sama jednak wiedza dotycząca transformacji neuron&oacute;w jest dopiero co odkrywana i sukcesywnie uzupełniana. Taki stan rzeczy wynika także z faktu, iż najczęściej przedmiotem badań jest przebieg zmian w ekspresji neurotranmiter&oacute;w jakie zachodzą w odniesieniu do neuron&oacute;w czuciowych (Helke i Rabchevsky, 1991; H&ouml;kfelt i in., 1994) oraz części jelitowej autonomicznego układu nerwowego (z ang. <em>enteric nervous system</em>; ENS) (Ekblad i in., 1996, 1998) podczas gdy neurony wsp&oacute;łczulne oraz przywsp&oacute;łczulne budzą proporcjonalnie mniejsze zainteresowanie. Tzw. &bdquo;reakcja ciała kom&oacute;rkowego&rdquo; (<em>cell body reaction</em>) neuronu prowadząca do cytoplazmatycznego wzrostu ekspresji określonych substancji biologicznie aktywnych jest prawdopodobnie podstawą przetrwania uszkodzonych kom&oacute;rek i w konsekwencji warunkiem do ich dalszej regeneracji. Interesująco wzrost ekspresji związk&oacute;w o charakterze neuroprotekcyjnym jest skorelowany ze znaczną redukcją poziomu substancji nieprzydatnych do ratowania zagrożonej kom&oacute;rki nerwowej (Zigmond i in., 1996). Jednak sam schemat zmian w kodzie chemicznym neuron&oacute;w jest niejednorodny i może zależeć przede wszystkim od rodzaju uszkodzenia, charakteru samych neuron&oacute;w, a także od gatunku zwierzęcia. Dla przykładu uszkodzenie wł&oacute;kna nerwowego (przerwanie transportu aksonalnego) neuron&oacute;w ENS powoduje wzrost ekspresji wazoaktywnego poplipeptydu jelitowego (VIP) oraz jego mRNA (Ekblad i in., 1996) podczas gdy w trakcie atrofii jelit poziom VIP znacznie maleje (Ekelund i Ekblad, 1999). Podobnie poziom neuronalnej izoformy syntetazy tlenku azotu (nNOS) wzrasta po aksotomii (Ekblad i in., 1996) ale pozostaje niezmienny w trakcie hipertrofii jelita (Ekblad i in., 1998). Zachowanie niekt&oacute;rych substancji wydaje się być niezależne od charakteru neuronu. Poziom galaniny oraz VIP wzrasta zar&oacute;wno po uszkodzeniu wł&oacute;kien neuron&oacute;w czuciowych (H&ouml;kfelt i in., 1994), wsp&oacute;łczulnych (Schreiber i in., 1994; Klimaschewski 1997) jak i neuron&oacute;w ENS (Ekblad i in., 1996). Stosunkowo jednak najmniej poznano zmiany&nbsp;jakie zachodzą w zawartości neurotransmiter&oacute;w&nbsp;neuron&oacute;w przywsp&oacute;łczulnych i wiedza ta jest wyjątkowo fragmentaryczna. Jak wykazano, kultywowane neurony przywsp&oacute;łczulne pochodzące z tchawicy świnki morskiej zachowują zdolność do produkcji i uwalniania acetylocholiny oraz wykazują obecność funkcjonalnego hamującego receptora muskarynowego M<sub>2</sub> (Fryer i in., 1996). Neurony unerwiające naczyni&oacute;wkę oka ze zwoju rzęskowego wykazują wzrost ekspresji somatostatyny (funkcjonalny kotransmiter acetylocholiny) wyłącznie wtedy gdy są kultywowane w obecności makromolekuł produkowanych przez kom&oacute;rki warstwy naczyniowej oka (<span>Coulombe i Nishi, 1991) Z kolei po sympatektomii ekspresja nNOS w neuronach zwoju skrzydłowo-podniebiennego znacząco spada (Warn i in., 1997), zaś przywsp&oacute;łczulne neurony sercowe hodowane in vitro wykazują znaczący wzrost eksp...",07/01/25,23,Wpływ hodowli in vitro na kodowanie chemiczne przywspółczulnych neuronów zwoju skrzydłowo-podniebiennego owcy,"plastyczność neuronów, hodowle neuronalne, immunocytochemia, kodowanie chemiczne, zwój skrzydłowo-podniebienny, owca",173
99,4265,opisProjektuStanWiedzy,"<p align=""left""><font size=""2"">Matematyczne modelowanie i symulacja procesu (computer aided process engineering &ndash; CAPE) są coraz częściej stosowane do r&oacute;żnych dziedzin i r&oacute;żnych przemysł&oacute;w. Zagadnienia modelowania i symulacji komputerowej są r&oacute;wnież stosowane w przemyśle papierniczym. Przykładem może tu posłużyć PulpSim (Simulation System for Pulp and Paper Industry) bądź inne. Są to jednak większości aplikacje modelujące i symulujące cały układ produkcyjny i mają charakter komercyjny. Brak jest szczeg&oacute;łowych przypadk&oacute;w modelowania i symulacji samego młyna [8][9][12][18]. <br />Przebieg mielenia podobnie jak przebieg innych złożonych proces&oacute;w technologicznych, zależy od wielu parametr&oacute;w i czynnik&oacute;w, kt&oacute;re można podzielić na systemowo-konstrukcyjne związane z układem mielenia i jego wyposażeniem oraz technologiczne. <br />Do pierwszej grupy czynnik&oacute;w należą: stosowany system mielenia (okresowy, ciągły, okresowo-cykliczny), liczba urządzeń mielących i ich podział na zespoły (rafinowanie, mielenie właściwe, domielanie), system połączeń młyn&oacute;w i kadzi oraz charakterystyka stosowanych urządzeń (rodzaj, typ, prędkość obwodowa, elementy mielące itp.). Powyższych czynnik&oacute;w nie można zmieniać w trakcie bieżącej eksploatacji układu mielenia. Istotne są więc czynniki technologiczne związane z pracą pojedynczego młyna. Do najbardziej istotnych parametr&oacute;w drugiej grupy możemy zaliczyć: <br />- właściwości masy makulaturowej, <br />- stężenie masy, <br />- temperaturę masy, <br />- rodzaj i zawartość r&oacute;żnych substancji w masie.&nbsp;</font></p>
<font size=""2"">
<p align=""center""><img alt="""" src=""/OSFImageLoader.do?idImageDB=7393"" /><br /></p>
</font>
<p align=""center""><font size=""2""></font></p>
<p align=""center""><font size=""2"">Rys. 1. Parametry procesu mielenia mas makulaturowych w młynie tarczowym</font></p>
<p align=""justify""><font size=""2"">Na rysunku 1 przedstawiono najważniejsze czynniki związane z procesem mielenia a jednocześnie mającymi wpływ na pob&oacute;r mocy w trakcie mielenia oraz efektywność procesu. Jak widać proces ten jest złożony. Ujęcie modelowe takiego procesu jest bardzo trudne jednak możliwe po przyjęciu odpowiednich założeń.</font></p>
<p align=""justify""><font size=""2"">Problem naukowy, kt&oacute;ry podejmuje projekt badawczy jest obecnie bardzo intensywnie analizowany i opisywany w literaturze fachowej. Jest jednym z kilku kierunk&oacute;w rozwoju badań nad procesem mielenia (Cele badań nad procesem mielenia sformułowane przez Page`a). Kierunek ten opisuje teorię oddziaływania mielenia na wł&oacute;kna, a w szczeg&oacute;lności oddziaływania sił na pojedyncze wł&oacute;kna i ich reakcji (właściwości gotowego papieru) na te siły [3][8][17][24]. Projekt badawczy będzie uwzględniał r&oacute;wnież zagadnienie wieloparametrowego opisu procesu mielenia [11]. Ideę mechanizmu procesowo-energetycznego zgodną z tymi teoriami przedstawia rysunek 2.</font></p>
<font size=""2"">
<p align=""center""><img alt="""" src=""/OSFImageLoader.do?idImageDB=7526"" /></p>
<p align=""center"">Rys. 2. Oddziaływania na wł&oacute;kna podczas mielenia - podstawowe procesy i związane z nimi moce procesowe </p>
<p align=""justify""><br />Analizowane w projekcie zagadnienia korespondują z badaniami i teoriami podawanymi m.in. przez Martineza i Kerekesa oraz Lumiainena. Projekt bazuje zatem na nowych teoriach i wynikach badań. W ramach prac badawczych teorie te zostaną uzupełnione o własne dokonania badawcze. Pozwoli to ostatecznie opracować własny model pracy młyna tarczowego i przeprowadzić symulację jego działania z uwzględnieniem jego parametr&oacute;w pracy, poborem mocy oraz jakością uzyskiwanego papieru. Dotychczas przeprowadzone pr&oacute;by opracowania takiego modelu charakteryzowały się dużym stopniem uproszczenia [12][13][14][17]. <br /></p>
</font>
<p align=""justify""><font size=""2""></font></p>
<p></p>
<p></p>",07/01/29,23,Energooszczędny proces rozdrabniania mas włóknistych w młynach,"masa makulurowa, młyn, rozdrabnianie, energooszczędność",173
100,4320,opisProjektuStanWiedzy,"<p style=""text-align: justify;"" class=""MsoNormal""><span style=""font-size: 10pt; font-family: Arial;"">Dotychczasowe badania dotyczące zabytków fortyfikacji dzielą się głównie na opracowania dotyczące ich historii (studia historyczne poszczególnych obiektów, obszarów obronnych czy szkół fortyfikacyjnych) oraz opisy działań adaptacyjnych i rewitalizacyjnych z punktu widzenia technologii konserwacji i doktryny konserwatorskiej. Nowatorskim aspektem pracy jest połączenie problemu rewitalizacji fortyfikacji zależnej od ich typologii z zarządzaniem i tematyką partycypacji społecznej. W problematyce zarządzania dziedzictwem kulturowym w Polsce jest ten specjalistyczny dział słabo dotąd zbadany. Mimo iż autorka zamierza odwołać się do bogatych zachodnioeuropejskich doświadczeń na polu partycypacji w procesach rewitalizacji, na gruncie polskim systematyczne badanie tego aspektu reprezentuje nowe podejście do tego zagadnienia. Takie ujęcie ma szanse znacznie wpłynąć na efektywność podejmowanych działań praktycznych i w konsekwencji poprawić stan zabytków fortyfikacji. <o:p></o:p></span></p>",07/01/23,23,Rewitalizacja krajobrazów fortyfikacyjnych jako przedmiot działania organizacji pozarządowych,"krajobraz fortyfikacyjny, partycypacja społeczna, organizacje pozarządowe, rewitalizacja",173
101,4321,opisProjektuStanWiedzy,"<p align=""justify""><strong><em><font face=""Times New Roman"" size=""3"">Jaki oryginalny wkład wniesie rozwiązanie postawionego problemu do dorobku danej dyscypliny naukowej w kraju i na świecie?</font></em></strong></p>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Oryginalność przedstawionego problemu polega na tym, że uzyskane wyniki badań pozwolą przedstawić argumenty, kt&oacute;re staną się podstawą do określenia wartości granicznych dla tej mikotoksyny w środkach żywienia (karmach) dla ps&oacute;w a przez to wyeliminować czynnik sprzyjający występowaniu r&oacute;żnych perturbacji w układzie rozrodczym suk &ndash; dysfunkcje hormonalne, ropomacicze czy niepłodność. Stworzą możliwości do określenie charakteru i mechanizmu działania r&oacute;żnych stężeń zearalenonu występujących w karmie na stan czynnościowy układu immunologicznego suk. Pozwolą na określenie wpływu kr&oacute;tkoterminowego podawania zearalenonu w niskich dawkach, na stan układu rozrodczego suk z uwzględnieniem objaw&oacute;w klinicznych oraz obrazu hematologicznego i biochemicznego surowicy krwi. Wykonane zostanie oznaczenie zawartości zearalenonu i gł&oacute;wnych metabolit&oacute;w (&alpha;-zearalenolu i &beta;-zearalenolu) we krwi zwierząt.</font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Najtrudniejsza będzie interpretacja uzyskanych wynik&oacute;w z racji <strong><em>braku tego rodzaju</em></strong> <strong><em>prac na świecie</em></strong>, przez co będą one niepor&oacute;wnywalne ale za to pionierskie. </font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font>&nbsp;</div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3""></font></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3"">Czy w kraju i na świecie jest to problem nowy czy kontynuowany i w jakim zakresie weryfikuje utarte poglądy?</font></em></strong></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><strong><em><font face=""Times New Roman"" size=""3""></font></em></strong></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Ze względu na niezwykle dużą toksyczność mikotoksyn, już w około sześćdziesięciu krajach świata wprowadzone są urzędowe limity maksymalnych tolerowanych wielkości niekt&oacute;rych mikotoksyn (jak np. aflatoksyn B1, B2, G1, G2 czy M1 oraz ochratoksyny A) chcąc zarządzać ryzykiem związanym z nowoczesną produkcją pasz. Dla niekt&oacute;rych mikotoksyn rozrzut maksymalnych wartości dopuszczalnych w r&oacute;żnych krajach jest olbrzymi np. mikotoksyny T-2 od 1 do 12 mg/kg paszy lub od 0,08 do 0,64 mg/kg mc/dzień. W 1995r. zaproponowano np. dla ochratoksyny A i patuliny tymczasowe tolerowane wartości pobrania wynoszące 0,0001 mg ochratoksynyA/kg mc/tydzień czy 0,0004 mg patuliny/kg mc/dzień.</font></div>
<div align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;</font><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; W chwili obecnej obowiązują nas w Unii Europejskiej następujące akty prawne: <strong>(I)</strong> Rozporządzenie Komisji (WE) NR 856/2005 z dnia 6 czerwca 2005 r. zmieniające rozporządzenie (WE) nr 466/2001 w ...",07/01/22,23,Obraz zmian w wybranych tkankach u suk jako wynik krótkoterminowej mikotoksykozy zearalenowej,"zearalenon, suki, układ rozrodczy, układ immunologiczny, metabolizm",173
102,4329,opisProjektuStanWiedzy,"<p>1. Oryginalny wkład projektu.</p>
<ul>
    <li>Oryginalny wkład mojej pracy&nbsp;widzę w&nbsp;holistycznym ujęciu problemu interakcji między strukturami władzy a społeczeństwem miejskim wczesnego Bizancjum w warunkach zakł&oacute;cenia porządku publicznego, spowodowanego kryzysami o rozmaitym podłożu.&nbsp; </li>
    <li>
    <div align=""justify"">Kolejnym&nbsp;novum wprowadzonym do nauki będzie skonstruowanie modelu prawidłowości zachodzących w tego typu przypadkach (tzw. &quot;anatomia buntu&quot;).&nbsp; </div>
    </li>
    <li>Dodatkowym oryginalnym wkładem będzie r&oacute;wnież uzupełnienie dotychczas istniejących analiz źr&oacute;deł antycznych i bizantyńskich traktujących zagadnienia będące punktem&nbsp;zaintersowania moich rozważań. </li>
    <li>W końcu,&nbsp;istotnym wkładem będzie&nbsp;zestawienie i krytyka istniejących pogląd&oacute;w naukowych. </li>
</ul>
<p>2. Dotychczasowy stan badań w nauce krajowej i zagranicznej.</p>
<p align=""justify"">Ani w nauce polskiej, ani w zagranicznej nie powstało jak dotąd całościowe opracowanie bunt&oacute;w i rozruch&oacute;w, mających miejsce w największych miastach wczesnego Bizancjum (Antiochia nad Orontesem, Aleksandria przy Egipcie, Konstantynopol, Tesalonika). W fachowej literaturze można napotkać jedynie pojedyncze artykuły, omawiające konkretne bunty. O powstaniach i rozruchach wspomina się także w syntetycznych ujęciach historii wspomnianych metropolii. Powstały także analizy tumult&oacute;w ludowych w całej epoce wczesnobizantyńskiej (IV - VI w. n.e.). Często są to prace stare, wymagające korekt i uzupełnienień. Dla przykładu, syntezy dotyczące azjatyckiej części imperium pochodzą z&nbsp;okresu od lat&nbsp;50&nbsp;do początku lat&nbsp;70. XX w. (prace P. Petita, G. Downeya, J. Liebeschuetza, G. Kurbatowa), a prace przyczynkarskie ukazują się co prawda stale, ale nie rozwiązują postawionego przeze mnie problemu. Gdy chodzi bizantyński&nbsp;Egipt, ostatnia&nbsp;monografia (pi&oacute;ra Ch. Haasa)&nbsp;ukazała się w roku 1997.&nbsp;Interesujący mnie okres IV wieku&nbsp;omawia zbyt og&oacute;lnie (praca poświęcona jest IV i V stuleciu). &nbsp;&nbsp;</p>
<p>Podsumowując, podjęcie proponowanego tematu ma&nbsp;na celu </p>
<ul>
    <li>uzupełnienie dotkliwej luki istniejącej w fachowej literaturze, </li>
    <li>a także zweryfikowanie&nbsp;niekt&oacute;rych obecnych w niej pogląd&oacute;w, szczeg&oacute;lnie w zakresie
    <ul>
        <li>wpływu czynnik&oacute;w religijnych i gospodarczych na zaistnienie niepkoj&oacute;w społecznych w miastach bizantyńskich, </li>
        <li>kwestii postawy władz państwowych i miejskich wobec zaburzeń społecznych, </li>
        <li>
        <div align=""justify"">środk&oacute;w&nbsp;i służb porządkowych będących do dyspozycji władz miejskich i państwowych, </div>
        </li>
        <li>oraz tak zwanej &quot;anatomii buntu&quot; - prawidłowowści cechujacych niepokoje społeczne we wczesnym Bizancjum. </li>
    </ul>
    </li>
</ul>
<p>Dodatkowo por. znaczenie projektu oraz literatura. <br /><br /></p>
<p>&nbsp;</p>",07/01/30,23,Bunty i rozruchy w miastach wczesnego Bizancjum (IV w. n.e.),rozruchy; miasta; Bizancjum; wczesne Bizancjum,173
103,4335,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""margin: 5pt 0cm; text-align: justify;""><span style="""">Ze względu na znaczenie aerozolu prowadzone są obecnie globalne obserwacje jego właściwości (gł&oacute;wnie grubości optyczne) przez naziemne sieci pomiarowe o zasięgu kontynentalnym (EARLINET, <strong style="""">E</strong>uropean <strong style="""">A</strong>erosol <strong style="""">R</strong>esearch <strong style="""">Li</strong>dar <strong style="""">Net</strong>work) jak i globalnym (AERONET, <strong>Ae</strong>rosol<strong> Ro</strong>botic <strong>Net</strong>work) oraz przy pomocy satelit&oacute;w (np. MODIS na platformie satelit&oacute;w Aqua i Terra). Projektuje się i prowadzi duże eksperymenty mające na celu zbadanie właściwości aerozolu oraz jego oddziaływania na klimat i zdrowie ludzkie, np. ACE I, ACE II, INDOEX. Bada i modeluje się r&oacute;wnież rozprzestrzenianie aerozolu w r&oacute;żnych skalach przestrzennych. Od transportu aerozolu przez tak zwaną bryzę miejską, po transport pył&oacute;w saharyjskich w obrębie Europy. W związku z tym dobrze jest rozwinięta metodyka wyznaczania właściwości mikrofizycznych i optycznych aerozolu przy pomocy technik zdalnych (fotometry słoneczne, lidary), satelitarnych jak i <em>in situ</em> (impaktory, liczniki cząstek). Każda z tych technik ma jednak swoje ograniczenia i może prowadzić do błęd&oacute;w w specyficznych przypadkach. Ponadto każda z tych metod jest cały czas udoskonalana.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style="""">Na terenie Polski regularnie monitorowany jest pył zawieszonego PM&nbsp;10 i PM&nbsp;2,5 oraz zawartość zanieczyszczeń gazowych w ramach sieci prowadzonej przez Państwową Inspekcję Ochrony Środowiska. Pomiary pyłu zawieszonego prowadzone są wyłączni w miastach i rejestruje się w tych punktach przekroczenia dopuszczalnych poziom&oacute;w. Niestety z powody braku pomiar&oacute;w na terenach niezurbanizowanych nie można odnieść tych pomiar&oacute;w do poziomu tła. Opr&oacute;cz tego w Centralnym Obserwatorium Geofizycznym IGF&nbsp;PAN w Belsku prowadzone są regularne obserwacje właściwości optyczne aerozolu przy pomocy lidaru w ramach europejskiej sieci lidarowej (EARLINET) oraz przy pomocy fotometr&oacute;w słonecznych w ramach sieci AERONET. Poza regularnymi obserwacjami przeprowadza się kampanie pomiarowe mające na celu zadanie właściwości aerozolu danego typu. Należy tu wymienić obserwację aerozolu morskiego prowadzone przez IOP&nbsp;PAN w Sopocie, pomiary aerozolu miejskiego przy pomocy lidaru prowadzone przez Zakład Optyki IFD&nbsp;UW oraz pomiary aerozolu pustynnego prowadzone przez IGF&nbsp;UW.<o:p></o:p></span></p>
<p class=""MsoNormal"" style=""text-align: justify;""><span style="""">Prowadzone do tej pory badania, zar&oacute;wno regularne jak i kampanie pomiarowe, nie pozwoliły na kompleksowe zbadanie zmienności aerozolu w obrębie aglomeracji miejskiej. W związku z tym połączenie kilku technik pomiarowych w kilku stacjach pozwoli na ocenę wpływu aglomeracji warszawskiej na właściwości aerozolu oraz wyznaczenie średnich wartości dla poszczeg&oacute;lnych stacji pomiarowych.<o:p></o:p></span></p>",07/01/29,23,Badanie wpływu aglomeracji miejskiej na właściwości optyczne aerozolu.,"aerozol, właściwości optyczne, promieniowanie słoneczne",173
104,4337,opisProjektuStanWiedzy,"<p style=""text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style="""">Ilekroć czytamy jakiś tekst lub słuchamy czyjejś wypowiedzi, poszczególne słowa trafiającego do nas przekazu muszą zostać zintegrowane ze sobą, co w efekcie umożliwia ich interpretację. Proces ten bazuje zarówno na strukturalnych (syntaktyka) jak i znaczeniowych (semantyka) cechach poszczególnych wyrazów. Obydwa rodzaje informacji są niezbędne do poprawnego rozumienia języka. Jedną z największych kontrowersji w psycholingwistyce wzbudza pytanie, kiedy i w jaki sposób system poznawczy korzysta z syntaktycznego i semantycznego źródła informacji. <o:p></o:p></span></p>
<p style=""text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""""><span style="""">      </span>Na przestrzeni wielu lat zaproponowano kilka modeli procesu przetwarzania zdań. Łączy je założenie, że do zrozumienia sensu całego zdania potrzebne jest skonstruowanie jego struktury syntaktycznej. <o:p></o:p></span></p>
<p style=""text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""""><span style="""">      </span>Pierwsza klasa modeli to modele ‘syntax-first’, czerpiące źródło z poglądów Chomskiego </span><span style="""">(1986)</span><span style=""""> na przetwarzanie języka. Zakładają, że pierwszym krokiem w rozumieniu zdania jest zbudowanie jego struktury syntaktycznej i że odbywa się to bez udziału semantyki. Ta ostatnia jest wykorzystywana dopiero na drugim etapie, przede wszystkim do wyboru spośród strukturalnych niejednoznaczności, bądź do pokierowania procesem reanalizy strukturalnej zdania, gdy wstępna interpretacja strukturalna okazała się nieprawidłowa. <o:p></o:p></span></p>
<p style=""text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""""><span style="""">      </span>Wpływowym przykładem modelu tej klasy jest model Angeli Friederici </span><span style="""">(1995; 2002)</span><span style="""">. Jego ogromną zaletą jest to, że wiąże on poszczególne etapy przetwarzania zdań z odpowiadającymi im komponentami potencjałów wywołanych EEG. Autorka wyróżnia w nim 3 etapy przetwarzania poszczególnych słów, stanowiących część większej wypowiedzi. W pierwszym etapie, system poznawczy buduje ogólną strukturę syntaktyczną, na podstawie informacji o kategorii słowa; pojawienie się słowa należącego do niedopuszczalnej w danym miejscu kategorii gramatycznej związane jest z wystąpieniem komponentu ELAN (<em>Early Left Anterior Negativity</em>). Drugi etap związany jest z przetwarzaniem informacji leksykalno-semantycznych (błąd w tym zakresie wywołuje komponent N400) oraz morfo-syntaktycznych (błąd wywołuje komponent <em>Left Anterior Negativity</em> - LAN). Efektem tego etapu jest ustalenie ról tematycznych w zdaniu, takich jak agens czy pacjens. Podczas trzeciego etapu następuje końcowa integracja wszystkich dotychczasowych informacji i wtedy też, w razie trudności, może zostać uruchomiony proces reanalizy całego zdania w celu znalezienia lepszej interpretacji ról tematycznych lub składni (związany z komponentem P600). Ważną cechą wszystkich modeli tej klasy jest założenie o istnieniu niezależnych modułów, odpowiedzialnych za przetwarzanie semantyki i syntaktyki.<o:p></o:p></span></p>
<p style=""text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""""><span style="""">      </span>W przeciwieństwie do modeli ‘syntax first’, modele przetwarzania informacji z ograniczeniami (constraint-based) zakładają jej wykorzystanie wcześniej – już w czasie budowy ogólnej struktury składniowej. Jedna z bardziej popularnych teorii tej klasy została zaproponowana przez Macdonald ze wsp. </span><span style="""">(1994)</span><span style="""">. Autorzy zakładają, że przeczytanie każdego kolejnego słowa w zdaniu aktywuje bogaty zbiór informacji związanych z nim. Na przykład słowo <em>Matka</em> w zdaniu <em>Matka przytula dziecko</em>, wzbudza nie tylko informację o kategorii gramatycznej słowa, ale także dodatkowe informacje leksykalne: to że to słowo jest...",07/01/25,23,Poznawcze mechanizmy recepcji zdań - automatyczna predykcja informacji semantycznych na podstawie wskazówek syntaktycznych. Badania przy użyciu potencjałów wywołanych EEG na języku polskim,"psycholingwistyka, potencjały wywołane EEG, mechanizmy recepcji zdań, interakcja syntaktyki i semantyki w recepcji języka, automatyczna predykcja słów w czasie czytania zdań, komponent P600, komponent N400, komponent LAN, żywotność gramatyczna",183
105,4345,opisProjektuStanWiedzy,"Przedstawiony projekt jest rozwiązaniem pionierskim w skali kraju, a jego gł&oacute;wnymi atutami&nbsp;są: aktywne wykorzystanie możliwości Internetu w tworzeniu regionalnego zasobu bazy naukowej,&nbsp;aktywizowanie wyspecjalizowanych zespoł&oacute;w badawczych, obniżenie koszt&oacute;w przedsięwzięcia&nbsp;dzięki&nbsp;użytkowaniu już istniejących podręcznych zasob&oacute;w sprzętowych&nbsp;będących&nbsp;w dyspozycji poszczeg&oacute;lnych wykonawc&oacute;w projektu.&nbsp;&nbsp;Zar&oacute;wno na krajowych jak i zagranicznych stronach internetowych nie znaleziono dotąd podobnego rozwiązania&nbsp;- interaktywnego, interdyscyplinarnego narzędzia dla tego rodzaju badań.",07/01/12,23,Baza internetowa biblioteki akademickiej informacyjną platformą naukową badań nad dziejami lecznictwa w regionie,Informacja naukowa - Internet - Górny Śląsk - Dzieje lecznictwa - Archiwa,173
106,4353,opisProjektuStanWiedzy,"<p style=""text-indent: 35.4pt; line-height: 150%; text-align: justify;"" class=""MsoNormal""><br />    Lotne związki organiczne VOC’s (volatile organic compounds) przedostają się do atmosfery z różnych źródeł, m.in. ze spalarni odpadów medycznych i niebezpiecznych i stanowią  poważne zagrożenie dla środowiska [1,2]. Katalityczne usuwanie VOC’s polega na ich całkowitym dopaleniu do CO<sub>2</sub> i pary wodnej, co jest szczególnie korzystne przy ich niewielkim stężeniu w gazach (&gt; 5000 ppm) [3], a temperatura procesu może nie przekraczać 400<sup>o</sup>C. Ze względu na różnorodność utlenianych związków i ich stężeń, warunki procesu degradacji (temperatura, katalizator, szybkość przepływu czy konstrukcja i materiał aparatury) muszą być starannie dobrane. Najczęściej stosowanymi katalizatorami spalania VOC’s są platyna [4] i pallad [5], znane są jednak przypadki stosowania tlenków niklu, magnezu, miedzi, żelaza, manganu, ceru i chromu w różnej kompozycji oraz różnej proporcji ilościowej [6]. W przypadku utleniania związków chlorowcopochodnych mogą również, oprócz chlorowodoru powstawać bardzo groźne polichlorodibenzodioksyny oraz polichlorofurany. Katalityczny rozkład tych związków możliwy jest wobec katalizatorów: V<sub>2</sub>O<sub>5</sub>/TiO<sub>2</sub> [7], MnO<sub>x</sub>/Al<sub>2</sub>O<sub>3 </sub>[8], MnO<sub>x</sub>/TiO<sub>2</sub> [9 ], ZrO<sub>2</sub> [10], a także układów bimetalicznych: Pd-Pt [5,11], Bi-Pd [12], Ni-Cu [13], Pd-Ag [14]. Stosuje się rownież katalizatory chromowe i wanadowe na różnych nośnikach oraz metale szlachetne: platynę i pallad naniesione na tlenki glinu, krzemu, cyrkonu, TiO<sub>2</sub>-SiO<sub>2</sub>, zeolity, węgiel aktywny [7] czy perowskity zawierające lantanowce. Do najbardziej aktywnych układów tlenkowych w reakcjach utleniania należą katalizatory manganowe oraz cerowe, zarówno nośnikowe, oparte na Al<sub>2</sub>O<sub>3</sub> [8] i nośnikach ułatwiających przebieg procesów utleniania-redukcji lub wpływających na stabilizację wyższych stopni utlenienia manganu czy ceru, np. TiO<sub>2</sub> [9], ZrO<sub>2</sub> [10], jak również katalizatory współstrącane o specyficznych właściwościach strukturalnych [15,16]. Zagadnienie katalitycznego usuwania związków chloroorganicznych omówione jest szczegółowo w monografii Sarbaka [3] oraz innych publikacjach [17,18]. <br /></p>
<br />
<p style=""text-indent: 35.4pt; line-height: 150%; text-align: justify;"" class=""MsoNormal""> W projekcie porównawczo będą wykorzystane trzy techniki nanoszenia materiału aktywnego a) metoda CIM (Clasical Impregnation Method) – klasyczna metoda adsorpcji jonów metalu z roztworu wodnego, b) metoda wymiany jonowej oraz c) metoda DIM (Duble Impregnation Method) – metoda podwójnej impregnacji z wykorzystaniem EDTA. Formalnie technikę DIM można zaliczyć do metody impregnacyjnej wykorzystującej elementy wymiany jonowej i chemii koordynacyjnej. Techniki te [19] mogą służyć do modelowania właściwości katalitycznych materiałów, gdyż rozkład wielkości krystalitów metalu zastosowanego jako faza aktywna oraz ich struktura jest uzależniona między innymi od sposobu pozyskiwania. W pracach [20,21] opisano szczegółowo to zagadnienie i udowodniono, że zarówno sposób otrzymywania, jak i warunki obróbki (np. kalcynacji lub redukcji) prekursorów katalizatorów mają ogromny wpływ na rozkład wielkości krystalitów metalu. Można więc preparować katalizatory o zbliżonych, lub wręcz identycznych średnich wielkościach krystalitów metalu, lecz o zupełnie odmiennych właściwościach katalitycznych. Wysoką dyspersję takich metali, jak platyna i pallad można osiągnąć stosując tradycyjne techniki preparatyki katalizatorów np. metodę impregnacyjną. Jednak w przypadku niklu, miedzi, manganu, wanadu czy chromu metoda impregnacyjna nie daje dobrych rezultatów i zachodzi konieczność stosowania bardziej skomplikowanych technik [20]. Procesy aktywacji katalizatorów można znacznie przyspieszyć stosując ogrzewanie mikrofalami. W porównaniu z ogrzewaniem tradycyjnym...",07/01/29,23,Katalityczne spalanie VOC oraz VOC-Cl obecnych w gazach pochodzących ze spalania odpadów medycznych i niebezpiecznych,"VOC, VOC-Cl, katalizator, spalanie, ozon, promieniowanie mikrofalowe ",173
107,4371,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><span style=""font-family: Arial;"">Usuwanie mikroorganizmów chorobotwórczych, czyli dezynfekcja, z wody przeznaczonej do spożycia, jest w technologii uzdatniania wody etapem ostatnim. W skali technicznej stosuje się głównie metody chemiczne, które polegają na wprowadzeniu do wody silnych utleniaczy takich jak gazowy chlor, podchloryny, dwutlenek chloru, chloraminy oraz ozon. Rzadziej wykorzystywane jest promieniowanie UV i to na ogół jako czynnik wspomagający ozonowanie [1].  <o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><span style=""font-family: Arial;"">Z drugiej strony, w literaturze przedmiotu do oczyszczania wody pitnej proponuje się stosować wyładowanie elektryczne. Badane są następujące typy wyładowań:<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""margin-left: 64.35pt; text-align: justify; text-indent: -18pt;""><font size=""3""><span style=""font-family: Symbol;"">·<span style=""font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;"">        </span></span><span style=""font-family: Arial;"">do powierzchni cieczy (wyładowanie powierzchniowe),<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""margin-left: 64.35pt; text-align: justify; text-indent: -18pt;""><font size=""3""><span style=""font-family: Symbol;"">·<span style=""font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;"">        </span></span><span style=""font-family: Arial;"">wewnątrz cieczy (wyładowanie objętościowe),<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""margin-left: 64.35pt; text-align: justify; text-indent: -18pt;""><font size=""3""><span style=""font-family: Symbol;"">·<span style=""font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;"">        </span></span><span style=""font-family: Arial;"">mieszane, tj. do powierzchni cieczy i wewnątrz cieczy.<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><span style=""font-family: Arial;"">Badaniami metody wykorzystującej powierzchniowe wyładowanie elektryczne do oczyszczania cieczy zajmowały się zespoły badawcze z Francji, Holandii, USA i Japonii. Wyniki ich prac wykazały, że procesy fizyko-chemiczne aktywowane wyładowaniem są zbliżone do tych, które zachodzą podczas wyładowań w wilgotnym powietrzu (produkcja tlenków azotu i kwasów azotowych). Znaczna część energii wyładowania elektrycznego jest zużywana w procesach zachodzących w fazie gazowej, a tylko niewielka część jest transferowana do procesów związanych z oczyszczaniem wody.<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><span style=""font-family: Arial;"">W Polsce próby stosowania ślizgającego się wyładowania elektrycznego do powierzchni cieczy prowadzone były przez zespół badawczy z Instytutu Chemii Przemysłowej w Warszawie.<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><span style=""font-family: Arial;"">W związku z małą wydajnością energetyczną oczyszczania wody za pomocą wyładowań powierzchniowych, w niniejszym projekcie proponujemy podjąć tematykę zastosowań wyładowań objętościowych do oczyszczania wody pitnej.<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 1cm;""><font size=""3""><strong><span style=""font-family: Arial;"">Wyładowania elektryczne w wodzie, zwane wyładowaniami elektrohydraulicznymi</span></strong><span style=""font-family: Arial;"">, dzielą się na dwa zasad...",07/01/29,23,Oczyszczanie wody z mikroorganizmów za pomocą wyładowań elektrohydraulicznych,"ochrona środowiska, oczyszczanie wody, wyładowania elektryczne, plazma",173
108,4405,opisProjektuStanWiedzy,"<p>&nbsp;&nbsp;&nbsp; W literaturze i przepisach podawane są kryteria potencjałowe ze szczeg&oacute;lnym uwzględnieniem konstrukcji chronionych katodowo. W przypadku konstrukcji typu tunele metra podawany jest zalecany przedział zmian r&oacute;żnicy pomiedzy średnim potencjałem z godzin nocnych (tzw. ciszy nocnej) i średnim potencjałem z godzin szczytu komunikacyjnego (porannego lub popołudniowego) albo średnim z całej doby. Ponieważ podziemne&nbsp; konstrukcje metalowe są izolowane coraz lepszymi materiałami, a dotyczy to szczeg&oacute;lnie rurociąg&oacute;w trazytowych wysokociśnieniowych, w najnowszej normalizacji z tego zakresu odchodzi się od kryteri&oacute;w potencjałowych na&nbsp; rzecz korozymetrii rezystancyjnej - pr&oacute;bek wystawianych na oddziaływanie środowiska wok&oacute;ł&nbsp; konstrukcji, dzięki kt&oacute;rym określa się szybkość korozji. Takie postępowanie związane jest gł&oacute;wnie z faktem występowania zwiększonych oddziaływań indukcyjnych elektroenergetycznych linii przesyłowych prądu przeminnego jak i sieci kolejowych prądu zmiennego na dobrze izolowane rurociągi. Przy dużych gęstościach prądu przemiennego (powyżej 30 A/m<sup><span style=""FONT-FAMILY: Arial"">2</span></sup>) przepływającego pomiędzy metalem konstrukcji a otaczającą go ziemią (elektrolitem) przestają obowiązywać znane kryteria potencjałowe, ponieważ pomimo zapewnienia zalecanego potencjału ochrony w uszkodzeniach izolacji pojawia się korozja elektrochemiczna od prądu przemiennego.<br /></p>
<p>&nbsp;&nbsp;&nbsp; W przypadku tuneli metra, kt&oacute;rych rezystancja przejścia&nbsp; pomiędzy żelazem (zbrojeniem) a ziemią jest&nbsp; minimalna -wynikająca ze stosowania bentonitu pomiędzy żeliwnymi tubingami a ziemią oraz powłok uszczelniających konstrukcje żelbetowe przed wodą -&nbsp;nie należy oczekiwać pojawienia się tak dużych gęstości prądu przemiennego, aby kryteria potencjałowe zostały podważane. Proponowane badania przebiegu potencjału wybranych elektrod powinny potwierdzić, że&nbsp;przyczyna znajduje się w torze pomiarowym i wszystkie proponowane kryteria oceny są zbieżne. W przeciwnym przypadku błąd może występować w realizacji układu pomiarowego, albo zjawiska mają charakter lokalany i wtedy pod dyskusję należy będzie poddać krytreium spadku napięia na konstrukcji tunelu o długości odcinka obszaru zasilania trakcyjnego połączonych elektrycznie podstacji.&nbsp;</p>
&nbsp;",07/01/31,23,Ustalenie przyczyn nietypowych zachowań potencjału niektórych elektrod zainstalowanych w systemie Monitoringu Prądów Błądzących,prądy bądzące; potencjał konstrukcji; cynkowe elektrody odniesienia;,173
109,4411,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt""><span lang=""PL""><o:p><font face=""Times New Roman"" size=""3""><span lang=""PL""><font face=""Times New Roman"" size=""3""><span lang=""PL""><span><span lang=""PL"">Eugleniny są pierwotniakami spokrewnionymi z heterotroficznymi wiciowcami z grup taksonomicznych Kinetoplastida i Diplonemida, tworząc z nimi wsp&oacute;lną grupę Euglenozoa, w obrębie większego taksonu Excavata (Rys 1). Excavata, posiadające barokową strukturę genomu i zadziwiającą r&oacute;żnorodność mechanizm&oacute;w ekspresji gen&oacute;w (Simpson Roger 2004), <span>&nbsp;</span>są na og&oacute;ł uważane za jedno z wcześniejszych odgałęzień drzewa filogenetycznego organizm&oacute;w eukariotycznych (Sogin 1991). Niemniej, rozważana jest r&oacute;wnież kwestia ich bliskiego pokrewieństwa z grupami organizm&oacute;w fotosyntetyzujących takich jak rośliny (Plantae) czy Chromalveolata (Stechmann<span>&nbsp; </span>Cavalier-Smith 2002). </span>Eugleniny mogą być bezbarwne lub zielone, w zależności od ich zdolności do fotosyntezy. Eugleniny bezbarwne dzielą się na dwie grupy, pierwotnie i wt&oacute;rnie heterotroficzne. Te ostatnie, jak niekt&oacute;re gatunki z rodzaj&oacute;w <em>Euglena</em> czy <em>Phacus</em>,<em> </em>utraciły zdolność do fotosyntezy. Eugleniny zielone i wt&oacute;rnie heterotroficzne tworzą grupę monofiletyczną, co sugeruje, że uzyskanie chloroplastu miało miejsce tylko raz w toku ewolucji euglenin (Preisfeld <em>et al</em></span><span lang=""PL"">. 2001). Chloroplasty euglenin zawierają chlorofil <em>a</em> i <em>b</em> i otoczone są potr&oacute;jna błoną, co sugeruje iż są efektem wt&oacute;rnej endosymbiozy, najprawdopodobniej między heterotroficzną eugleniną a zielenicą (Gibbs 1978). Bliski wsp&oacute;łczesny krewny zielenicy będącej prekursorem chloroplast&oacute;w euglenin nie został do tej pory zidentyfikowany, choć wymienia się w tym miejscu takie grupy zielenic jak Ulvophyceae czy&nbsp;Prasinophyceae (Marin 2004). Rozważane są r&oacute;wnież inne, uważane jednak za mniej prawdopodobne, scenariusze uzyskania chloroplast&oacute;w przez eugleniny takie jak możliwość, że chloroplasty euglenin mają wsp&oacute;lne pochodzenie z chloroplastami niekt&oacute;rych ameboidalnych Chlorarachniophyceae z grupy Cercozoa (Cavalier-Smith 1999, Rogers <em>et al.</em> 2007) lub że akt endosymbiozy u Euglenozoa miał miejsce przed rozejściem się Kinetoplastida i Euglenida (Hannaert <em>et al.</em> 2003, El-Sayed<em> et al.</em> 2005).</span></span></font></span> </font></o:p></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt""></p>
<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt""></p>
<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt""></p>
<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt; TEXT-ALIGN: center"" align=""center""><span lang=""PL""><font size=""3""><font face=""Times New Roman""><v:shapetype id=""_x0000_t75"" coordsize=""21600,21600"" o:spt=""75"" o:preferrelative=""t"" path=""m@4@5l@4@11@9@11@9@5xe"" filled=""f"" stroked=""f""><v:stroke joinstyle=""miter""></v:stroke><v:formulas><v:f eqn=""if lineDrawn pixelLineWidth 0""></v:f><v:f eqn=""sum @0 1 0""></v:f><v:f eqn=""sum 0 0 @1""></v:f><v:f eqn=""prod @2 1 2""></v:f><v:f eqn=""prod @3 21600 pixelWidth""></v:f><v:f eqn=""prod @3 21600 pixelHeight""></v:f><v:f eqn=""sum @0 0 1""></v:f><v:f eqn=""prod @6 1 2""></v:f><v:f eqn=""prod @7 21600 pixelWidth""></v:f><v:f eqn=""sum @8 21600 0""></v:f><v:f eqn=""prod @7 21600 pixelHeight""></v:f><v:f eqn=""sum @10 21600 0""></v:f></v:formulas><v:path o:extrusionok=""f"" gradientshapeok=""t"" o:connecttype=""rect""></v:path><o:lock v:ext=""edit"" aspectratio=""t""></o:lock></v:shapetype><v:shape id=""_x0000_i1025"" style=""WIDTH: 431.25pt; HEIGHT: 243.75pt"" type=""#_x0000_t75""><v:imagedata src=""file:///C:\DOCUME~1\jk\LOCALS~1\Temp\msohtml1\01\clip_image001.jpg"" o:title=""euktree""></v:imagedata></v:shape></font></font></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0in 0in 0pt"" align=""center""><span lang=""PL""><o:p><font face=""Times New Roman"" size=""3"">&nbsp;<img style=""WIDTH: 7...",07/02/20,23,Ewolucja ekstrajądrowego genomu zielonych euglenin,"chloroplast, endosymbioza, Euglena, Eutreptia, Eutreptiella, mitochondrium, pyrolityczne sekwencjonowanie DNA",173
110,4417,opisProjektuStanWiedzy,"<p lang=""pl-PL"" align=""justify"" style=""margin-left: 0.68cm; text-indent: 0.53cm; margin-bottom: 0cm; font-style: normal; line-height: 150%;""> <span style=""""><span lang=""pl-PL""><font size=""3""><font face=""Times New Roman CE, serif""><font color=""#000000""><span style=""font-style: normal;"">Nieniszczące metody modalne bazują na znajomości wartości i wektorów własnych analizowanych układów. Najczęściej wykrywanie uszkodzeń polega na porównywaniu parametrów dynamicznych z wartościami określonymi dla stanu wzorcowego tj. bez uszkodzenia [1,2]. Znane są też metody bazujące na porównywaniu krzywizny postaci drgań własnych [3,4]. Proponowana nieniszcząca metoda dzięki zastosowaniu dodatkowego parametru sterującego wprowadzanego do układu (masa, podpora) pozwala na identyfikacje uszkodzenia bez znajomości stanu układu bez uszkodzenia. Podstawy matematyczne metody dla pełnego modelu modalnego podane zostały w pracy K. Demsa i Z. Mroza (2001) </span><em>„Identification of damage in beam and plate structures using parameter-dependent frequency change” </em><span style=""font-style: normal;"">[6].</span></font></font></font></span></span><br /><font color=""#000000""><font face=""Times New Roman CE, serif""><font size=""3"">    Obecnie w pomiarach rzeczywistych obiektów konstrukcyjnych dostępną aparaturą pomiarową rzadko uzyskujemy pełną informacją o badanym obiekcie. Dodatkowo też prawie zawsze uzyskane wyniki są wynikami z błędami pomiarowymi. Wobec tych problemów zastosowanie metody bazującej na pełnym modelu matematycznym i opracowanie skutecznego algorytmu jest utrudnione. Zastosowanie niekonwencjonalnych metod (,,miękkie’’ metody obliczeniowe) przetwarzania niepełnych danych może pozwolić wyeliminować te problemy i umożliwić efektywne wykorzystanie wyników pomiarów. Stosując takie podejście przy ocenie stanu układu możemy ograniczyć się do analizy tylko wybranych parametrów modelu modalnego.</font></font></font></p>",07/01/27,23,Wykrywanie uszkodzeń w układach prętowych z wykorzystaniem zmian parametrów modelu modalnego,"identyfikacja uszkodzeń, model modalny, metoda elementów skończonych, sztuczna sieć neuronowa, symulacja numeryczna, badania eksperymentalne",183
111,4421,opisProjektuStanWiedzy,"<span style=""mso-font-kerning: 0pt""><span style=""mso-spacerun: yes""><font size=""4"">
<p class=""Style1"" style=""MARGIN: 0cm 0cm 0pt; LINE-HEIGHT: normal; tab-stops: 35.4pt"" align=""justify""><font face=""Times New Roman""><span style=""mso-font-kerning: 0pt; mso-bidi-font-size: 12.0pt""><font size=""3""><span style=""mso-spacerun: yes"">&nbsp;</span></font></span><span style=""mso-font-kerning: 0pt; mso-bidi-font-size: 12.0pt""><font size=""3""><span style=""mso-spacerun: yes"">&nbsp;&nbsp;</span><span style=""mso-spacerun: yes"">&nbsp;</span></font></span></font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font face=""Times New Roman"">&nbsp; Promieniowanie ultrafioletowe (UV) jest to promieniowanie optyczne o częstotliwościach pomiędzy zakresem światła widzialnego a promieniowaniem rentgenowskim, co odpowiada długości fali od 390 nm do około 100 nm. Istnieje kilka kryteri&oacute;w podziału zakresu promieniowania ultrafioletowego na podzakresy. Pierwsza klasyfikacja to podział na ultrafiolet tzw. bliski (390&nbsp;nm &ndash; 190&nbsp;nm) i daleki<span style=""mso-spacerun: yes"">&nbsp; </span>(190&nbsp;nm &ndash; 100 nm). Ze wzgledu na oddziaływanie biologiczne stosuje się podział na UV-A (315&nbsp;nm &ndash; 400&nbsp;nm), UV-B (280&nbsp;nm &ndash; 315&nbsp;nm) i UV-C (200 nm &ndash; 280 nm). Wyodrębnia się też zakres ultrafioletu pr&oacute;żniowego VUV (40&nbsp;nm &ndash; 200 nm). Coraz częściej wykorzystuje się także zakres skrajnego ultrafiolet EUV (np. 13,5 nm) w fotolitografii scalonych układ&oacute;w p&oacute;łprzewodnikowych. W aplikacjach przemysłowych UV wykorzystuje się gł&oacute;wnie zakres pomiędzy 200&nbsp;nm&nbsp;&ndash;&nbsp;400&nbsp;nm. Promieniowanie ultrafioletowe stanowi 1% promieniowania elektromagnetycznego, kt&oacute;re dociera do Ziemi ze Słońca. Promieniowanie UV-C jest prawie całkowicie pochłaniane przez warstwę ozonową atmosfery. Promieniowanie UV-B stanowi 5%, zaś promieniowanie UV-A stanowi 95%<span style=""mso-spacerun: yes"">&nbsp; </span>promieniowania UV docierającego do powierzchni Ziemi. </font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font face=""Times New Roman"">&nbsp;&nbsp; Sztucznie światło ultrafioletowe<span style=""mso-spacerun: yes"">&nbsp; </span>jest generowane gł&oacute;wnie przy użyciu lamp wyładowawczych z parami rtęci i np. przez łuk, powstający w procesie spawania elektrycznego [1]. Dopiero w ostatnim okresie zaczęły się pojawiać oświetlacze p&oacute;łprzewodnikowe LED na światło UV, jednakże charakteryzują się one relatywnie małymi mocami.</font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font face=""Times New Roman""><span style=""mso-spacerun: yes"">&nbsp;&nbsp; </span>W 1912 roku A.&nbsp;Ciamcian opublikował w czasopiśmie Science artykuł, w kt&oacute;rym zwr&oacute;cił uwagę przemysłu amerykańskiego na duże możliwości zastosowania reakcji fotopolimeryzacji zachodzącej pod wpływem UV. Jednym z pierwszych, na dużą skalę, praktycznych zastosowań UV było opracowanie w latach sześćdziesiątych przez przemysł japoński technologii otrzymywania nylonu. Promieniowanie ultrafioletowe wywołuje fluorescencje, fotoluminescencje, jonizuje powietrze, wywołuje zjawisko fotoelektryczne, odznacza się silnym działaniem fotochemicznym i duża aktywnością biologiczną. Wszystkie te zjawiska są aktualnie wykorzystywane w zastosowaniach przemysłowych i badaniach naukowych. Pierwsze dane dotyczące działania promieniowania ultrafioletowego na żywe organizmy pochodzą z roku 1877, kiedy to uczeni zauważyli że wywołuje ono inaktywacje bakterii. Ultrafiolet UV-C jest jednym z najskuteczniejszych środk&oacute;w dezynfekcyjnych. Dzisiaj używa się go zar&oacute;wno do dezynfekcji sal operacyjnych jak r&oacute;wnież do dezynfekcji transport&oacute;w owoc&oacute;w. Promieniowanie ultrafioletowe działa mutagennie, powodując łączenie wiązaniami kowalencyjnymi dw&oacute;ch zasad pirymidynowych leżących w jednym łańcuchu DNA. Nasze kom&oacute;rki be...",07/01/25,23,Badania detektorów promieniowania optycznego zakresu UV przeznaczonych do pracy w sprzęcie wojskowym.,"UV, detektory UV, szumy detektorów UV, centra defektowe detektorów UV, LFNS",141
112,4423,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""text-align: justify; text-indent: 35.4pt; line-height: 150%;""><font size=""3""><strong><span style=""font-family: Arial;"">Cis pospolity (<em>Taxus baccata</em> L.)</span></strong><span style=""font-family: Arial;""> to drzewo bardzo wolno rosnące (zar&oacute;wno na wysokość, jak i na grubość), na terenie Polski w sprzyjających warunkach osiąga 8 do 15 m wysokości, długowieczne (cis rosnący w Henrykowie Lubańskim ma 1250 lat), w niekorzystnym siedlisku rośnie krzaczasto (Białobok i Hellwig 1955; Bugała i in. 1975, Seneta 1987; Thomas i Polwart 2003, </span><span style=""font-family: Arial;"">Szeszycki 2006</span><span style=""font-family: Arial;"">). Gatunek ten występuje w Europie, Afryce Pn., na Kaukazie i w Azji Zach. (istnieje około ośmiu gatunk&oacute;w cisa, wszystkie jednak są do siebie tak podobne, że uznawane są za odmiany cisa pospolitego). Cis jest gatunkiem dwupiennym (Allison 1991, DiFazio i inni 1996, Iszkuło i Jasińska 2004, Iszkuło i Boratyński 2005).<o:p></o:p><br />&nbsp;&nbsp;&nbsp; Dawniej cis był pospolitym i charakterystycznym składnikiem naszych las&oacute;w, występował w podszyciu las&oacute;w liściastych i mieszanych. Jednak z powodu na duże walory drewna (twarde, elastyczne i trwałe, w średniowieczu służyło do wyrobu kusz, łuk&oacute;w i ozdobnych mebli oraz jako surowiec wywożone było za granicę, igły i gałązki cis&oacute;w wykorzystywane były w medycynie<span style="""">&nbsp; </span>i obrzędach ludowych) został niemal doszczętnie wytępiony. Obecnie <em>Taxus baccata </em>jest objęty w Polsce od 1934 r. ochroną gatunkową, ale już w 1423 r. drzewo zostało uznane przez Władysława Jagiełłę za rzadkie i cenne (Czartoryski 1975, Bugała 1979; Białobok i Browicz 1969; Nawara i Sandecki 1999; Krussmann 1983; </span><span style=""font-family: Arial;"">Szeszycki 2006</span><span style=""font-family: Arial;"">).<o:p></o:p><br />&nbsp;&nbsp;&nbsp; Gatunek ten w naszym kraju jest od dawna przedmiotem licznych badań, prace dotyczą przede wszystkim występowania tego gatunku, jego historii, pochodzenia, morfologii, ekologii, ochrony i użytkowania (Gołąb i Kr&oacute;l 1990 &ndash; Bibliografia cisa pospolitego (<em>Taxus baccata </em>L.) w Polsce).<o:p></o:p><br /><span style=""font-weight: bold;"">&nbsp;&nbsp;&nbsp; </span>Cisy najlepiej rosną na glebach żyznych i wilgotnych, często kamienistych i zasobnych w węglan wapnia. Należą do drzew cieniolubnych, szczeg&oacute;lnie w pierwszych latach źle rosną w pełnym słońcu i na otwartej przestrzeni, gdzie często w&oacute;wczas przemarzają. Sprzyja im wilgotny klimat morski o łagodnych zimach (choć drzewa wytrzymują mrozy do -25<sup>0</sup>C), są odporne na zanieczyszczenia powietrza, dlatego często sadzone w parkach i ogrodach miejskich (Białobok i Hellwig 1955, Thomas i Polwart 2003, </span><span style=""font-family: Arial;"">Szeszycki 2006</span><span style=""font-family: Arial;"">). <o:p></o:p></span></font></p>
<p class=""MsoBodyTextIndent2"" style=""margin-left: 0cm; text-align: justify; text-indent: 35.4pt; line-height: 150%;""><font size=""3""><span style=""font-size: 12pt; font-family: Arial;"">W Polsce najwięcej naturalnych stanowisk znajduje się na Pojezierzu Pomorskim, Wyżynie Małopolskiej oraz w Sudetach i Karpatach, gdzie cisy występują w piętrze pog&oacute;rza i regla dolnego. Przez nasz kraj przebiega wschodnia granica zasięgu cisa w Europie (Atlas... 1969, Atlas... 1997, Kruszelnicki 2001).<o:p></o:p></span></font></p>
<p class=""MsoBodyTextIndent2"" style=""margin-left: 0cm; text-align: justify; text-indent: 0cm;""><span style=""font-size: 12pt; font-family: Arial;""><font size=""3"">Fot. 1. Żeński osobnik cisa pospolitego (<em>Taxus baccata</em> L.). Rezerwat &bdquo;Cisy Rokickie&rdquo;.</font><o:p></o:p></span></p>
<p class=""MsoBodyTextIndent2"" style=""margin-left: 0cm; text-align: justify; text-indent: 0cm;""><span style=""font-size: 12pt; font-family: Arial;"">&nbsp;<img width=""500"" height=""353"" alt="""" src=""/OSFImageLoader.do?idImageDB=14879"" /><o:p></o:p></sp...",07/07/19,27,Chronologie przyrostów radialnych cisa pospolitego w Polsce i ich dendroklimatologiczna analiza,"dendrochronologia, przyrosty roczne drzew, warunki klimatyczne, cis pospolity (Taxus baccata L.) ",183
113,4428,opisProjektuStanWiedzy,"<div style=""text-align: left;""><font size=""3"">&nbsp;&nbsp;&nbsp; Spoczynek jest mechanizmem adaptacyjnym, pozwalającym nasionom przeżyć niekorzystne warunki zewnętrzne i skiełkować w momencie najbardziej odpowiednim. Głęboki spoczynek nasion najczęściej jest utożsamiany ze spoczynkiem zarodkowym, powstającym podczas ostatnich etap&oacute;w embriogenezy. Najczęściej ustępuje on w wyniku działania takich warunk&oacute;w, jak długotrwała niska temperatura działająca na spęczniałe nasiona (stratyfikacja chłodna). Wywołany przez mechanizmy wewnętrzne głęboki, fizjologiczny spoczynek zarodkowy charakteryzujący nasiona jodły pospolitej, buku zwyczajnego i klonu zwyczajnego może ustąpić w wyniku chłodnej, kilkunastotygodniowej stratyfikacji w temperaturze 2-5&deg;C. </font><br /><font size=""3"">&nbsp;&nbsp;&nbsp; Białko jako produkt ekspresji genu może być zasadniczym czynnikiem wpływającym na ustępowanie spoczynku i kiełkowanie nasion (Misra i Bewley 1985). Większość autor&oacute;w omawia zmiany jakościowe i ilościowe białek nasion niespoczynkowych, nie wymagających w celu skiełkowania chłodnej stratyfikacji. Nasiona te, kiełkują zaraz po uwodnieniu. Podczas uwadniania nasion pomidora Bergervoet i in. (1994) obserwowali zmiany jakościowe białek. Ekspresja ekspansyny była związana z kiełkowaniem nasion pomidora (Chen i Bradford 2000). Białko SBP65 (Dehaye i in. 1997) wyizolowane z nasion grochu uznano za związane z utrzymaniem stanu spoczynku. Białko to zanikło pod wpływem uwodnienia. Obserwowano także, w miarę uwadniania nasion zanikanie białek należących do grupy LEA (Blackman i in. 1995). Badania nasion owsa pokazały, że ponad 20 białek jest związanych ze spoczynkiem i jego ustępowaniem (Li i Foley 1994). Białko kodowane przez gen AFD4 może być represorem kiełkowania, kt&oacute;ry utrzymuje nasiona w stanie spoczynku. Gen AFN5, kt&oacute;ry uaktywnia się bardzo wcześnie podczas uwadniania, może kodować białko potrzebne do przekazania sygnału lub zapoczątkowania wczesnych etap&oacute;w kiełkowania (Johnston i in. 1996). Do znanych produkt&oacute;w ekspresji gen&oacute;w związanych z ustępowaniem spoczynku należą r&oacute;wnież enzymy hydrolityczne (amylaza, proteazy, nukleazy) i enzymy katabolizmu produkt&oacute;w hydrolizy substancji zapasowych. W represji tych gen&oacute;w bierze udział ABA, natomiast ich aktywacja odbywa się z udziałem hormon&oacute;w &ndash; stymulator&oacute;w kiełkowania (gł&oacute;wnie GA<sub>3</sub>). Jednym z mechanizm&oacute;w powodujących spoczynek jest r&oacute;wnież zahamowanie aktywności niekt&oacute;rych białek enzymatycznych, katalizujących kluczowe reakcje ważnych szlak&oacute;w metabolicznych. Aktywacja tych enzym&oacute;w podczas ustępowania spoczynku zachodzi bądź na skutek zaniku inhibitora (najczęściej ABA), bądź też w wyniku zwiększenia stężenia stymulatora (np. GA<sub>3</sub>, Derkx i Karssen 1993). De Castro i in. (1995) na początku kiełkowania nasion pomidora obserwowali znaczny wzrost zawartości &beta;-tubulin. Pojawienie się mikrotubul jest wstępnym warunkiem wzrostu kom&oacute;rek. De Castro (1995) sugeruje, że wzrost kom&oacute;rek zbiega się z wysoką zawartością &beta;-tubulin. </font><br /><font size=""3"">&nbsp;&nbsp;&nbsp; Badania ostatnich lat dotyczyły r&oacute;wnież identyfikacji białek syntetyzowanych podczas chłodnej stratyfikacji. Już podczas imbibicji i na początku stratyfikacji nasion sosny Schneider i Gifford (1994) obserwowali pojawienie się dużej liczby nowych białek, kt&oacute;rych obecność zbiegła się ze spadkiem poziomu ABA. W nasionach daglezji obserwowano wzrost poziomu polipeptyd&oacute;w, scharakteryzowanych jako białka grupy LEA, wywołany chłodną stratyfikacją (Jarvis i in. 1997). Pojawiły się one najpierw w zarodku a następnie w megagametoficie. Białka związane z ustępowaniem spoczynku pod wpływem chłodnej stratyfikacji znalazł r&oacute;wnież w osiach zarodkowych i liścieniach Ambrosia trifida Ballard i in. (1996). W nasionach cedru Ren i Kermode (2000) zaobserwowal...",07/01/31,23,Proteomika ustępowania spoczynku i kiełkowania nasion wybranych gatunków drzew,"Abies alba, Acer platanoides, białka, drzewa, Fagus sylvatica,  kiełkowanie, nasiona, proteomika, ustępowanie spoczynku ",183
114,4433,opisProjektuStanWiedzy,"Poszukiwanie publikacji w temacie wniosku w Polsce dało w wyniku jedynie dwie pozycje:<br />
<ol>
    <li>Piotr Wołowik, Politechnika Poznańska “Zastosowanie sygnału EEG w interfejsach łączących człowieka z komputerem”, Poznańskie warsztaty Telekomunikacyjne, Poznań 9-10 grudnia 2004</li>
    <li>M. Byczuk, A. Materka “Komunikacja człowieka z komputerem za pomocą sygnału EEG”, Elektronika Prace Naukowe, Wydział Elektroniki Politechniki Łódzkiej, Łódź 2003, Zeszyt Nr. 8, pp. 35-52</li>
</ol>
Pierwsza z nich to krótki przegląd literatury światowej, druga zawiera opis konstrukcji od podstaw dwukanałowego wzmacniacza do pomiaru EEG, i wspomnienie możliwości jego zastosowania w konstrukcji BCI. Autorzy pomijają problem polegający na tym, że większość BCI opartych na EEG wymaga jednoczesnych pomiarów ze znacznie większej liczby kanałów. Wedle najlepszej wiedzy autorów wniosku, w skali kraju problem można zapewne uznać za nowy.<br /><br />Przykładami z obszernej literatury światowej są pozycje [2-11] z załączonej listy, więc w skali światowej problem jest niewątpliwie kontynuowany, od początku lat 90. XX wieku [22-23]. Jednak zastosowanie metod opisanych w punkcie D.4 wniesie również w skali światowej istotny wkład, potwierdzony publikacjami w prestiżowych czasopismach.",07/01/25,23,Interfejs mózg-komputer,"Brain-computer interface, BCI, EEG, event-related desynchronization, ERD, event-related synchronization, ERS, matching pursuit, MP, czas-częstość, analiza sygnałów",173
115,4435,opisProjektuStanWiedzy,"<div style=""text-align: justify;""><span style=""font-family: Arial;"">N. P. Erugin w monografii [8] rozpatrywał zagadnienia o istnieniu globalnych rozwiązań autonomicznego i nieautonomicznego r&oacute;wnania Riccatiego. W monografii D. H. Jacobsona [13] udowodniono dostateczne warunki istnienia rozwiązań wybuchowych autonomicznych kwadratowych układ&oacute;w r&oacute;wnań r&oacute;żniczkowych oraz wyprowadzono wzory na oszacowanie czasu wybuchu. Zjawiska wybuchowe w innych klasach r&oacute;wnań r&oacute;żniczkowych i pewnych autonomicznych układach r&oacute;wnań r&oacute;żniczkowych były przedmiotem badań wielu autor&oacute;w [5-7,9-12,14-16]. Jednak zjawiska wybuchowe w układach r&oacute;wnań r&oacute;żniczkowych bez założenia, że są one autonomiczne, dotychczas zbadano niewystarczająco. W kraju i na świecie był to problem nowy do 2006 roku. W 2006 roku wyniki prac [8,13] zostały uog&oacute;lnione w naszych pracach [1-3] i na tej podstawie rozwiązane powyższe problemy dla nieautonomicznych układ&oacute;w kwadratowych r&oacute;wnań r&oacute;żniczkowych, a także analiza tych problem&oacute;w dla przypadk&oacute;w, gdy układ kwadratowy możemy zapisać jako nieautonomiczne macierzowe r&oacute;wnanie Riccatiego oraz jako nieautonomiczny układ Lotki-Volterry. W [4] zapoczątkowane zostało r&oacute;wnież badanie wybuchowych rozwiązań dla nieautonomicznych wielomianowych układ&oacute;w Kolmogorowa.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> </div>",07/01/30,23,Rozwiązania wybuchowe układów wielomianowych równań różniczkowych,"układ wielomianowy równań różniczkowych, układ Kolmogorowa, układ semiliniowych równań przewodnictwa cieplnego, regularne stochastyczne (losowe) równanie różniczkowe, wybuch, rozwiązanie wybuchowe, czas wybuchu, wartość początkowa, maksymalny przedział istnienia",173
116,4443,opisProjektuStanWiedzy,"&nbsp;
<div align=""justify""><strong>&nbsp;&nbsp; Oryginalnym wkładem, jaki wniesie rozwiązanie postawionego w projekcie problemu do dorobku elektroniki będzie nowa w skali światowej metoda, zaimplementowana sprzętowo, prowadząca do zwiększenia wydajności częstotliwościowej systemu scalonego, dzięki asynchronicznemu sterowaniu aktywnością&nbsp;blok&oacute;w funkcjonalnych.</strong></div>
<div align=""justify"">&nbsp;&nbsp; W kraju i na świecie jest to problem nowy, mieszczący się w dynamicznie rozwijającym się nurcie projektowania złożonych system&oacute;w cyfrowych<em>.</em></div>
<div align=""justify"">&nbsp;&nbsp; Wiele ośrodk&oacute;w naukowych pracuje intensywnie nad wzrostem wydajności i ograniczaniem energii zasilania system&oacute;w scalonych. Do klasyki należy już podejście polegające na dynamicznym r&oacute;żnicowaniu napięć zasilania poszczeg&oacute;lnych części złożonego systemu scalonego [Shi06]. Poszukuje się kompleksowych strategii zarządzania energią zasilania złożonych system&oacute;w VLSI zar&oacute;wno metodami deterministycznymi jaki probabilistycznymi [Ira05]. Inne podejście, bardziej szczeg&oacute;łowe, polega na nałożeniu ostrych warunk&oacute;w na szczytową wartość mocy strat energii zasilania [Sha06]. W wielu zespołach na świecie, także w zespole wykonawc&oacute;w niniejszego wniosku, prowadzone są prace nad syntezą logiczną układ&oacute;w cyfrowych ukierunkowaną na minimalizację zmian stan&oacute;w logicznych podczas realizacji zadanej funkcji logicznej. W konsekwencji minimalizowana jest energia strat dynamicznych [Łub03], [Brz05]. Osobnymi&nbsp;ścieżkami poszukiwań są specyficzne konstrukcje tranzystor&oacute;w, pozwalające na pracę ultra-energooszczędną w obszarze podprogowym &ndash; niestety z małymi częstotliwościami [Ray05]; budowa energooszczędnych magistral sygnałowych [Gho06] oraz architektoniczna synteza układ&oacute;w ze względu na&nbsp;straty w magistralach połączeń (straty Joule&rsquo;a) [Pas06] &ndash; tu oryginalny wkład zespołu wykonawc&oacute;w [Kos05]. Jeszcze inne podejście do maksymalnego wykorzystania własności termodynamicznych system&oacute;w scalonych bez obawy o przekroczenia dopuszczalnych gęstości mocy, to dynamiczna regulacja częstotliwości pracy systemu z predykcją [Goł06] - autorstwa zespołu wykonawc&oacute;w.</div>
<div align=""justify"">&nbsp;&nbsp; Metody redukcji start energii <strong>powinny uwzględniać rzeczywiste procesy fizyczne</strong> występujące w systemach scalonych. Zmiany jakościowe i ilościowe dokonujące się we wsp&oacute;łczesnej elektronice system&oacute;w scalonych wymagają nowego podejścia ze względu na złożoność problem&oacute;w (systemy wbudowane, SoC) oraz nowe nanometrowe technologie, kt&oacute;rych fizyka zjawisk zmusza do pilnego opracowania nowych modeli i metod analizy oraz syntezy. Wynika to przede wszystkim ze zwiększenia udziału energii statycznej w całkowitym poborze energii [And06]. Jednocześnie prądy upływności, kt&oacute;re są źr&oacute;dłem właśnie tych strat, silnie zależą od temperatury. Wzrost gęstości mocy i co za tym idzie wzrost temperatury wywołuje eksponencjalny wzrost wartości tych prąd&oacute;w [Ros02]. Temperatura pracy poszczeg&oacute;lnych blok&oacute;w funkcjonalnych (sektor&oacute;w) moduł&oacute;w scalonych wpływa znacząco na parametry elektryczne [Lia05], [Goł04]. Projektowanie system&oacute;w VLSI i ULSI we wsp&oacute;łczesnych, a tym bardziej przyszłych technologiach musi być od samego początku poparte prawidłowymi symulacjami elektrotermicznymi aby uniknąć niepożądanych skutk&oacute;w lub, co gorsza, uszkodzeń [Jan00]. </div>
<div align=""justify"">&nbsp;&nbsp; Do chwili obecnej zesp&oacute;ł autor&oacute;w niniejszego projektu opracował oryginalne metody syntezy, kt&oacute;re umożliwiają zwiększenie wydajności systemu cyfrowego dzięki uwzględnieniu jego temperatury&nbsp; pracy i zastosowaniu aktywnego chłodzenia [Brz05], [Dzi01]. Daje to możliwość pracy z małym marginesem termodynamicznym. Doświadczenia wynikające z tyc...",07/01/30,23,ASTER: Asynchroniczny sterownik aktywności bloków funkcjonalnych SoC,"mikroelektronika, modelowanie i optymalizacja elektrotermiczna, systemy VLSI, CMOS, SoC, systemy wbudowane, modele elektrotermiczne, synteza logiczna, ASTER ",183
117,4444,opisProjektuStanWiedzy,"<p align=""left""><font size=""4"">&nbsp;&nbsp;&nbsp; <br />Efekt tranzycji C/T w pozycji 1843 genu receptora ryanodiny został dobrze poznany i opisany w szeregu prac zar&oacute;wno krajowych jak i zagranicznych.&nbsp;Największa częstość tej mutacji występuje w stadach świń rasy Pietrain i te zwierzęta charakteryzują się wysoka wrazliwością na stres. W przypadku innych ras mutacja C1843T występuje stosunkowo rzadko, jednak nie należy jej traktować marginalnie. W badaniach przeprowadzonych przez Buczyńskiego i wsp. (2006a)&nbsp;na świniach rasy złotnickiej białej wykazano, że wśr&oacute;d 91 osobnik&oacute;w, zidentyfikowano następujące genotypy RYR1: homozygoty dominujące (75 sztuk tj. 82,41%), heterozygoty (13 sztuk, tj. 14,28%) i homozygoty recesywne (3 sztuki tj. 3,29%). W badaniach nad rasą złotnicką pstrą (Buczyński i wsp. 2006b) nie zaobserwowano osobnik&oacute;w o genotypie RYR<sup>TT</sup>. &nbsp;W produkcji żywca wieprzowego ważne są nie tylko efekty uzyskiwane u producenta, ale istotnym problemem jest także przydatność technologiczna mięsa wieprzowego. Występowanie mięsa wadliwego (PSE) wywołane może być tak czynnikami genetycznymi jak i środowiskowymi. Spos&oacute;b skutecznego przeciwdziałania temu zjawisku zależy od prawidłowego rozpoznania przyczyn. Istotnym problemem jest tu także zawartość tłuszczu śr&oacute;dmięśniowego w tuszach.&nbsp;Badania ostatnich lat wykazały, że cechy te można badać z dużą dokładnością metodą ultrasonograficzną. </font></p>
<p align=""left""><font size=""4"">&nbsp;&nbsp;&nbsp; Hormon wzrostu (GH) pełni kluczowa rolę w regulacji tempa wzrostu i laktacji zwierząt gospodarskich. Liczne badania&nbsp;wykazały, że podawanie zwierzętom egzogennego hormonu wzrostu wpływało na takie cechy jak: tempo wzrostu, wykorzystanie paszy, masę tkanki mięśniowej w tuszy, otłuszczenie tuszy, powierzchnię &bdquo;oka&rdquo; polędwicy, produkcję mleka. W przypadku świń odnotowuje się zwiększenie: o 10-20% tempa przyrostu masy ciała, o 2-30% odkładanie białka, o 30-40% zmniejszenie przyrostu tkanki tłuszczowej i o 15-35% zwiększenie efektywności tuczu. Stąd wynika zainteresowanie badaniem wpływu polimorfizmu genu GH na jakość tuszy. Badania ostatnich lat wykazały, że genotyp GH wpływa na wartość większości cech charakteryzujących otłuszczenie tuszy, wielkość powierzchni &bdquo;oka&rdquo; polędwicy oraz procentową zawartość mięsa w tuszy.&nbsp;Wykazano r&oacute;wnież, że grubość słoniny na grzbiecie mierzona PIGLOG przy masie ciała około 80 kg oraz średnia grubość słoniny i zawartość mięsa w tuszy oceniane po uboju, były istotnie związane z genotypem genu GH identyfikowanym przy użyciu enzymu restrykcyjnego MspI. Polimorfizm genu GH u świni identyfikowano w drugim eksonie i drugim intronie enzymami restrykcyjnymi HaeII (AA, AB, BB)&nbsp;i MspI (CC,CD,DD). Stwierdzono, ze tuczniki o haplotypie &bdquo;AA/DD&bdquo; w układzie HaeII/MspI charakteryzowały się najlepszą jakością tuszy. U zwierząt o tym genotypie odnotowano największą powierzchnie &bdquo;oka&rdquo; polędwicy oraz najcieńszą słoninę we wszystkich punktach jej pomiaru. Z kolei haplotyp BB/CC w układzie HaeII/MspI okazał się najbardziej niekorzystny pod względem cech zar&oacute;wno otłuszczenia, jak i mięsności. Knorr i wsp. 1997 wykazali związek polimorfizmu genu GH z następującymi cechami otłuszczenia tuszy świń: średnią grubością słoniny na grzbiecie, powierzchnią przekroju słoniny nad LD przy 13/14 żebrze, stosunekiem masy mięsa do masy słoniny w tuszy. Zależność ta odnotowana została jedynie w rodzinie meishanxpietrain, podczas gdy w rodzinie dzikxpietrain okazały sie one nieistotne. Kontynuowanie badań nad tym zagadnieniem wydaje się być jak najbardziej wskazane. </font><font size=""4"">Wyniki badań przeprowadzone w ostatnich latach wykazały, że opr&oacute;cz wspomnianych wyżej funkcji, GH oddziałuje r&oacute;wnież na męskie i żeńskie procesy rozrodcze oraz prawidłowy rozw&oacute;j zarodka ssak&oacute;w. Liczne funkcje GH w regulacji funkcji jajnik&oacute;w...",07/01/17,23,"Badanie wpływu polimorfizmów genów RYR1, GH, LEP, MYF5 oraz SCD-1 na kształtowanie się cech produkcyjnych w populacji świń złotnickich.","trzoda chlewna, złotnicka biała, złotnicka pstra, polimorfizm genów RYR1, GH, LEP, MYF5, SCD-1",173
118,4445,opisProjektuStanWiedzy,"W Polsce jest używany (i to w ograniczonym zakresie) automatyczny system identyfikacji łusek i pocisk&oacute;w Arsenal firmy Papilon. Jest on jedynie graficzną stacją, kt&oacute;ra zawiera grupy cech (charakterystycznych rys) do analizy, por&oacute;wnuje grupy ślad&oacute;w (można wprowadzić, np. obrazy cech charakterystycznych dla narzędzi, tj. &bdquo;łamacz&rdquo; wkładek oraz powierzchnie wkładek zamk&oacute;w, bez możliwości odr&oacute;żnienia ich jako cech element&oacute;w broni, czy cech narzędzi włamywacza). Obszary zaznaczane w systemie Arsenal&nbsp;mają jedynie nadane nazwy np.: cz&oacute;łko zamka, pazur wyciągu, itp. <br />Na podstawie dotychczasowych badań własnych oraz wykorzystując badania przeprowadzone w ramach niniejszego projektu badawczego promotorskiego&nbsp;(a ujęte w pracy doktorskiej) uważamy, że możliwe jest&nbsp;opracowanie polskiej, oryginalnej metodyki identyfikacji broni i amunicji, a na jej podstawie zbudowanie użytkowej bazy danych broni i amunicji, kt&oacute;ra po wdrożeniu może w wydatny spos&oacute;b wspom&oacute;c pracę Policji i Żandarmerii Wojskowej. Zaproponowana w wyniku realizacji projektu koncepcja algorytmu Wirtualnej Bazy Broni i Amunicji Strzeleckiej winna umożliwiać na podstawie danych wejściowych (ślad&oacute;w zabezpieczonych na miejscu zdarzenia zidentyfikowanych na pocisku łusce i ciele ofiary lub innej przegrodzie) określenie:<br />- rodzaju i typu broni użytej na miejscu zdarzenia, <br />- rodzaju i typu naboju, kt&oacute;ry spowodował uszkodzenia, <br />- parametr&oacute;w balistyki końcowej pocisku,&nbsp;<br />- identyfikacji miejsca wystrzelenia oraz położenie lufy w czasie strzału. <br />",07/01/29,23,Badanie i modelowa identyfikacja mechanicznych skutków strzału z broni palnej,"broń palna, amunicja strzelecka, balistyka zewnętrzna, balistyka końcowa, identyfikacja, kryminalistyczne ślady użycia broni palnej, wirtualna baza broni i amunicji strzeleckiej",183
119,4449,opisProjektuStanWiedzy,"<p>W chwili obecnej dane dotyczące wielkości emisji antropogenicznej niezbędne do prognozowania zmian klimatu za pomocą regionalnych i globalnych modeli klimatycznych dostarczane są ze źr&oacute;deł statystycznych. Fizyczne metody pomiaru wielkości strumieni wymiany rożnych gaz&oacute;w (gł&oacute;wnie CO<sub>2</sub>) pomiędzy biosferą i atmosferą są powszechnie stosowane w r&oacute;żnych ekosystemach takich&nbsp;jak lasy, torfowiska, łąki czy pola uprawne. Wykorzystywane są gł&oacute;wnie dwie metody pomiar&oacute;w strumieni gaz&oacute;w do atmosfery (Gorczyca i in. 2003)&nbsp;oraz metoda znacznikowa służąca do tworzenia bilansu gaz&oacute;w w atmosferze wykorzystująca zar&oacute;wno znaczniki chemiczne jak np. SF<sub>6</sub>, jak i izotopowe (<sup>14</sup>C, atmosferyczny radon, stabilne izotopy węgla, tlenu, azotu i siarki). Pierwsza rodzina metod tzw. komorowych, pozwala na bezpośredni pomiar wielkości strumieni dla bardzo małych powierzchni (rzędu 1m<sup>2</sup> i mniej) i jej podstawą jest analiza szybkości narostu stężenia badanych gaz&oacute;w wewnątrz komory przykrywającej fragment gruntu (metoda kom&oacute;r statycznych), bądź pomiaru gradientu stężenia w przepływającym gazie (metoda kom&oacute;r dynamicznych). Metody te doskonale nadają się do wyznaczania wielkości strumieni w skali lokalnej, jednak interpretacja tego typu pomiar&oacute;w dla większych obszar&oacute;w nastręcza szereg trudności. Drugą klasą metod są metody gradientowe, bądź kowariancyjne dostarczające danych reprezentujących większe obszary (do kilku km<sup>2</sup>). Pierwsza z nich polega na pomiarze pionowych gradient&oacute;w stężenia i wyznaczaniu na tej podstawie&nbsp;wsp&oacute;łczynnik&oacute;w pionowej dyfuzji turbulentnej (Dupont i in. 1999). Druga metoda kowariancji wir&oacute;w polega na r&oacute;wnoczesnym pomiarze składowej pionowej wiatru za pomocą anemometr&oacute;w akustycznych oraz zmienności stężenia wybranego gazu (Grimmond i in. 2002). Ostatnia z opisywanych metod jest najbardziej obiecująca i w ostatnich latach przeżywa dynamiczny rozw&oacute;j. W przypadku dużych aglomeracji miejskich żadna z opisanych metod nie znalazła powszechnego zastosowania mimo, że na ich terenie zlokalizowana jest większość źr&oacute;deł antropogenicznych gaz&oacute;w cieplarnianych. Pierwsza grupa metod (komorowych) nie nadaje się do zastosowania ze względu na charakter rozmieszczenia źr&oacute;deł na terenach miejskich (kominy, arterie komunikacyjne).&nbsp;Dla metody kowariancji wir&oacute;w są publikacje prezentujące wyniki pomiar&oacute;w na terenie dużych miast, jednakże&nbsp;ze względu na duże zr&oacute;żnicowanie&nbsp;terenu, zastosowanie tej metody napotyka&nbsp;na trudności. Z kolei metody znacznikowe wykorzystywane są dość powszechnie na terenach zurbanizowanych (Kuc i in.&nbsp;1998, Florkowski i in.&nbsp;2002, Kuc i in.&nbsp;2003, Zimnoch i in.&nbsp;2004,&nbsp;Chmura i in.&nbsp;2005) , jednak nie dają one bezpośredniej informacji o wielkości i zmienności strumieni a jedynie pozwalają konstruować bilans gaz&oacute;w r&oacute;żnego pochodzenia stanowiąc metodę pozwalającą np. na weryfikację pomiar&oacute;w strumieni. W ramach projektu zastosowane będzie nowatorskie podejście polegające na wykorzystaniu pomiar&oacute;w stężenia wybranych gaz&oacute;w w warstwie przypowierzchniowej przy użyciu techniki chromatografii gazowej oraz spektrometrii alfa promieniowania jonizującego pochodzącego od produkt&oacute;w rozpadu atmosferycznego radonu oraz pomiaru głębokości warstwy mieszania za pomocą lidarowych sondowań akustycznych atmosfery (Baklanow 2001, COST-715 2004). Powiązanie tych dw&oacute;ch technik pomiarowych pozwoli na wyznaczenie wielkości i zmienności czasowej strumieni wybranych gaz&oacute;w śladowych w atmosferze Krakowa.</p>",07/01/29,23,Ocena wielkości strumieni gazów śladowych na terenie Krakowa z wykorzystaniem pomiarów chromatograficznych oraz sodarowych,"emisje antropogeniczne, gazy śladowe, efekt cieplarniany, zmiany klimatu, miejska warstwa graniczna, ",183
120,4451,opisProjektuStanWiedzy,"<p align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Technologia RFID (Radio Frequency IDentification) jest to metoda służąca do bezprzewodowej identyfikacji obiekt&oacute;w oznakowanych transponderami (znacznikami). System RFID składa się z co najmniej jednego czytnika (reader, interrogator) komunikującego się poprzez łącze radiowe z co najmniej jednym transponderem (transponder, tag) w celu odczytania (zapisania) niepowtarzalnego numeru identyfikacyjnego i innych danych znajdujących się w nieulotnej pamięci znacznika [1,18]. Daje to możliwość zastosowania i wdrożenia do powszechnego użytku technologii radiowej w systemach automatycznej identyfikacji. Bieżąca pozycja technologii RFID na rynku, przyrost jej zastosowań i nowe opracowania badawcze wskazują na dominującą rolę system&oacute;w identyfikacji RFID wśr&oacute;d innych technik identyfikacji, z perspektywą rozszerzenia zakresu zastosowań (np. systemy sensorowe) [8, 18]. </p>
<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zalety technologii RFID skłaniają do prowadzenia prac badawczych nad rozszerzeniem zastosowań system&oacute;w RFID na obszary charakteryzujące się złymi warunkami propagacji fal radiowych i zagrożeniami środowiskowymi (g&oacute;rnictwo podziemne) [17]. W kraju realizowano wieloośrodkowy (Akademia G&oacute;rniczo-Hutnicza, Politechnika Śląska, Centrum Mechanizacji G&oacute;rnictwa KOMAG, ELSTA Sp. z o.o.) projekt dotyczący identyfikacji element&oacute;w maszyn g&oacute;rniczych z zastosowaniem technologii RFID [14] i jest to pierwsze wdrożenie technologii RFID do pracy w g&oacute;rnictwie [7]. W Katedrze Elektroniki AGH prowadzi się prace badawcze i projektowe nad przenośnym czytnikiem RFID oraz terminalem operatora [7]. Uzyskano pozytywne rezultaty wykonując prototyp czytnika z kubkową anteną ferrytową [7]. Stanowi to punkt wyjścia do planowanej w prezentowanym projekcie optymalizacji układ&oacute;w antenowych czytnika RFID. Symulacje będą prowadzone przy użyciu metody element&oacute;w skończonych za pomocą uznanych program&oacute;w obliczeniowych (ANSYS, Maxwell 2D/3D).</div>
<div align=""justify"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Istotną częścią projektu są badania związane z wytwarzaniem p&oacute;l RF dla indukcyjnego przekazu energii. Ze względu na procedury dopuszczania urządzeń do eksploatacji przy zagrożeniach środowiskowych [5], w technologii RFID konieczne jest określenie oddziaływania pola magnetycznego RF z materiałami konstrukcyjnymi, w szczeg&oacute;lności podzespołami transponder&oacute;w, a wiąże się to przede wszystkim z określeniem efekt&oacute;w termicznych [5]. Oryginalnym wkładem byłoby skonstruowanie stanowiska laboratoryjnego i opracowanie metodyki badawczej w tym zakresie. Ponieważ podobna problematyka jest spotykana w hipertermii magnetycznej [10,11,15,16] badania rozszerzone zostaną na materiały i metody stosowane w tej dziedzinie. Zasadniczym ich elementem realizowanym w projekcie byłoby modelowanie pola magnetycznego w obszarze oddziaływania z implantem (ferrofluidem), i w rezultacie skonstruowanie układu cewek wytwarzających pole [21] w stanowisku laboratoryjnym. Pole magnetyczne RF wykorzystywane jest szeroko w typowych procesach indukcyjnej obr&oacute;bki cieplnej [20]. W proponowanym projekcie zbudowane stanowisko laboratoryjne powala na obr&oacute;bkę cieplną pr&oacute;bek o niewielkiej masie (ok. 5g). Można więc je zastosować do przeprowadzania proces&oacute;w szybkiej obr&oacute;bki cieplnej (RTP &ndash; Rapid Temperature Processing) materiał&oacute;w cienkowarstwowych [13]. Nowością byłoby wyposażenie stanowiska laboratoryjnego w sterownik umożliwiający realizację algorytmu sterowania bazującego na matematycznym modelu procesu indukcyjnego przekazu energii. </div>
<div align=""justify""></div>",07/01/30,23,Wytwarzanie i sterowanie polami magnetycznymi dla radiowej identyfikacji obiektów RFID oraz indukcyjnego przekazu energii,"modelowanie pól magnetycznych, RFID – zdalna identyfikacja obiektów, grzanie indukcyjne, generator mocy w.cz., rezonansowe układy przetwarzania energii, hipertermia magnetyczna",173
121,4453,opisProjektuStanWiedzy,"<p>&nbsp; </p>
<div>Inwerter złożony z połączonych kaskodowo tranzystora MOS z wzbogacanym kanałem p i komplementarnego tranzystora z kanałem n jest układem kt&oacute;remu cyfrowa technologia CMOS zawdzięcza swoją olbrzymią popularność &ndash; gł&oacute;wnie dzięki względnie dużemu marginesowi zakł&oacute;ceń i teoretycznie zerowemu statycznemu poborowi mocy. Oczywiście jak każdy układ cyfrowy jest on w istocie przesterowanym wzmacniaczem odwracającym i jako jeden z wariant&oacute;w wzmacniacza zbudowanego na tranzystorach MOS jest też prezentowany w klasycznych podręcznikach układ&oacute;w elektronicznych. W dotychczasowych fazach rozwoju technologii CMOS, gdy ograniczenia napięciowe, o kt&oacute;rych wspomniano powyżej, nie były jeszcze tak ostre, układy analogowego przetwarzania sygnału opierano gł&oacute;wnie na blokach zawierających r&oacute;żne modyfikacje pary r&oacute;żnicowej ze sprzężeniem źr&oacute;dłowym. Uniwersalność tego bloku wynika gł&oacute;wnie z symetrycznej struktury, możliwości symetrycznego lub niesymetrycznego sterowania, symetrycznego lub niesymetrycznego wyjścia, dużego tłumienia sygnał&oacute;w wsp&oacute;lnych (CMRR), dużego tłumienia zmian napięć&nbsp;zasilających (PSRR) oraz kompensacji parzystych harmonicznych na wyjściu r&oacute;żnicowym. Jednak układy wykorzystujące parę r&oacute;żnicową powinny pracować przy napięciu zasilającym około czterech napięć progowych gdyż w przeciwnym wypadku&nbsp;następuje degradacja quasi-liniowego obszaru pracy pary r&oacute;żnicowej, wyrażająca się dużą nieliniowością charakterystyk przejściowych. Dzieje się tak dlatego, że obw&oacute;d zasilania obejmuje zawsze stos trzech lub więcej tranzystor&oacute;w, a w ścieżce sygnałowej występują zawsze co najmniej trzy napięcia progowe. W inwerterze występują jedynie dwa tranzystory a zatem wymogi napięciowe tego bloku są najniższe z możliwych dla danej technologii</div>
<div>Jednym z pierwszych autor&oacute;w, kt&oacute;ry zwr&oacute;cił uwagę na praktyczne zastosowanie inwertera w funkcji analogowego przetwarzania sygnału jest B. Nauta [9]. Zaproponował on<span>&nbsp;&nbsp; transkonduktor złożony z symetrycznie sterowanych dwu inwerter&oacute;w. Co ciekawe, Nauta w swoich poszukiwaniach kierował się nie tyle ograniczeniami napięcia zasilania, kt&oacute;re w czasach jego publikacji były znacznie łagodniejsze, lecz wymogami pracy przy wysokich częstotliwościach. Inwerter ma bowiem tę szczeg&oacute;lną cechę, że nie posiada węzł&oacute;w wewnętrznych, a zatem nie wprowadza pasożytniczych zer i biegun&oacute;w do transmitancji integratora. Rok p&oacute;źniej ukazał się artykuł Singha, Hansona, i Vlacha [10] w kt&oacute;rym autorzy r&oacute;wnież rozpatrują inwerter w funkcji transkonduktora notabene nie powołując się na publikację Nauty, ale wskazując za to na wcześniejsze prace Fernandeza i Schaumanna z połowy lat osiemdziesiątych.&nbsp;Do zasadniczej&nbsp;idei Nauty nawiązuje praca Rauta [11] oraz wspomniany już w innym kontekście artykuł Mu&ntilde;oza i innych [4]. Wszyscy autorzy podkreślają szerokie pasmo częstotliwości pracy układ&oacute;w opartych na inwerterze. W tym samym roku co [4] Andreani i Mattisson [12] publikują artykuł rozważający zastosowania transkonduktora Nauty dla syntezy niskoczęstotliwościowych filtr&oacute;w pasmowych.&nbsp;Analogowe filtry w czasie ciągłym jako jeden z najciekawszych obszar&oacute;w elektroniki układowej występują zresztą najczęściej w omawianym kontekście także w innych publikacjach [13-14], choć inwerter bywa rozważany jako komparator dla przetwornik&oacute;w analogowo-cyfrowych [15,16] a także w roli komplementarnej do zaproponowanej przez Nautę [9] &ndash; wzmacniacza transrezystancyjnego [25]. Wszystkie te przesłanki dowodzą dużej uniwersalności bloku inwertera a obserwowana ostatnio intensyfikacja publikacji powstałych w r&oacute;żnych ośrodkach&nbsp;na temat inwertera w funkcji analogowego przetwarzania sygnał&oacute;w utwierdza w przekonaniu o istotności przedstawionego ...",07/07/30,27,"Niskonapięciowe analogowe bloki funkcjonalne realizowane w oparciu o inverter CMOS w systemach VLSI. Metody projektowania, ograniczenia, struktury i architektury układowe oraz aplikacje","Technologia CMOS, niskonapięciowe układy analogowe, mikro- i nanoelektronika",183
122,4459,opisProjektuStanWiedzy,"<p align=""left"">European Academy of Allegry and Clinical Immunology określiła nowe zasady klasyfikacji i nazewnictwa stosowane w alergicznych jednostkach chorobowych [Sampson 2004]. Niepożądane reakcje wywołane spożyciem określonych składnik&oacute;w żywności czyli nadwrażliwość żywieniowa (pokarmowa) obejmuje jakiekolwiek nienaturalne reakcje będące konsekwencją spożycia żywności i może być traktowana jako efekt nietolerancji żywieniowych lub nadwrażliwości żywieniowej/alergii. Nietolerancje żywieniowe obejmują niepożądane reakcje jako efekt unikatowej charakterystyki fizjologicznej gospodarza i obejmują np. nietolerancję laktozy. Alergia pokarmowa jest definiowana jako niepożądana reakcja immunologiczna &ndash; nadwrażliwość po spożyciu żywności i jako taka nigdy nie obejmuje pojedynczych fizjologicznych dysfunkcji organizmu [Bruijzeel-Koomen et al., 1995]. Alergie i nietolerancje pokarmowe są coraz częściej występującymi nieprawidłowymi reakcjami na pokarm. Duże tempo życia i towarzyszący mu stres, pogłębiające się zanieczyszczenie środowiska, modyfikacje r&oacute;żnych składnik&oacute;w żywności oraz wzrost udziału przetworzonej żywności w diecie mogą być przyczyną zwiększającego się występowania alergii pokarmowej u ludzi [Czarnecki, Targoński 2002]. Alergia jest powszechnym problemem zdrowotnym dotykającym coraz większą liczbę ludzi na całym świecie. Stale poszerza się wachlarz substancji, czyli alergen&oacute;w, powodujących r&oacute;żnego rodzaju uczulenia. Organizm człowieka traktuje je jako ciało obce, przed kt&oacute;rym broni się wytwarzaniem przeciwciał rozpoznawanych jako reakcje alergiczne. Reakcje alergiczne na żywność są zr&oacute;żnicowane i zależą nie tylko od rodzaju alergenu, ale r&oacute;wnież od osobniczej reakcji organizmu na antygen. Alergie wywołane przez składniki żywności można podzielić na te związane z wydzielaniem przeciwciał IgE i te nie związane z tym procesem. Choroby związane z nagłym pojawieniem się symptom&oacute;w i ostrym ich przebiegiem po spożyciu pokarm&oacute;w zwykle związane są z wydzielaniem przeciwciał IgE co prowadzi do stanu uczulenia. Inną grupę chor&oacute;b związanych z nadwrażliwością na składniki żywności stanowią podostre i chroniczne jednostki chorobowe wyzwalane reakcją kom&oacute;rek typu T np. celiakia czy enteropatia wywoływana przez białka mleka [Sampson, Anderson 2000]. Leczenie alergii pokarmowych opiera się przede wszystkim na wykryciu alergenu i stosowaniu diety eliminacyjnej. W diagnostyce stosuje się r&oacute;wnież dodatkowo testy sk&oacute;rne i oznaczenie we krwi przeciwciał IgE skierowanych przeciwko konkretnym alergenom. <br />Prawie wszystkie alergeny (antygeny) są białkami lub glikoproteinami. Ich masa cząsteczkowa wynosi zwykle od 5 kDa do 70 kDa [Taylor 1992]. Prawidłowa identyfikacja alergenu warunkuje p&oacute;źniejszą, prawidłową charakterystykę jego właściwości biochemicznych i immunologicznych oraz wyjaśnienie mechanizmu uczulania. Ze względu na konieczność zachowania alergennego białka w natywnej formie, fundamentalnego znaczenia nabiera proces jego izolowania i oczyszczania. Proces ten nie jest łatwym zadaniem z uwagi na zwykle niewielkie ilości alergennego białka obecne w spożywanych pokarmach lub obecność lipid&oacute;w czy fenoli utrudniających ekstrakcję białka. Charakterystyka białek wywołujących nietolerancje i alergie pokarmowe obejmuje metody immunochemiczne, chromatograficzno-spektralne, elektroforetyczne i spektrometrię mas. <br />Najczęściej stosowaną grupą metod wykrywania alergen&oacute;w w żywności są metody immunochemiczne, np. immunoblotting, immunoelektroforeza, czy ELISA (enzyme-linked immunosorbent assay) [Besler et al., 2000]. Testy ELISA są używane np. do wykrywania glutenu pszenicy a także innych alergennych białek pszenicy. Innymi grupami białek roślinnych wykrywanymi przy użyciu takich test&oacute;w są białka soi, orzeszk&oacute;w ziemnych, seler&oacute;w i gorczycy. Metody immunochemiczne są stosowane także do wykrywania ...",07/01/25,23,Molekularne biomarkery - produkty specyficznej proteolizy jako narzędzia oceny zdrowotnej jakości produktów żywnościowych,"alergia, białka żywności, biomarkery, proteoliza",183
123,4461,opisProjektuStanWiedzy,"<p align=""left""><font size=""3""><strong>System proteolityczny plemnik&oacute;w ssak&oacute;w <br /></strong>Enzymy proteolityczne plemnik&oacute;w ssak&oacute;w odgrywają ważną rolę w procesie zapłodnienia uczestnicząc w trawieniu osłonki przejrzystej oocytu. System proakrosyna/akrosyna stanowi większość trypsynopodobnej aktywności plemnik&oacute;w ssak&oacute;w, co wskazuje na dominującą rolę akrosyny w procesie zapłodnienia. Jednakże ostatnie badania (Baba i wsp., 1994; Adham i wsp., 1997) pomniejszają rolę akrosyny w procesie zapłodnienia i wskazują na istnienie innego niż akrosyna systemu proteinaz serynowych plemnik&oacute;w uczesniczącego w procesie zapłodnienia. <br />Obecnie, na świecie prowadzi się badania mające na celu charakterystykę innych niż akrosyna, proteinaz plemnik&oacute;w. Pierwszym opisanym trypsynopodobnym enzymem, r&oacute;żniącym się od akrosyny był spermiogen (Siegel i wsp., 1987). Wiele wskazuje na to, że jest on składową systemu proakrosyna/akrosyna (Yu i Yi, 2001). Spermiogen powstaje najprawdopodobniej w wyniku potranslacyjnych modyfikacji proakrosyny (Pyoung i Yi, 2004). Przypuszczalnie uczestniczy on w początkowej fazie kontaktu plemnika z oocytem. Innym trypsynopodobnym enzymem zidentyfikowanym w plemnikach knura jest CSP (cetyltrimethylammonium bromide-extracted sperm protease, Akama i wsp., 1994). Enzym ten r&oacute;żni się od akrosyny lokalizacją, specyficznością substratową i wrażliwością na jony wapnia. W plemnikach buhaja opisano obecność proteinazy serynowej o masie 66kDa, zwanej BSp66 (bovine sperm serine protease), kt&oacute;ra podczas kriokonserwcji nasienia tworzy dimer BSp120 (Cesari i wsp, 2003; Cesari i wsp., 2004). Do tej pory nie poznano struktury wymienionych proteinaz. Stosując techniki biologii molekularnej zidentyfikowano proteinazy serynowe, obecne w dojrzałych plemnikach. Obecność transkrypt&oacute;w probiałka konwertazy 4, uczestniczącej w przetwarzaniu prohormon&oacute;w, proneuropeptyd&oacute;w i białek powierzchniowych opisano w spermatocytach i spermatydach gryzoni (Mbikay i wsp., 1997; Tadros i wsp., 2001). Plemniki myszy pozbawione PC4 (proprotein convertase 4) wykazywały obniżoną zdolność zapładniającą, co wskazuje na udział PC4 w procesie zapłodnienia. W proces ten zaangażowane są także dwie inne proteinazy serynowe zidentyfikowane w akrosomach plemnik&oacute;w myszy zwane TESP1 i TESP2 (testicular serine proteinases, Kohno i wsp., 1998). <br />Reasumując, przedstawione dane wskazują na niewątpliwy udział proteinaz w przebiegu procesu zapłodnienia. Okazuje się, że system proteolityczny jest bardziej złożony niż wcześniej sądzono. Dlatego niezbędne są dalsze badania nad ustaleniem roli poszeg&oacute;lnych składowych systemu proteolitycznego plemnik&oacute;w oraz jego kontroli w nasieniu. <br /><strong>System proteolityczny plemnik&oacute;w ptak&oacute;w</strong> <br />Aktywność proteolityczną nasienia ptak&oacute;w wiąże się gł&oacute;wnie z akrosyną (Milne i wsp., 1997). Jej udział w trawieniu osłony ż&oacute;łtkowej oocytu ptak&oacute;w wydaje się bezsporny (Bakst i Howarth, 1977; Okumura i Nishiyama; 1978 Koyanagi i wsp., 1988; Bedford, 1998). Prowadzone prace badawcze systemu akrosynowego plemnik&oacute;w ptak&oacute;w (indor, kogut) doprowadziły do wstępnej izolacji i charakterystyki białek o aktywności proteolitycznej (Richardson i wsp., 1988; Froman, 1990). Aktywność proteolityczną plemnik&oacute;w ptak&oacute;w dotychczasowi badacze (Richardson i wsp., 1988; Richardson i wsp., 1992; Thurston i wsp., 1993) wiązali wyłącznie z aktywnością akrosyny. Opisywane więc właściwości fizykochemiczne i kinetyczne akrosyny dotyczyć mogą także innych niż akrosyna proteinaz serynowych plemnik&oacute;w ptak&oacute;w (Richardson i wsp., 1988; Richardson i wsp., 1992; Thurston i wsp., 1993). Do tej pory nie poznano sekwencji akrosyny ptak&oacute;w. W literaturze brak jest informacji na temat innych niż akrosyna proteinaz serynowych plemnik&oacute;w ptak&oacute;w. Nieznany jes...",07/01/31,23,Izolacja i charakterystyka proteinaz serynowych plemników indora (Meleagris gallopavo),"proteinazy serynowe, plemniki, indor.",183
124,4465,opisProjektuStanWiedzy,"<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;"">Peter Drucker (P.Drucker, <em>Zarządzanie organizacją pozarządową – teoria i prak</em>tyka, Fundusz Współpracy: Program PHARE Dialog Społeczny NGOs, 1995) stwierdza, że „czterdzieści lat temu już samo słowo „zarządzanie” było traktowane przez organizacje pozarządowe jako coś niewłaściwego; oznaczało ono „biznes”, a z nim przecież organizacje te nie miały nic wspólnego.” Ewa Bogacz-Wojtanowska, autorka jedynej pozycji zwartej polskiego autorstwa dotykającej problematyki zarządzania organizacjami pozarządowymi (E.Bogacz-Wojtanowska, <em>Zarządzanie organizacjami pozarządowymi na przykładzie stowarzyszeń krakowskich</em>, Wydawnictwo UJ, Kraków 2006) cytując opinię C.Perrowa zwraca uwagę, że często, zdaniem wielu praktyków, pogląd o braku potrzeby zarządzania (organizacjami non-profit) pokutuje do dziś. Pisze też dalej, że „jednak kiedy w latach dziewięćdziesiątych stały się one (organizacje non-profit) jedną z głównych sił społeczno-politycznych na świecie, ich funkcjonowanie i wyzwania związane ze skutecznym zarządzaniem stały się bardzo aktualne.”<o:p></o:p></span></p>
<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;"">W Polsce istnieje wyraźna luka, zarówno w stanie wiedzy, jak i na rynku wydawniczym,<span style="""">  </span>dotycząca problematyki zarządzania strategicznego organizacjami pozarządowymi. O ile można znaleźć wiele pozycji literaturowych odnoszących się do pierwszego i drugiego sektora, to trzeci wydaje się być wyraźnie niedoceniany przez naszych rodzimych badaczy. Jednocześnie – jak pisze cytowana wyżej E.Bogacz-Wojtanowska – „trudno jest analizować literaturę związaną z zarządzaniem organizacjami pozarządowymi, gdyż ta problematyka jest czasem niekompatybilna z kwestiami związanymi z zarządzaniem polskimi NGOs(...). Dodatkowo trudno porównywać teorie zarządzania np. organizacjami pozarządowymi w Stanach Zjednoczonych, gdyż różnice, chociażby w otoczeniu organizacji, są bardzo znaczne.”<o:p></o:p></span></p>
<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;"">Brak jest nie tylko prób implementacji koncepcji zarządzania strategicznego dla polskich organizacji non-profit. Brakuje również badań empirycznych na temat stosowanych przez nie strategii, metod i technik zarządzania.<o:p></o:p></span></p>
<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;"">Prezentowany projekt wypełni zidentyfikowane wyżej luki. Poprzez wykonanie badań empirycznych będzie stanowił źródło cennych danych zarówno dla ludzi nauki jak i dla praktyków działających w III sektorze. Wydana w wyniku projektu pozycja zwarta będzie również zawierała propozycje konkretnych zastosowań zarządzania strategicznego dla wybranych organizacji pozarządowych.<o:p></o:p></span></p>
<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;""> <o:p></o:p></span></p>
<p style=""margin-left: 18pt; text-align: justify; line-height: 150%;"" class=""MsoNormal""><span style=""font-size: 10pt;"">Tematyka organizacji pozarządowych w Polsce była dotychczas przedmiotem badań, którymi zajmowali się pedagodzy i historycy. Dokonywano analiz funkcji społeczno-wychowawczych i oświatowych organizacji stowarzyszeniowych na szerszym tle (I.Ratman-Liwerska, <em>Stowarzyszenie i jego funkcje</em>, Stowarzyszenie Oświatowców Polskich, Poznań 1995, s. 125). Stowarzyszeniami zajmowali się A.Kamiński, I.Leparczyk, K.Kabziński – pisząc w latach siedemdziesiątych i osiemdziesiątych XX w. prace z zakresu andragogiki. Problematyka ta dość dobrze rozpoznana jest na gruncie nauk socjologicznych – już w latach siedemdziesiątych XX w. K.Z.Sowa pisał o socjologicznej teorii zrzeszeń.<o:p></o:p></span></p>
<p style=""margin-l...",07/01/22,23,Zarządzanie strategiczne organizacjami non-profit w Polsce,"zarządzanie strategiczne, trzeci sektor, organizacje pozarządowe",-1
125,4467,opisProjektuStanWiedzy,"&nbsp;
<div>Modelowanie proces&oacute;w fizycznych w układach FPGA jest rozwiązaniem nowym będącym obecnie na etapie badań zar&oacute;wno w Polsce jak i w świecie. Elementy FPGA wykorzystywane dotąd często w układach regulacji system&oacute;w mechatronicznych można implementować jako model odzwierciedlający działanie procesu fizycznego w czasie rzeczywistym. W tym sensie jest to działanie nowatorskie, osiągalne w wyniku rozwoju możliwości obliczeniowych układ&oacute;w FPGA. </div>
<div>Wnioskodawcy przeprowadzili wstępne badania modeli układ&oacute;w energoelektronicznych implementowanych w elementach FPGA. Wyniki badań zostały następnie wykorzystane do budowy rzeczywistych przekształtnik&oacute;w wielokom&oacute;rkowych, w ramach <span>grantu 3T10A05526, co potwierdziło przydatność tej technologii do analizy a następnie projektowania układ&oacute;w fizycznych.</span></div>",07/01/29,23,Modelowanie systemów mechatronicznych w układach FPGA,"FPGA, mechatronika, modelowanie, energoelektronika, DSP, SoC",173
126,4469,opisProjektuStanWiedzy,"<font size=""3"">&nbsp; </font>
<div align=""justify""><font size=""3"">Dynamiczny rozw&oacute;j mikroelektroniki, ściśle związany ze zmniejszeniem wymiar&oacute;w struktur przestrzennych w układach scalonych, w&nbsp;decydującym stopniu jest warunkowany postępem w&nbsp;opracowaniu nowych system&oacute;w litograficznych. Wymiary struktur p&oacute;łprzewodnikowych wytwarzanych w tej technice zależą w gł&oacute;wnej mierze od długości fali promieniowania optycznego oraz właściwości zastosowanych układ&oacute;w optycznych&nbsp;[1,2]. </font></div>
<p align=""justify""><font size=""3"">W wielu ośrodkach naukowych na&nbsp;świecie prowadzone są prace badawcze nad technologiami wykorzystującymi promieniowanie o&nbsp;&nbsp;długości fali 13,5&nbsp;nm&nbsp; leżące w zakresie widmowym skrajnego nadfioletu, w&nbsp;skr&oacute;cie <em>promieniowania&nbsp;EUV</em>&nbsp;(ang.&nbsp;<em>Extreme</em><em> Ultraviolet</em>)&nbsp;[3]. Na rysunku&nbsp;1 przedstawiono widmo promieniowania elektromagnetycznego oraz wybrane źr&oacute;dła promieniowania emitowanego w&nbsp;danym zakresie długości fal. Technologia ta umożliwi w przyszłości wytwarzanie struktur o&nbsp;minimalnych wymiarach poniżej&nbsp;35&nbsp;nm&nbsp;[4]. </font></p>
<p align=""center""><font size=""3""><img height=""292"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3856"" /></font></p>
<p><font face=""Courier New""><font size=""3"">Rys.&nbsp;1.&nbsp;<span>&nbsp;&nbsp;&nbsp;Widmo oraz wybrane źr&oacute;dła promieniowania elektromagnetycznego&nbsp;[5]</span></font></font></p>
<div align=""justify""><font size=""3"">Jednym z ważnych zagadnień występujących w pracach nad rozwojem litografii&nbsp;EUV są bezwzględne pomiary energii promieniowania o długości fali 13,5&nbsp;nm. Pomiary takie są&nbsp;konieczne do określenia charakterystyki źr&oacute;deł promieniowania&nbsp;EUV oraz układ&oacute;w wykorzystywanych w urządzeniach do litografii. Niepewność pomiaru energii tymi przyrządami jest wyznaczana na specjalnie skonstruowanych stanowiskach pomiarowych&nbsp;[6,&nbsp;7]. W gł&oacute;wnej mierze jej wartość zależy od dokładności określenia czułości widmowej zastosowanych fotodetektor&oacute;w. Ze względu na kr&oacute;tką drogę absorpcji promieniowania EUV w materiałach, istotny wpływ na czułość widmową detektor&oacute;w ma ich konstrukcja i stan powierzchni aktywnej. Każda zmiana struktury detektora ma wpływ na jego parametry detekcyjne. Zmiana ta może zostać wywołana reakcjami chemicznymi (utlenianiem lub pracą detektora w&nbsp;obecności gaz&oacute;w), oddziaływaniem promieniowania o dużej energii oraz gromadzeniem się r&oacute;żnego rodzaju zanieczyszczeń powstających np. podczas wytwarzania promieniowania.</font></div>
<p align=""justify""><font size=""3"">Badania wykonane w Physikalisch-Technischen Bundesanstalt -&nbsp;Berlin&nbsp;(PTB&nbsp;Berlin) wykazały przykładowo, że czułość fotodiod krzemowych zmniejsza się o ok.&nbsp;30&nbsp;% po&nbsp;6&nbsp;latach użytkowania tych detektor&oacute;w&nbsp;(rys.&nbsp;2). </font></p>
<p align=""center""><font size=""3""><img height=""205"" alt="""" width=""600"" src=""/OSFImageLoader.do?idImageDB=3857"" /></font></p>
<p><font face=""Courier New"" size=""3"">Rys.&nbsp;2.&nbsp;Droga absorpcji w krzemie dla promieniowania EUV(a) i zmiana czułości widmowej detektora krzemowego zmierzona w PTB Berlin, linia ciągła &ndash; pierwotna charakterystyka, przerywana-po 3-letnim okresie używania fotodiody(b) [8,9]</font></p>
<div align=""justify""><font size=""3"">Istotne zatem stało się opracowanie stanowisk, kt&oacute;re umożliwiałyby nie tylko wyznaczenie czułości detektor&oacute;w promieniowania&nbsp;EUV, ale r&oacute;wnież okresową kontrolę jej wartości. Na świecie istnieje jedynie kilka ośrodk&oacute;w zajmujących się powyższą problematyką. Do&nbsp;najważniejszych z nich należy zaliczyć PTB Berlin, National Institute of Standards and Technology &ndash; Gaithersburg (NIST &ndash; USA), Budker Institute of Nuclear Physics (Rosja &ndash; Nowosybirk), Electrotechnical Laboratory (ETL), TERAS &nd...",07/01/25,23,"Stanowisko do badań czułości widmowej detektorów promieniowania o długości fali 13,5 nm na potrzeby nanolitografii","litografia, promieniowanie EUV, wzorcowanie detektorów",141
127,4471,opisProjektuStanWiedzy,"<p align=""left"">&nbsp;Nazwa beton podwodny (ang. underwater concerete &ndash; UWC) określa betony cementowe o specyficznych właściwościach mieszanki, układane w deskowaniu lub na istniejącej konstrukcji za pomocą pomp lub innych urządzeń bezpośrednio przez warstwę wody (stojącej bądź płynącej). <br />Mieszanki do wykonywania beton&oacute;w podwodnych, tzw. mieszanki UWC powinny charakteryzować się: <br />- odpornością na wypłukanie i segregację w czasie betonowania podwodnego, tak by straty były jak najmniejsze; <br />- zdolnością do samoczynnego zagęszczania i wypoziomowania; <br />- swobodą przepływu w przestrzeniach między prętami zbrojenia; <br />- zdolnością do całkowitego odpowietrzania się w czasie betonowania; <br />- stałością konsystencji w czasie transportu i układania [1]. <br />Zastosowanie technologii betonu samozagęszczalnego w połączeniu z nowoczesnymi domieszkami zwiększającymi lepkość umożliwiło&nbsp; wykonywanie konstrukcji betonowej pod wodą bez konieczności zagęszczania mieszanki i przy ograniczeniu strat wypłukania w trakcie betonowania do minimum [2]. Prekursorem powstania nowoczesnych beton&oacute;w podwodnych są Niemcy, gdzie&nbsp;w 1989 r&nbsp;wynaleziono domieszkę chemiczną, ktora zwiększała lepkość betonu w kontakcie z wodą i zapobiegała jego segregacji [3]. <br />Betony podwodne są szeroko stosowane w Japonii a ich rozw&oacute;j związany jest z dynamicznym rozwojem technologii beton&oacute;w samozagęszczalnych oraz rynku domieszek chemicznych do betonu [5]. Drugim regionem, w kt&oacute;rym obserwuje się już od&nbsp;wielu lat dynamiczny rozw&oacute;j technologii betonowania podwodnego jest USA [1, 2, 6, 7]. W ostatnich 5 latach Chiny stały się r&oacute;wnież regionem, w kt&oacute;rym betony podwodne stosowane są na dużą skalę, jednakże z uwagi na barierę językową brak jest publikacji z tego regionu. W Europie betony podwodne znalazły duże zastosowanie w takich krajach jak Niemcy [3, 4] oraz kraje skandynawskie. W Polsce są stosowane bardzo sporadycznie, na małą skalę, a&nbsp;samozagęszczalne betony podwodne zastosowano jedynie przy jednym z etap&oacute;w remontu zapory w Porąbce [8]. Zastosowanie beton&oacute;w samozagęszczalnych w technologii betonowania podwodnego jest zagadnieniem nowym, a wprowadzenie na rynek nowych tzw. domieszek kompleksowych spowodowało dość dynamiczny rozw&oacute;j zastosowania beton&oacute;w SCC w technologii betonowania podwodnego.&nbsp;Betony podwodne w tym&nbsp;r&oacute;wnież betony&nbsp;podwodne samozagęszczalne&nbsp; wymagają bezwzględnego stosowania domieszek zwiększających lepkość tzw. domieszek antyrozpływowych, w literaturze określanych r&oacute;wnież jako domieszki typu AWA (ang. antiwashout admixture) [6]. Ich zadaniem jest zwiększenie lepkości mieszanki betonowej tak by nie ulegała ona segregacji a przede wszystkim wypłukiwaniu w trakcie betonowania pod wodą, oraz zapewniała odporność na odziałania erozyjne&nbsp;środowiska&nbsp;[2, 6]. <br />Wymogi stawiane mieszankom UWC związane są ściśle z zastosowaniem domieszki AWA a także z technologią betonowania konstrukcji. W zależności od regionu stosuje się domieszki AWA o rożnym składzie chemiczny, a więc o r&oacute;żnym oddziaływaniu, a także r&oacute;żnorodne technologie betonowania. Dlatego też występują r&oacute;żnice pomiędzy kryteriami jakie powinny spełniać mieszanki UWC wg wymagań japońskich [5], amerykańskich [7] czy niemieckich [ 3]. Kryteria te oparte są w gł&oacute;wnej mierze na jednoparametrycznych testach urabialności mieszanek betonowych takich jak: oznaczanie konsystencji metodą stożka Abramsa, czas przepływu mieszanki przez lejek V-funnel, badanie samozagęszczalności za pomocą aparatu L-box czy przepływu przez zbrojenie pierścieniem J-ring. Wymienione wyżej metody badawcze są uznawane za podstawowe testy stosowane do badania beton&oacute;w samozagęszczalnych [ 9]. Dla samozagęszczalnych beton&oacute;w podwodnych wartości graniczne tych test&oacute;w wykazują w wielu przypadkach znaczne r&oacute;żni...",07/01/19,23,Kształtowanie właściwości samozagęszczalnych betonów podwodnych w aspekcie minimalizacji strat wypłukania mieszanki przy betonowaniu,"betony samozagęszczalne, betony podwodne, domieszki chemiczne do betonu, sztuczne sieci neuronowe",173
128,4472,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font><font face=""Times New Roman"">Dotychczas analizując zmienność wśr&oacute;d zwierząt hodowcy i biolodzy zadawalali się podziałem na podstawowe komponenty. Wraz z rozwojem technik molekularnych, uzyskaliśmy możliwość badania pojedynczych gen&oacute;w i ich loci. Skoro jednak wiele cech zwanych ilościowymi determinowanych jest przez więcej niż jeden locus, to zainteresowani jesteśmy r&oacute;wnież określeniem wielkości wpływu poszczeg&oacute;lnych loci tak by wiedzieć, kt&oacute;ry jest w tym miejscu najważniejszy a kt&oacute;re są mniej ważne. Dzięki temu moglibyśmy prowadzić selekcje pod kątem pojedynczych gen&oacute;w. Możliwe w pewnym stopniu byłoby zatem przewidywanie na przykład wydajności zwierzęcia na podstawie znanego genotypu. <br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sekwencje mikrosatelitarne stały się w ostatnich latach jedną z najliczniejszych grup marker&oacute;w. Ich rozproszenie w genomie, wysoki stopień polimorfizmu oraz proste metody ich identyfikowania pozwalają na wykorzystanie ich w mapowaniu genomu, analizie r&oacute;żnic wewnątrz i międzyodmianowych jak i mapowaniu loci cech ilościowych. Markery te umożliwiają r&oacute;wnież identyfikację gen&oacute;w odpowiedzialnych za cechy ważne z punktu widzenia hodowcy. <br />Wykorzystanie marker&oacute;w mikrosatelitarnych w połączeniu z nowoczesnymi, zautomatyzowanymi metodami ich analizy, umożliwiło zastosowanie precyzyjnej kontroli pochodzenia zwierząt. Polimorficzne sekwencje DNA występują w populacji ze zr&oacute;żnicowaną liczbą powt&oacute;rzeń danego motywu nukleotydowego, co wyraża się w długości ciągu powt&oacute;rzeń i jest łatwe do wykrycia metodami biologii molekularnej. W praktyce analiza marker&oacute;w mikrosatelitarnych przebiega w 3 kolejnych etapach: izolacja DNA z materiału biologicznego, specyficzna amplifikacji wyizolowanego DNA oraz rozdziału elektroforetycznego.&nbsp;Amplifikacja, czyli namnożenie wybranych fragment&oacute;w DNA in vitro, wykonywana jest za pomocą łańcuchowej reakcji polimerazy - PCR (Polymerase Chain Reaction), Reakcja ta wykorzystuje właściwości polimerazy DNA - enzymu, kt&oacute;ry syntetyzuje DNA komplementarny do matrycowego w mieszaninie zawierającej cztery tr&oacute;jfosforany dezoksyrybonukleotyd&oacute;w (dATP, dCTP, dGTP i dTTP) i dwie sekwencje starterowe. <br />Dotychczas znaczenie sekwencji mikrosatelitarnych jest raczej negatywne, gdyż wykazano związek niekt&oacute;rych z nich i chor&oacute;b neurologicznych. I nie chodzi tu wyłącznie o zależność statystyczną. W niekt&oacute;rych przypadkach zmienne sekwencje mikrosatelitarne zakł&oacute;cają działanie innych gen&oacute;w. Większość, bo ponad 90% tych sekwencji leży poza genami. Natomiast ogromna ich ilość i obecność w okolicach gdzie swoje loci maja geny szlak&oacute;w regulatorowych, powoduje, że niekt&oacute;rzy autorzy poważnie traktują możliwość istnienia określonych funkcji tych odcink&oacute;w DNA. Bardzo prawdopodobne jest, że mogą one mieć wpływ na ekspresję gen&oacute;w oraz na miejsca rekombinacji. Niekt&oacute;re obserwacje sugerują, że mikrosatelity mogą działać jak elementy wzmacniające transkrypcję genu.&nbsp;</font><font face=""Times New Roman""><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Końcowym etapem analizy sekwencji mikrosatelitarnych jest elektroforeza pionowa w 4% żelu poliakrylamidowym, w laserowym sekwenatorze DNA sprzężonym z komputerem, wyposażonym w specjalistyczne programy komputerowe. Analiza wielkości zamplifikowanych fragment&oacute;w DNA wykonywana jest automatycznie. Zainstalowany czytnik laserowy skanuje znakowane fluorescencyjnie produkty PCR w trakcie rozdziału elektroforetycznego, z kt&oacute;rego dane, w postaci sygnał&oacute;w fluorescencji, są transmitowane do pamięci komputera. Wielkości poszczeg&oacute;lnych alleli, przetwarzane są automatycznie przez program komputerowy na...",07/01/27,23,Wykorzystanie polimorfizmu sekwencji mikrosatelitarnych w doskonaleniu cech ilościowych zwierząt futerkowych z rodziny canidae.,"sekwencje mikrosatelitarne, STR, ",173
129,4481,opisProjektuStanWiedzy,"<div style=""text-align: justify;"">
<meta name=""GENERATOR"" content=""OpenOffice.org 2.0  (Linux)"" />
<meta name=""CREATED"" content=""20061206;7415700"" />
<meta name=""CHANGED"" content=""16010101;0"" /><style type=""text/css"">

	<!--
		@page { size: 21cm 29.7cm; margin: 2cm }
		P { margin-bottom: 0.21cm }
	-->
	</style> </div>
<p lang=""pl-PL"" style=""margin-bottom: 0cm; line-height: 150%; text-align: justify;""><font size=""2"" style=""font-size: 11pt;"">Grupa naukowa związana z kierownikiem projektu prowadzi od wielu lat intensywną współpracę naukową z renomowanymi ośrodkami naukowymi. Współpraca ta zaowocowała opublikowaniem wielu artykułów naukowych. Nie ulega więc wątpliwości, że istnieje potrzeba kontynuowania współpracy i rozszerzenia badań na nieco inne aspekty związane z możliwością otrzymania nowych wyników obserwacji z Hinode. Oczekujemy, że rozwój proponowanych badań przyczyni się do znacznej konsolidacji grup badawczych. Oczekiwane korzyści związane są ściśle z programem proponowanych badań. Badania te zaowocują znaczną ilością ważnych rezultatów, które poprawią naszą wiedzę o falach MHD w pętlach korony słonecznej. </font></p>",07/01/16,23,Fale i oscylacje w pętlach magnetycznych korony słonecznej,"korona słoneczna, pętle magnetyczne, magnetohydrodynamika, oscylacje magnetohydrodynamiczne",183
130,4482,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Mikroklimat w znaczeniu encyklopedycznym jest to klimat charakterystyczny dla małej części środowiska, kt&oacute;rej odrębność jest wynikiem specyfiki układu czynnik&oacute;w ją tworzących, np. wysokością i wahaniami temperatury, wilgotności, szybkością ruchu powietrza itp. Określonym mikroklimatem może charakteryzować się zar&oacute;wno obszar geograficzny, jak i tw&oacute;r sztuczny zbudowany przez człowieka (Wielka Encyklopedia PWN 2000). </font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font size=""3""><font face=""Times New Roman""><span style=""COLOR: black"">&nbsp;&nbsp;&nbsp;&nbsp; W zakresie postawionego problemu wpływu zbiornik&oacute;w wodnych na mikroklimat i</span> modyfikacji mikroklimatu przez oddziaływania antropogeniczne brak jest&nbsp;najnowszych badań, ponieważ wymagają one, mimo niewielkiej skali obszarowej, dużo większego nakładu pracy i większej liczby punkt&oacute;w pomiarowych, niż te dotyczące zagadnień związanych z bioklimatem (Krawczyk, Błażejczyk 1999), topoklimatem (Bokwa 2006, Błażejczyk 2001, Kaczorowska 1953) czy mezoklimatem (Hess i in. 1981, Nowak 1968). Istniejące dotykają także zgoła innej problematyki, np.: ekosystem&oacute;w leśnych (Durło i in. 2005, Olszewski 1991, Obrębska-Starklowa 1968), warunk&oacute;w miejskich (Boryczka 1966, Paszyński 1955) lub stricte rolniczej (Paszyński 1957). Dodatkowo prace badawcze muszą być w miarę kompleksowe i obejmować swoim zasięgiem znajomość zagadnień z dziedzin pokrewnych, np.: hydrologii.</font></font></p>
<p class=""MsoBodyTextIndent2"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 0cm; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Najszerzej zagadnieniem wpływu zbiornik&oacute;w wodnych na klimat w Polsce zajmowała się do tej pory Lewińska (1966, 1967, 1973, 1984), nie mniej jednak dotyczyły one budowli sztucznych (zbiorniki zaporowe ), kt&oacute;re dodatkowo były położone w specyficznym obszarze klimat&oacute;w g&oacute;rskich. Autorka w swoich publikacjach odwołuje się także do badań prowadzonych nad innymi zbiornikami retencyjnymi (Włocławek, Czorsztyn, Otmuch&oacute;w, Solina), kt&oacute;re stanowiły opinie i ekspertyzy. Jednak wnioski w nich zawarte nie wynikały z bezpośrednich pomiar&oacute;w, ale gł&oacute;wnie z og&oacute;lnej znajomości wpływu powierzchni wodnych na klimat obszar&oacute;w sąsiadujący z nimi. </font></p>
<p class=""MsoBodyTextIndent2"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 0cm; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Podobną problematyką zajmował się także Marzec (1966, 1967, 1984). Autor na podstawie danych dla zbiornika rożnowskiego z okresu 1959-1963, w oparciu o meldunki 4&nbsp;stacji meteorologicznych, kilku urządzeń rejestrujących umieszczonych wzdłuż jednego profilu podłużnego w stosunku do zbiornika oraz pomiar&oacute;w patrolowych ustalił domniemany zasięg oddziaływania do 5 km od zbiornika. Pr&oacute;bę określenia warunk&oacute;w mikroklimatycznych wok&oacute;ł zbiornik&oacute;w wodnych (staw&oacute;w) podjął także Szymański i in. (1991). </font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Dotychczasowe poglądy opierają się na wyrywkowych badaniach terenowych, w oparciu o bardzo rzadką sieć punkt&oacute;w pomiarowych i w og&oacute;le nie uwzględniają wpływu cyrkulacji atmosferycznej.</font></p>",07/07/26,27,Mikroklimat w obrębie naturalnych zbiorników wodnych jako czynnik zagospodarowania przestrzennego i zrównoważonego rozwoju terenów pojeziernych Polski północno-wschodniej,"mikroklimat, strefa przybrzeżna jezior, antropopresja, zrównoważony rozwój, planowanie przestrzenne, Polska północno-wschodnia",173
131,4487,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm -7.9pt 0pt 0cm; LINE-HEIGHT: 150%; TEXT-ALIGN: justify; tab-stops: -9.0pt""><strong><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-bidi-font-size: 11.5pt"">Cukrzyca typu 1<o:p></o:p></span></strong></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm -7.9pt 0pt 0cm; LINE-HEIGHT: 150%; TEXT-ALIGN: justify; tab-stops: -9.0pt""><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-bidi-font-size: 11.5pt"">&nbsp;<o:p></o:p></span></p>
<p><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-bidi-font-size: 11.5pt; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">W populacjach europejskich takich jak Polska, Niemcy czy Francja ryzyko zachorowania na cukrzycę typu 1 wynosi około 0,2 &ndash; 0,4 %. Genetyczna predyspozycja do ujawniania się cukrzycy jest uwarunkowana oddziaływaniem wielu gen&oacute;w. Czynniki środowiskowe odgrywają istotną rolę w rozwoju procesu autoimmunologicznego prowadzącego do destrukcji kom&oacute;rek </span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: ’Times New Roman’; mso-bidi-font-size: 11.5pt; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">&beta;</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-bidi-font-size: 11.5pt; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA""> trzustki u os&oacute;b predysponowanych genetycznie [Myśliwiec M 2007].</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA""> Badania sugerują, że destrukcja kom&oacute;rek </span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: ’Times New Roman’; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">&beta;</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA""> jest wynikiem interakcji pomiędzy składnikami odporności kom&oacute;rkowej (makrofagi, limfocyty CD4<sup>+</sup>, limfocyty CD8<sup>+</sup>). Rozwijają one chroniczny stan zapalny, w kt&oacute;rym funkcje efektorowe pełnią cytokiny prozapalne (IL1, TNF-</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: ’Times New Roman’; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">&alpha;</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">, IFN-</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: ’Times New Roman’; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">&gamma;</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">) [Mandrup-Poulsen T 2001].<span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA""> Dochodzi do zaburzenia r&oacute;wnowagi, kt&oacute;ra prowadzi do apoptozy kom&oacute;rek </span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: ’Times New Roman’; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">&beta;</span><span style=""FONT-SIZE: 10pt; FONT-FAMILY: Tahoma; mso-fareast-font-family: ’Times New Roman’; mso-ansi-language: PL; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"">. W takich okolicznościach zadziałanie mechanizmu hamującego aktywowane limfocyty autoreaktywne jest konieczne do prawidłowego <span style=""FONT-SIZE: 10pt; FONT...",06/12/20,23,Limfocyty T regulatorowe CD4+ CD25+ jako cel terapii w leczeniu cukrzycy typu 1,"cukrzyca typu 1, limfocyty T regulatorowe CD4+ CD25+, Foxp3",183
132,4497,opisProjektuStanWiedzy,"<p align=""justify"">Organizacja i działalność administracji publicznej &ndash; bo w takim obszarze poznawczym mieści się zgłoszony projekt badawczy &ndash; niezmiennie budzi zainteresowanie nauki zar&oacute;wno w Polsce, jak i w innych krajach. Stopień rozpoznania tego problemu jest naturalnie zr&oacute;żnicowany, w zależności od czasookresu i terytorium. W Polsce lat 1944 &ndash; 1950, kt&oacute;rych dotyczy projekt, problemy te budzą zainteresowanie, gł&oacute;wnie poprzez pryzmat działalności organ&oacute;w policyjnych. W &oacute;wczesnym systemie i praktyce ustrojowej, organy te były częścią tzw. administracji specjalnej, niezespolonej. Takie ich usytuowanie sprawiło, że pozostała część aparatu administracyjnego państwa &ndash; tzw. administracja zespolona, pozostaje na marginesie badań. Zubaża to znacząco stan wiedzy o pierwszych latach powojennych. Z tego punktu widzenia jest to tym bardziej dokuczliwe, iż &oacute;wczesna struktura administracji publicznej, była niezmiernie złożona, bowiem tworzyły ją trzy podstawowe segmenty &ndash; administracja rządowa centralna i terytorialna, administracja samorządowa oraz rady narodowe, jako nowa instytucja, zapożyczona z ZSRR. W historiografii okresu PRL, z wspomnianych trzech segment&oacute;w, pewnym zainteresowaniem badawczym cieszyły się rady narodowe oraz niekt&oacute;re, centralne organy administracji rządowej. Pozostałe części aparatu administracyjnego, pozostawały przedmiotem uwagi wyłącznie w nielicznych pracach regionalnych, kt&oacute;rych ustalenia wymagają reinterpretacji. W historiografii lat 1990 &ndash; 2005, ta część aparatu administracyjnego - jak już sygnalizowano, pozostawała i pozostaje na marginesie badań, zdominowana przez kwestie związane z funkcjonowaniem aparatu policyjnego. Przedstawiony projekt badawczy powinien w efekcie przynieść, z jednej strony poszerzenie wiedzy o działalności znaczącego fragmentu aparatu administracyjnego państwa, z drugiej prowadzić do reinterpretacji &ndash; jeśli badania będą dawały do tego podstawy &ndash; czynionych wcześniej ustaleń. Opr&oacute;cz owych najbardziej elementarnych cel&oacute;w, w polu zainteresowań znajdą się kwestie nieco bardziej szczeg&oacute;łowe, jak relacje między administracją og&oacute;lną a administracją specjalną; analiza funkcjonowania administracji og&oacute;lnej w okresie kierowania nią przez reprezentanta Polskiego Stronnictwa Ludowego w odniesieniu do okres&oacute;w gdy funkcje te pełnili reprezentanci PPR i PZPR; obszary kompetencji i zainteresowań administracji og&oacute;lnej, zespolonej, odnoszone do okresu międzywojennego; analiza obsady kadrowej, r&oacute;wnież odnoszona do okresu międzywojennego. </p>",07/01/15,23,Ministerstwo Administracji Publicznej; 1944 - 1950,Administracja zespolona,173
133,4499,opisProjektuStanWiedzy,"Mimo imponującego rozwoju informatyki w ostatnich dziesięcioleciach, polisomnogramy ciągle opisywane są metodą wzrokową tak jak 36 lat temu [1]. Opis i analiza całonocnego snu nierzadko zajmuje specjaliście tyle samo czasu co sam sen. Po latach licznych prób zrezygnowano z konstruowania urządzeń do automatycznej konstrukcji polisomnografu ze względu na brak dostatecznej zgodności opisu z analizą wzrokową w przypadkach patologicznych. Obecnie panuje zgodny pogląd, że lepiej jest wykorzystać możliwości komputera do obliczania ilościowych  wskaźników polisomnogramu, które nie są dostępne w analizie wzrokowej, a zawierają dużą wartość diagnostyczną (analiza mikrostruktury zapisu EEG snu). Przykładem może być chronospektrogram fal delta, czyli wykres widma mocy fal delta w kolejnych minutach snu. <br />Polisomnogram zawiera jeszcze wiele ważnych informacji, które mają bezpośrednie znaczenie diagnostyczne, jak liczba bezdechów, liczba wybudzeń, liczba wrzecion snu i zespołów K, a w przypadku danych epileptycznych częstość występowania iglic epileptycznych. Jest jeszcze wiele informacji o znaczeniu poznawczym, których wartość praktyczna wymaga weryfikacji. <br />Jako jedyne powszechnie akceptowane w encefalografii metody automatyczne wymienić można właściwie tylko uśrednianie w czasie potencjałów wywołanych oraz estymację mocy widmowej w ustalonych przedziałach częstości [4]. Jednym z podstawowych powodów tej sytuacji jest to, że współczesne metody oferują opis szeregów czasowych w postaci mało intuicyjnych parametrów, nie dających się prosto przetłumaczyć na język elektroencefalografii klasycznej. Do opisu EEG snu próbowano stosowa m.in. analizę falkową [5] i sztuczne sieci neuronowe [6]; metody te jednak nie oferują parametrów, które można by przetłumaczyć bezpośrednio na język stosowany w analizie wzrokowej. Pierwszą metodą, oferującą opis obecnych w sygnale struktur explicite przez ich czas trwania, częstość i amplitudę, jest dopasowanie krokowe (matching pursuit, [9]). Metodę tę wprowadzono do analizy sygnałów biomedycznych w Zakładzie Fizyki Biomedycznej IFD UW [7]. Dla efektywnego zastosowania w praktyce klinicznej i naukowej potrzebna jest jej weryfikacja na większej ilości danych oraz optymalizacja parametrów. <br />Znaczenie rozwiązania problemu postawionego w projekcie dla nauki i encefalografii światowej polega na ostatecznej weryfikacji metody na większej ilości danych oraz optymalizacji parametrów analizy zapisów polisomnograficznych.<br />Projekt dostarczy znaczącego argumentu doświadczalnego na rzecz tezy o możliwości połączenia współczesnych metod analizy sygnałów ze skarbnicą wiedzy zgromadzonej przez dziesięciolecia klasycznej, wzrokowej analizy EEG.",07/01/25,23,Mikrostruktura polisomnogramów,"Elektroencefalogram (EEG), hipnogram, czsowo-częstotliwościowa analiza sygnału, adaptywne aproksymacje",183
134,4501,opisProjektuStanWiedzy,"&nbsp;
<div>Oddziaływania zasad azotowych w DNA decydują nie tylko o utworzeniu oraz rodzaju konformacji ściśle upakowanej struktury DNA. Determinują one r&oacute;wnież przebieg proces&oacute;w fotofizycznych, kt&oacute;re mogą wystąpić po pochłonięciu energii fotonu z dziedziny ultrafioletu (około 5 eV). Struktura energetyczna kolektywnych (tj. ekscytonowych) stan&oacute;w elektronowych powstających w rezultacie oddziaływania zasad decyduje o szybkości rozpraszania energii wzbudzenia elektronowego na ciepło, co ma zasadnicze znaczenie dla trwałości DNA. Struktura ta zależy od sprzężeń wzbudzonych stan&oacute;w elektronowych, kt&oacute;re wynikają z bliskości powłok elektronowych zasad przede wszystkim wzdłuż osi helisy jak r&oacute;wnież od sprzężeń poprzez wiązania wodorowe w komplementarnych parach zasad. </div>
<div>Og&oacute;lnie przyjęty opis wzbudzonych stan&oacute;w elektronowych polinukleotyd&oacute;w opiera się na tzw. teorii ekscytonowej, w kt&oacute;rej uwzględnia się oddziaływania elektron&oacute;w sąsiadujących ze sobą zasad ograniczone tylko do oddziaływań kulombowskich, tj. energii potencjalnej przyciągania/odpychania ładunk&oacute;w elektrycznych elektron&oacute;w walencyjnych i rdzeni atom&oacute;w w oddziałujących molekułach. Nie uwzględnia się w nim oddziaływań wymiennych polegających na częściowym &quot;uwsp&oacute;lnieniu&quot; elektron&oacute;w walencyjnych w przylegających do siebie powłokach elektronowych zasad. Ponadto baza stan&oacute;w wzbudzonych, z kt&oacute;rych konstruuje się opis (tj. funkcje falowe) kolektywnych stan&oacute;w wzbudzonych, jest ograniczana do zbioru stan&oacute;w wzbudzonych osiąganych w przejściach pi-pi* i zawsze pomija możliwy udział stan&oacute;w osiąganych w przejściach n-pi*. Ważne osiągnięcia tego modelu stanowią: jakościowe objaśnienie kształtu widm dichroizmu kołowego (CD) oraz hipochromizmu, kt&oacute;re są bardzo przydatne dla diagnozowania stanu konformacyjnego oraz renaturacji i hybrydyzacji DNA. </div>
<div>Jednak ani w pierwotnej wersji, w kt&oacute;rej oddziaływanie elektron&oacute;w przybliżano modelem dipoli punktowych [<strong>1,2</strong>], ani też w niedawno opracowanej wersji uwzględniającej rozciągłość przestrzenną molekuł zasad azotowych [<strong>3-5</strong>], ten opis stan&oacute;w wzbudzonych nie przewiduje występowania niskoenergetycznych stan&oacute;w wzbudzonych w polinukleotydach, kt&oacute;rych istnienie wyraźnie przejawia się w widmach fluorescencji silnie przesuniętych w stronę długofalową w stosunku do widm absorpcji. Istnienia takich stan&oacute;w nie potwierdziły badania dichroizmu liniowego, stanowiącego potencjalnie narzędzie do takich cel&oacute;w. Może to jednak wynikać ze zbyt małej intensywności pasm odpowiadających tym stanom w widmie absorpcji. Dodatkową niesprzyjającą okolicznością jest to, że badania spektroskopowe polinukleotyd&oacute;w i DNA (w tym badania dichroizmu liniowego) z reguły wykonywane są na preparatach w normalnej temperaturze (w gradiencie przepływu w roztworze lub w rozciąganej folii polimerowej), co znacznie pogarsza spektralną zdolność rozdzielczą w rejestrowanych widmach, z natury szerokich i nie wykazujących wewnętrznej struktury w takich warunkach. </div>
<div>&nbsp;</div>
<div>Zasadniczy postęp w badaniach elektronowych stan&oacute;w wzbudzonych polinukleotyd&oacute;w został osiągnięty stosunkowo niedawno, dzięki zastosowaniu technik spektroskopowych z dużą czasową zdolnością rozdzielczą (poniżej pikosekund). Pozwalają one zaobserwować zar&oacute;wno zmienność czasową widma fluorescencji [<strong>6</strong>] jak i ewolucję stan&oacute;w wzbudzonych (także tych, kt&oacute;re nie emitują fluorescencji) poprzez obserwację zmian widma absorpcji badanej pr&oacute;bki po impulsie światła wzbudzającego [<strong>7</strong>]. Badania te ujawniły istnienie stan&oacute;w wzbudzonych polinukleotyd&oacute;w w szerokim zakresie energii. Niekt&oacute;re z tych stan&oacute;w, szczeg&oacute;lnie stany długożyciowe, przyp...",07/01/29,23,Elektronowe stany wzbudzone polinukleotydów - badania metodą spektroskopii efektu Starka,"DNA, polinukleotydy, stany elektronowe, spektroskopia, efekt Starka",173
135,4506,opisProjektuStanWiedzy,"<p align=""justify"">Podjęta w projekcie problematyka jest kontynuacją badań nad wielowymiarową normalnością rokładu. Badania takie od wielu lat są prowadzone przez statystyk&oacute;w zar&oacute;wno w kraju jak i za granicą. Tematyka ta jest nadal aktualna gdyż nie zostały jeszcze rozwiązane wszystkie problemy z nią związane. Oryginalnym wkładem niniejszego projektu będzie opracowanie nowego testu, obliczenie jego wartości krytycznych oraz por&oacute;wnanie z innymi, znanymi już testami (między innymi: uog&oacute;lnionym testem Shapiro &ndash; Wilka, testami skośności i kurtozy). Testy powszechnie dostępne w literaturze dotyczą badania wielowymiarowego rozkładu dla pr&oacute;by prostej (jeden obiekt badany na jednorodnych jednostkach eksperymentalnych). W projekcie rozważymy także problem og&oacute;lniejszy, dotyczący obserwacji opisanych dowolnym modelem liniowym. </p>",07/01/26,23,"Badanie wielowymiarowej normalności obserwacji - opracowanie nowego testu statystycznego, porównanie go  z  innymi testami oraz jego aplikacja w analizie wielowymiarowych obserwacji z doświadczeń rolniczych","wielowymiarowy rozkład normalny, test statystyczny, wielocechowe obserwacje, doświadczenia rolnicze",183
136,4509,opisProjektuStanWiedzy,"<p style=""text-align: justify;"" class=""MsoNormal""><font size=""3""><span style=""font-family: Times New Roman;"">            Proponowany projekt jest fragmentem prac w obszarze badań nad polimerami pamiętającymi kształt. Dotyczy on wąskiej grupy inteligentnych polimerów posiadających jednocześnie bardzo cenne z punktu widzenia zastosowań biomedycznych własności; tj biodegradowalność i biokompatybilność.      </span><br style=""font-family: Times New Roman;"" /><span style=""font-family: Times New Roman;""> Pamięcią kształtu nazywamy własność materiałów polegającą na zdolności do powrotu z kształtu przejściowego (“zamrożonego”) otrzymanego w wyniku mechanicznej deformacji, do wcześniejszego pierwotnego kształtu, wywoływaną zdefiniowanym bodźcem, którym najczęściej jest temperatura. Materiały te ze względu na tą interesującą własność są obecnie powszechnie stosowane w różnorodnych dziedzinach przemysłu. Pierwszymi poznanymi materiałami charakteryzującymi się tymi własnościami są różnorodne stopy metaliczne (SMAs), takie jak stopy TiNi, CuZnAl czy FeNiAl. W 1960 ukazała się pierwsza publikacja opisująca istnienie efektu pamięci kształtu polietylenu naświetlanego promieniowaniem jonizującym [1,2]. Dopiero nieco później, w latach osiemdziesiątych ubiegłego wieku pojawiły się doniesienia o możliwości syntezy innych polimerów mających w pełni własność zapamiętania kształtów (SMPs), nie tylko na drodze sieciowania radiacyjnego lecz również chemicznego [1].      </span><br style=""font-family: Times New Roman;"" /><span style=""font-family: Times New Roman;""> Polimery SMPs cechują się wieloma zaletami w porównaniu do stopów SMAs, są lżejsze, wykazują silniejsze własności pamięci kształtu, lepszą stabilność kształtu, oraz większą możliwość dostosowywania temperatury powrotu do kształtu pierwotnego. Są również stosunkowo łatwo przetwarzalne i generalnie tańsze [1,3,4]. SMPs można podzielić na dwa typy polimerów; termoplastyczne i polimery usieciowane. Termoplastyczne SMPs, najbardziej interesujące ze względu na łatwe przetwórstwo, a co za tym idzie szerokie pole zastosowań, charakteryzowane są jako liniowe kopolimery zawierające w swojej strukturze łańcucha sztywne i elastyczne segmenty. Sztywne segmenty, w wyniku separacji faz, tworzą typowe fazy krystaliczne, a segmenty elastyczne odpowiedzialne są za tworzenie fazy amorficznej. Generalnie materiał taki można dowolnie kształtować poprzez ogrzanie do temperatury zbliżonej do temperatury mięknięcia domeny zbudowanej z segmentów sztywnych. Ten pierwotny kształt może być zapamiętany po schłodzeniu polimeru, w wyniku fizycznego międzycząsteczkowego oddziałowywania  segmentów sztywnych, co tworzy powstanie efektu zbliżonego do fizycznego sieciowania łańcuchów. W wypadku polimerów sieciowanych, efekt ten związany z obecnością segmentów sztywnych w kopolimerze zastępuje się poprzez wprowadzenie kowalentnego wiązania łączącego segmenty elastyczne i w ten sposób usieciowania łańcuchów. Wiązanie to uzyskać można na drodze radiacyjnej, jak ma to miejsce podczas otrzymywania pamiętającego kształt polietylenu, czy mieszanin polimerowych z jego udziałem, lub przez sieciowanie chemiczne, po wprowadzenie do łańcucha grup zdolnych do tej reakcji [5,6]. Jeśli następnie materiał ten ulegnie deformacji mechanicznej w temperaturze niższej od temperatury mięknięcia segmentów sztywnych, otrzymuje kształt przejściowy. Powrót do pierwotnego kształtu może nastąpić po podgrzaniu materiału powyżej temperatury zeszklenia segmentów elastycznych, ale niższej od temperatury mięknięcia segmentów sztywnych.       </span><br style=""font-family: Times New Roman;"" /><span style=""font-family: Times New Roman;"">          Efekt pamięci kształtu można uzyskać zarówno w polimerach semikrystalicznych, jak i amorficznych, pod warunkiem uzyskania wspomnianego specyficznego efektu sieciowania liniowego łańcucha. Obecnie najciekawszą, ze względu na swoje potencjalne zastosowanie, wydaje się podgrupa MSPs zawierająca semikrystaliczne kopolimery...",06/12/07,23,Synteza biodegradowalnych i biokompatybilnych kopolimerów z pamięcią kształtu do zastosowań biomedycznych,"biodegradowalność, biokompatybilność,pamięć kształtu, poliester, laktyd. glikolid",173
137,4510,opisProjektuStanWiedzy,"<p class=""MsoBodyText"" style=""margin: 0cm 0cm 0.0001pt; text-align: justify;"">Wyb&oacute;r tematyki badań jest podyktowany dynamicznym rozwojem sieciowych struktur organizacyjnych, gł&oacute;wnie w sektorze handlu i usług detalicznych. Ich istotą jest budowanie wielowymiarowych związk&oacute;w w sieci i z otoczeniem, kt&oacute;re naturalną koleją rzeczy wynikają z paradygmatu marketing relacji. <span style="""">&nbsp;</span><span style="""">&nbsp;</span></p>
<p class=""MsoBodyText"" style=""margin: 0cm 0cm 0.0001pt; text-align: justify;"">Literatura z tego zakresu, w kraju i na świecie, ogranicza się w większości przypadk&oacute;w do rozważań stricte teoretycznych i raczej bardzo og&oacute;lnikowo przedstawia powyższe zagadnienia. <span style="""">&nbsp;</span>Zdecydowanie brakuje publikacji monograficznych. <span style="""">&nbsp;</span><o:p></o:p></p>
<p class=""MsoBodyText"" style=""margin: 0cm 0cm 0.0001pt; text-align: justify;"">W tym kontekście szczeg&oacute;lnie istotne wydaje się przeprowadzenie pogłębionych badań empirycznych, kt&oacute;re nie były dotychczas prowadzone w kraju na szerszą skalę. </p>
<p class=""MsoBodyText"" style=""margin: 0cm 0cm 0.0001pt; text-align: justify;"">Badania, kt&oacute;rych celem będzie zidentyfikowanie modeli relacji i czynnik&oacute;w sukcesu r&oacute;żnych struktur sieci organizacyjnych wydaje się mieć szczeg&oacute;lne znaczenie w perspektywie konkurowania polskich firm na rynku Unii Europejskiej. <span style="""">&nbsp;</span><o:p></o:p></p>
<p class=""MsoNormal""><span style=""""><o:p>&nbsp;</o:p></span></p>",07/02/20,23,Zarządzanie relacjami w sieciach organizacyjnych,"organizacje i struktury sieciowe, zarzadzanie marketingiem relacji, modele relacji  marketingowych ",173
138,4511,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Według Goldberg&rsquo;a (1975) i Philips&rsquo;a (1980) niekt&oacute;re bezkręgowce morskie mogą być przydatne w biomonitoringu zanieczyszeń metalicznych na obszarze przybrzeżnych w&oacute;d morskich. Potencjalnymi bioindykatorami powinny być tylko organizmy o odpowiedniej wielkości, szeroko rozpowszechnione, pospolite i dostępne, łatwo rozpoznawalne, względnie stacjonarne (osiadłe), osiągalne w ciągu całego roku oraz tolerancyjne na niskie zasolenia (Szefer, 2002). Z danych literaturowych jednoznacznie wynika, że na uwagę zasługuje <em>Mytilus edulis</em> (Szefer i Szefer, 1990; Szefer i in., 2002, 2006; Bryan, 1980; Goldberg i in., 1983; Philips, 1980) ze względu na jego użyteczność jako bioindykatora zanieczyszczenia akwen&oacute;w morskich pierwiastkami śladowymi. Jest szeroko rozpowszechniony, nawet tam gdzie inne organizmy bentosowe są nieobecne. Może to wskazywać na szeroki zakres tolerancji oraz duże zdolności adaptacyjne tego gatunku.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Znaczącą rolę w procesie zanieczyszczania środowiska morskiego metalami i metaloidami odgrywają osady denne. Ich cząstki są spożywane przez większość organizm&oacute;w bentosowych, m.in. przez osobniki <em>Macoma balthica.</em> Posiadają zdolność do zmiany sposobu pobierania pokarmu przechodząc od filtratora do osadożercy, a więc mogą pobierać pokarm zar&oacute;wno z zawiesiny naddennej, jak i z podłoża osadowego (fakultatywny spos&oacute;b odżywiania). Jest atrakcyjnym obiektem badań dotyczących oceny stopnia przyswajalności pierwiastk&oacute;w metalicznych w odniesieniu do cząstek materiału osadowego. Badania nad zawartością pierwiastk&oacute;w śladowych w osadach i żyjących w nich osobnikach <em>Macoma balthica</em> były przeprowadzone m.in. przez Luoma i in. (1993), jednak brak jest dostępnych danych literaturowych dotyczących składu pierwiastkowego tkanki miękkiej <em>Macoma balthica</em> w odniesieniu do substratu osadowego oraz zmienności sezonowej i regionalnej na obszarze Morza Bałtyckiego.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp; Wielowymiarowe techniki statystyczne są często stosowane w ilościowej ocenie rozmieszczenia pierwiastk&oacute;w śladowych, a także makroelement&oacute;w w pr&oacute;bkach środowiskowych. Najczęściej stosowaną techniką chemometryczną jest m.in analiza wariancji (ang. <em>Analysis of Variance</em> - ANOVA), kt&oacute;ra umożliwia śledzenie i definiowanie zmian zawartości pierwiastk&oacute;w w pr&oacute;bach organizm&oacute;w w zależności od m.in. ich cech gatunkowych, regionu i sezonu, a także wielkości (wieku). W literaturze bardzo często spotyka się przykłady praktycznego zastosowania techniki PCA (ang. <em>Principal Component Analysis</em>) do interpretowania rozmieszczenia ładunk&oacute;w czynnikowych (zmiennych = pierwiastk&oacute;w) oraz obiekt&oacute;w (pr&oacute;bek) w celu m.in. określenia źr&oacute;deł ich pochodzenia w środowisku naturalnym.</font></p>",07/01/29,23,Ocena wpływu czynników środowiskowych na stopień bioakumulacji metali toksycznych w Macoma balthica południowego Bałtyku,"bioakumulacja, monitoring, mięczaki, pierwiastki toksyczne, Południowy Bałtyk",183
139,4517,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp; Pierwsze prace naukowe na temat przejścia szklistego pojawiły się na początku ubiegłego wieku. Pojęcie materiału szklistego ulegało znacznemu rozszerzeniu od pierwszej definicji Kauzmanna [4], że &bdquo;szkło to ciecz o zamrożonych pewnych stopniach swobody&rdquo;. Od roku 1968 badany jest nowy rodzaj szkieł tj. &bdquo;szkła orientacyjne&rdquo; [2] gdzie zamrażany jest nieporządek orientacji molekuł rozłożonych regularnie w sieci krystalicznej, a od roku 1974 &bdquo;szkła anizotropowe&rdquo;, gdzie zamrażana ciecz ma molekuły rozłożone przypadkowo w przestrzeni, a dalekozasięgowy charakter ma uporządkowanie osi wydłużonych molekuł [5]. Ostatnio badane są r&oacute;wnież szkła gdzie nieporządek dotyczy jedynie konformacji łańcuch&oacute;w molekularnych w fazie krystalicznej [6]. Analiza zachowania szkieł z pewnymi elementami uporządkowania ułatwia powiązanie efekt&oacute;w cieplnych obserwowanych w przejściu szklistym z dobrze zdefiniowanymi ruchami molekularnymi. R&oacute;wnocześnie, użycie kalorymetru w okolicy T<sub>g</sub> jako spektrometru do detekcji bardzo wolnej dynamiki pozwala rozszerzyć obserwacje prowadzone metodą dielektryczną od czas&oacute;w relaksacji rzędu 10<sup>-10</sup> s do czas&oacute;w makroskopowych [7]. Czas relaksacji &tau; (T) można oszacować z wartości naddatku entropii konfiguracyjnej S<sub>c</sub> [8] obserwowanego w fazie szklistej nad entropią fazy krystalicznej co przedstawia wz&oacute;r zaproponowany przez Adama i Gibbsa </font></p>
<p align=""center""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&tau; (T) = &tau;<sub>o</sub> exp(A/kTS<sub>c</sub>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;(1)</font></p>
<p align=""left""><font face=""Times New Roman"" size=""3""></font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">Do obserwacji &tau; (T) służy r&oacute;wnież bezpośredni pomiar spontanicznego, bardzo powolnego wzrostu temperatury T(t) w okolicy T<sub>g</sub> w warunkach adiabatycznych, kt&oacute;ry odzwierciedla dążenie entalpii do stanu r&oacute;wnowagi [9]. Obserwacje tego procesu będą opisane w pracy doktorskiej. Jego gł&oacute;wnym mechanizmem jest dynamika rotacyjna molekuł. <br />&nbsp;&nbsp;&nbsp; W ostatnich trzydziestu latach narasta liczba publikacji dotyczących uniwersalnych własności obserwowanych niezależnie od szczeg&oacute;ł&oacute;w budowy chemicznej szkieł [10]. Jedną z takich własności jest gwałtowne spowolnienie dynamiki molekuł przy zbliżaniu się do temperatury T<sub>g </sub>przejścia szklistego obserwowane jako relaksacja strukturalna. Jest ona opisywana &bdquo;superarrheniusowską&rdquo; zależnością czasu relaksacji od temperatury [11]</font></p>
<p align=""center""><font face=""Times New Roman"" size=""3"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&tau; (T) = &tau;<sub>o</sub> exp (DT<sub>o</sub>/(T-T<sub>o</sub>))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (2)</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">zwaną r&oacute;wnaniem Vogela-Fulchera-Tammanna. Parametr &bdquo;fragility&...",07/01/29,23,Badanie polimorfizmu i dynamiki izomerów dimetylobutanoli,"przejscia fazowe, ciecze przechłodzone, fazy ODIC, szkło fazy ciekłej i ODIC, dynamika rotacyjna  molekuł, badania dielektryczne ",173
140,4519,opisProjektuStanWiedzy,"<p class=""MsoPlainText""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">Są dwa podejścia do programowania w logikach modalnych: pierwsze podejście [5,18] polega na przetłumaczeniu programu w logice modalnej na program w klasycznej </span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">logice pierwszego rzędu</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">, a drugie podejście [1,2,11,15] polega na opracowaniu bezpośrednich semantyk i procedur obliczania dla programów w logikach modalnych. Pierwsze podejście jest proste, ale w pewnym sensie przerzuca ciężar problemu z jednego miejsca na drugie, bo logiki modalne stwarzają dodatkowe rozgałęzienia w procesie wyszukiwania i takie rozgałęzienia muszą być rozpatrywane w jakiś sposób. Używając translacji funkcyjnej [5], zmodyfikowany algorytm unifikacji może zwrócić różne najbardziej ogólne podstawienia uzgadniające, które powodują rozgałęzienie. Używając translacji semi-funkcyjnej [18], dodatkowe rozgałęzienia są spowodowane przez klauzule dodawane do programu dla reprezentowania właściwości struktur Kripkego danej logiki modalnej. W podejściu bezpośrednim, dodatkowe rozgałęzienia są spowodowane przez modalne reguły, które są używane jako meta klauzule. W moim przekonaniu, warto zbadać podejście bezpośrednie do programowania w logikach modalnych, ponieważ to wymaga głębszej analizy i może stworzyć efektywniejsze procedury obliczania. <o:p></o:p></span></p>
<p class=""MsoPlainText""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">Moja opracowana w [15] metoda do definiowania semantyki najmniejszego modelu, semantyki stałopunktowej i rachunku SLD-rezolucja dla pozytywnych programów w logikach modalnych używa podejścia bezpośredniego i jest zastosowalna do logik modalnych, których struktury Kripkego są scharakteryzowane przez seryjność (aksjomat <em>(D)</em>) i klauzule Horna używające relacji widoczności światów. Została ona zastosowana w [15] do całej klasy logik modalnych scharakteryzowanych przez aksjomaty <em>(D)</em> i dowolną kombinację uogólnionych aksjomatów <em>(T)</em>, <em>(I)</em>, <em>(B)</em>, <em>(4)</em> i <em>(5),</em> postaci, np., <img align=""middle"" src=""/OSFImageLoader.do?idImageDB=3801"" alt="""" />. Moja metoda jest lepsza od wcześniejszych metod używających podejścia bezpośredniego [1,2], ponieważ nie wymaga żadnego ograniczenia wystąpień modalnych operatorów w programie i w zapytaniu. Moja teoria programowania w logikach modalnych [15] jest bardzo przydatna i dlatego warto ją rozszerzyć i udoskonalić. <o:p></o:p> <o:p></o:p></span></p>
<p class=""MsoPlainText""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">Klasa seryjnych logik bezkontekstowo-gramatycznych <em>sCFG</em> jest duża; zawiera m.in. <em>D<sub>(n)</sub></em>, <em>T<sub>(n)</sub></em>, <em>KD4<sub>(n)</sub></em> i <em>S4<sub>(n)</sub></em> i użyteczne logiki modalne dla wnioskowania o przekonaniach, np. <em>KDI4</em>, <em>KDI4<sub>s</sub></em>, <em>KD4I<sub>g</sub></em> [15,17]. Niektóre logiki klasy <em>sCFG</em> są rozpatrywane w pracy [5] Debarta i innych, ale generalnie większość logik klasy <em>sCFG</em> nie była rozpatrywana w pracach [1,5,18,11,15]. Klasa <em>sCFG</em> jest podklasą klasy logik kontekstowo-gramatycznych rozpatrywanych w pracy [2] Baldoniego i innych, ale ci autorzy w [2] zbadali tylko pozytywne programy i zapytania bez egzystencjonalnych operatorów modalnych, a ja nie wymagam takiego ograniczenia dla programowania w <em>sCFG</em>. <o:p></o:p> <o:p></o:p></span></p>
<p class=""MsoPlainText""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;;"">Optymalizacje są bardzo ważne dla efektywnych procedur wnioskowania. Proponowane przeze mnie rodzaje optymalizacji będą miały duże znaczenie dla sukcesów programowania w logikach modalnych. Standardowa forma dla cykli rezolucji i zoptymalizowana forma dla modalnych reguł są specyficzne dla pode...",07/01/25,23,Teoria programowania w logikach modalnych i deskrypcyjnych,"logiki modalne, logiki deskrypcyjne, programowanie w logice, dedukcyjne bazy danych",183
141,4529,opisProjektuStanWiedzy,"Klaudiusz Hrabyk był dziennikarzem. Przed drugą wojną światową związał się z obozem narodowym. Okupację spędził głównie w Warszawie, aż do upadku powstania warszawskiego, w którym czynnie uczestniczył. Po upadku powstania przedostał się do Niemiec, gdzie po wyzwoleniu przez Amerykanów trafił do obozu dipisów. Działał tam jako redaktor prasy polskiej i organizator stowarzyszeń dziennikarskich. W 1949 roku wyemigrował do Stanów Zjednoczonych. Osiadł w Nowym Jorku, gdzie przyłączył się do obozu piłsudczyków. Działał w Lidze Niepodległości Polski, był także czynnym publicystą prasy polonijnej. Od 1956 roku zaczął odwiedzać Polskę, dojrzewając stopniowo do decyzji o przeniesieniu się tam na stałe, co nastąpiło w roku 1959. W Kraju poszedł na daleko idącą współpracę z reżymem, między innymi w zakresie &quot;rozpracowywania&quot; Polonii amerykańskiej, a także jej oczerniania w publikacjach prasowych. <br /><br />Hrabyk nie doczekał się wciąż rzetelnej naukowej biografii, choć zarówno w środowisku naukowym, jak też w szerszej opinii publicznej był postacią znaną, choć ocenianą negatywnie. Chcielibyśmy, by obszerny szkic biograficzny, który poprzedzić ma publikację źródeł, wypełnił tę lukę. Liczymy, że w toku kwerendy uda się zweryfikować opinie dotyczące autora.<br /><br />Poruszane przez Klaudiusza Hrabyka zagadnienia są przedmiotem zainteresowania historyków i politologów polskich, emigracyjnych i polonijnych (amerykańskich). Spuścizna ta była już częściowo przez nich wykorzystywana, jednak w niewielkim stopniu. Publikacja z pewnością ułatwi i poszerzy dostęp do tego cennego źródła. <br />",07/01/25,23,Wspomnienia i dzienniki Klaudiusza Hrabyka z lat 1902-1959,"Hrabyk Klaudiusz, obóz narodowy, konspiracja, wysiedleńcy, emigracja (1949-1959), Polonia, piłsudczycy, dziennikarstwo, II Rzeczpospolita (1918-1939), okupacja (1939-1945), Niemcy, Stany Zjednoczone, pamiętniki, wspomnienia",173
142,4531,opisProjektuStanWiedzy,"<p>Badania migracji gaz&oacute;w ze zlikwidowanych kopalń prowadzono na terenie Niemiec, Anglii,Czech i&nbsp;Polski ale koncentrowano się na badaniu wypływ&oacute;w metanu przez zlikwidowane szyby. </p>
<p>Prowadzone były też badania nad wypływem metanu do przewietrzanych wyrobisk (Trutwin 1973), ale bez szczeg&oacute;łowego zbadania mechanizmu wypływu pod wpływem zmian ciśnienia, a także bez uwzględnienia możliwości migracji gazu w kierunku przeciwnym do strumienia świeżego powietrza.</p>
<p>Wysnuto błędną teorię, że intensywność emisji gaz&oacute;w zależy jedynie od poziomu ciśnienia atmosferycznego. Dopiero w 2005r. składający niniejszy wniosek w swojej rozprawie doktorskiej udowodnił, że na wielkość emisji wpływają także takie czynniki jak: charakter zniżki, intensywność zniżki, czas jej trwania, historia zmian itd.</p>
<p>Pomiary w Skansenie prowadzone były nieregularnie i bez uwzględnienia pomiar&oacute;w prędkości gaz&oacute;w.&nbsp;Ninejszy projekt będzie podstawą do przyszłej pracy nad stworzeniem modelu matematyczno- fizycznego umożliwiającego prognozowanie wypływ&oacute;w gaz&oacute;w ze zlikwidowanych kopalń, a także np. zza otamowanych wyrobisk. Pr&oacute;by stworzenia takiego modelu podjęte zostały we wspomnianej pracy doktorskiej. </p>",07/01/19,23,Wpływ zmian ciśnienia barycznego na migrację dwutlenku węgla ku powierzchni w kierunku przeciwnym do strumienia wentylacyjnego w zlikwidowanej kopalni węgla kamiennego,"górnictwo, gazy zrobowe, emisja gazów, bezpieczeństwo, ochrona powietrza",173
143,4534,opisProjektuStanWiedzy,"<div><span>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span>Badania zmierzające do ominięcia energochłonnego procesu reformingu parowego metanu jako etapu konwersji metanu do metanolu sięgają początk&oacute;w XX wieku [2]. Przez dziesiątki lat utlenianie metanu było prowadzone w fazie gazowej w temperaturze 450 &ndash; 700<sup>o</sup>C, pod ciśnieniem 1&nbsp;&ndash;&nbsp;10&nbsp;MPa [2&nbsp;&ndash;&nbsp;6] lub pod ciśnieniem atmosferycznym, w obecności katalizator&oacute;w stałych [7&nbsp;&ndash;&nbsp;9]. Uzyskiwano stopień przemiany metanu w metanol lub formaldehyd niższy niż 10%. W tak wysokiej temperaturze zachodzą reakcje rodnikowe i reaktywność związk&oacute;w obecnych w mieszaninie reakcyjnej jest zdeterminowana siłą wiązań C&ndash;H. Energia dysocjacji pierwszego wiązania C&ndash;H w metanie wynosi 440 kJ&middot;mol<sup>-1</sup> a w metanolu 393 kJ&middot;mol<sup>-1</sup>. Aktywacja alkoholu zachodzi szybciej niż metanu i dlatego gł&oacute;wnymi produktami są tlenki węgla. </span>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W 1972 roku grupie Shilova [20] udało się otrzymać metanol w wyniku utlenienia metanu w temperaturze 120<sup>o</sup>C. Reakcję prowadzono w wodnym roztworze H<sub>2</sub>PtCl<sub>6</sub> i Na<sub>2</sub>PtCl<sub>4</sub>, pod ciśnieniem 3,7&nbsp;&ndash;&nbsp;4,3&nbsp;MPa. Stwierdzono, że nie był to proces rodnikowy, a reaktywność metanolu była podobna jak metanu. </span></div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W 1992 roku Periana i wsp&oacute;łpracownicy [11] opatentowali metodę konwersji węglowodor&oacute;w do alkoholi, tioli, estr&oacute;w, halogenopochodnych oraz wyższych węglowodor&oacute;w. Dla konwersji metanu do metanolu można ją przedstawić schematycznie:</span></div>
<div>1) metan + utleniacz + kwas = ester metylowy + zredukowany utleniacz</div>
<div>2) ester metylowy + woda = metanol + kwas</div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Katalizatorami pierwszego etapu (estryfikacji) były metale z grupy B układu Mendelejewa, kt&oacute;re ulegają łatwo redukcji dwuelektronowej, a trudno jednoelektronowej. </span></div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W literaturze zostało opisane kilkanaście katalizator&oacute;w estryfikacji metanu. Zastosowanie kilku z nich pozwoliło na otrzymanie produktu. Można tu wymienić: dichloro(</span>2-{2,2`-bipirymidyno})platyna(II) + oleum zawierające 20% wag. SO<sub>3</sub> (180&nbsp;&ndash;&nbsp;220<sup>o</sup>C; 3,45 MPa metanu, stopień przemiany metanu w ester 73%) [16], związki jodu + oleum zawierające 65% wag. SO<sub>3</sub> (180<sup>o</sup>C; 5,1 MPa metanu, stopień przemiany metanu w ester 73%) [14], HgSO<sub>4</sub> + kwas siarkowy (180<sup>o</sup>C, 3,45 MPa metanu, stopień przemiany metanu w ester 43%) [12], trifluorooctan palladu(II) + kwas trifluorooctowy (80<sup>o</sup>C; 5,52 MPa metanu, stopień przemiany metanu w ester 60%) [17], pallad + oleum zawierające 30% wag. SO<sub>3</sub> (160<sup>o</sup>C; 3,5 MPa metanu, stopień przemiany metanu w ester 23%) [21].</div>
<div><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Badania planowane w ramach projektu będą zmierzały do znalezienia nowych, efektywnych katalizator&oacute;w procesu utleniania metanu w fazie ciekłej, gdyż w literaturze opisanych jest kilka, w obecności kt&oacute;rych uzyskuje się wysoki stopień przemiany metanu w ester. Prace będą dotyczyły r&oacute;wnież jodu, ale medium reakcyjne będzie zawierało znacznie mniej tritlenku siarki (25% wag.) niż w doświadczeniach opisanych w literaturze [14]. Planuje się zbadanie możliwości zastosowania jako katalizator&oacute;w związk&oacute;w innych chlorowc&oacute;w: chloru i bromu. Podczas prowadzenia badań w ramach projektu badawczego 3 T09B 001 27 realizowanego w latach 2004-2006 stwierdzono, że w obecności związk&oacute;w palladu lub platyny metan ulega w oleum utlenieniu do wodorosiarczanu metylu. Celowe będzie więc zastosowanie jako potencjalnych katalizator&oacu...",07/01/22,23,Niskotemperaturowa aktywacja metanu  w oleum,"metan, utlenianie, oleum, estryfikacja",183
144,4536,opisProjektuStanWiedzy,"Jak wcześniej zostało nadmienione, takie rozwiązanie nie występuje w dostępnych źr&oacute;dłach informacji. Sugerowane rozwiązanie, w konfiguracji ze strumienicą wentylacyjną napędzaną promieniowaniem słonecznym, jest nowością. Pozostałe części składowe systemu, jako typ rozwiązania, są znane lecz nie występują w proponowanej konfiguracji. <br />Problemem do rozwiązania jest ich wzajemna wsp&oacute;łpraca i wymagane parametry. Potrzebne jest opracowanie systemu sterowania wraz z zasadami jego działania. R&oacute;wnocześnie planowane jest dopracowanie działania solarnej strumienicy wentylacyjnej pod względem sterowania wydatkiem. Poziom wiedzy o elementach systemu pozwala przewidywać pozytywną weryfikację hipotezy. <br />",07/01/31,23,System klimatyzacji kurnika wykorzystujący wyłącznie energię natury,"budynek inwentarski, wentylacja, energia słoneczna, odzysk energii, ochrona środowiska",173
145,4537,opisProjektuStanWiedzy,"<p>Homocysteina (Hcy) jest niebiałkowym aminokwasem siarkowym, uczestniczącym w metabolizmie metioniny (Met) i cysteiny (Cys). Pomimo ważnej roli, jaką Hcy odgrywa w przemianach wymienionych aminokwasów, jej podwyższony poziom jest szkodliwy dla organizmu i wskazuje na ryzyko wystąpienia chorób układu krążenia oraz chorób neurodegeneracyjnych, nowotworowych, chrób nerek i innych [1]. Choroby układu krążenia są najczęstszą przyczyną zachorowalności i umieralności w krajach rozwiniętych. Dokładnie poznane czynniki ryzyka, jak hipercholesterolemia, nadciśnienie tętnicze, palenie tytoniu, otyłość czy uwarunkowania genetyczne nie wyjaśniają jednak wszystkich przypadków pojawienia się chorób układu krążenia. Rośnie zatem zainteresowanie nowoodkrytym czynnikiem ryzyka, jakim jest hiperhomocysteinemia. </p>
<p>Jednym z głównych celów licznie prowadzonych badań dotyczących metabolizmu Hcy jest wyjaśnienie mechanizmu toksyczności tego aminokwasu. Hcy wykazuje działanie toksyczne w stosunku do śródbłonka naczyniowego. Zwiększa również proliferację komórek mięśni gładkich naczyń krwionośnych oraz agregację płytek krwi, a także wpływa na krzepnięcie i fibrynolizę [2]. Hcy indukuje lub działa synergistycznie z innymi czynnikami wywołującymi miażdżycę [3]. Zaobserwowano również niekorzystny wpływ zaburzeń metabolizmu Hcy na strukturę i funkcję włókien elastycznych. </p>
<p>Elastyna jest nierozpuszczalnym białkiem macierzy zewnątrzkomórkowej i rdzeniowym białkiem włókien elastycznych, które nadają elastyczność skórze, płucom, wiązadłom i ścianie tętnic. Jej prekursorem jest wydzielana przez różne komórki tropoelastyna o masie około 70 kDa. Tworzenie wiązań krzyżowych między cząsteczkami tropoelastyny katalizowane jest przez oksydazę lizynową i polega na oksydacyjnej deaminacji grupy ε-aminowej łańcuchów bocznych specyficznych reszt lizynowych, w wyniku czego powstaje desmozyna. </p>
<p>Zaburzenia sieci włókien elastycznych odgrywają ważną rolę w patogenezie chorób tętnic i skóry. Wyniki badań prowadzonych na myszach pozbawionych genu elastyny wykazały, że niedobór tego białka prowadzi do śmierci spowodowanej zaporową chorobą tętnic (ang. <em>obstructive arterial disease</em>) [4]. Mutacje genu elastyny powodują nadzastawkowe zwężenie aorty (ang. <em>supravalvular aortic stenosis</em>) [5], charakteryzujące się zgrubieniem ściany oraz ogniskowym lub rozproszonym zwężeniem aorty. Wzrost stężenia Hcy w osoczu szczurów, wywołany niedoborem witaminy B<sub>6</sub>, powoduje zmniejszenie liczby wiązań krzyżowych między resztami Lys elastyny [6]. Ponadto, Hcy indukuje aktywność elastolityczną w medium komórek VSMC (ang. <em>vascular smooth muscle cells</em>) [7]. Dieta wysokometioninowa prowadzi do zaburzenia struktury włókien elastycznych u kurcząt. Jednocześnie nie stwierdza się obniżenia zawartości desmozyny, ani spadku aktywności oksydazy lizynowej. Obserwuje się natomiast zmiany organizacji mikrofibryli oraz utratę immunoreaktywności fibryliny-2 [8]. Hiperhomocysteinemia u myszy powoduje przerost tętniczek mózgowych [9]. </p>
<p>Toksyczność Hcy może być spowodowana jej reakcją z białkami. Mechanizm inkorporacji Hcy do łańcucha polipeptydowego polega na acylacji grupy ε-aminowej reszt lizyny (N-homocysteinylacja) przez zaktywowaną grupę karboksylową tiolaktonu Hcy [10]. N-Homocysteinylacja białek została początkowo odkryta w hodowlach komórkowych [10-12], a następnie w organizmie człowieka [13, 14]. W krwi człowieka obecne są zarówno tiolakton Hcy [14-16] jak i N-homocysteinylowane białka [13, 17, 18]. Hipoteza tłumacząca toksyczność nadmiaru Hcy niekorzystnym wpływem tiolaktonu Hcy na białka jest według nas godna uwagi ponieważ: 1. N-homocysteinylacja zachodzi już przy fizjologicznych stężeniach tiolaktonu Hcy we krwi człowieka, 2. N-homocysteinylowane białka zawierają znacznie więcej Hcy niż pula traktowana jako ’całkowita Hcy’ (uwolniona po redukcji wiązań disiarczkowych w osoczu), 3. N-homocysteinylacja powoduje poważne konsekwencje fizjo...",07/01/29,23,Rola N-homocysteinylacji tropoelastyny w procesie utraty elastyczności naczyń krwionośnych w miażdżycy tętnic,"homocysteina, tiolakton homocysteiny, tropoelastyna, N-homocysteinylacja, włókna elastyczne, miażdżyca",173
146,4539,opisProjektuStanWiedzy,"<p>Zgodnie z Wytycznymi Europejskiego Towarzystwa Kardiologicznego dotyczącymi diagnostyki i leczenia tętniczego nadciśnienia płucnego&nbsp;badanie hemodynamiczne stanowi element oceny tętniczego nadciśnienia płucnego. Aby potwierdzić rozpoznanie tętniczego nadciśnienia płucnego należy wykonać cewnikowanie prawego serca. Badanie to pozwoli ocenić nasilenie zaburzeń hemodynamicznych i sprawdzić reaktywność naczyń krążenia płucnego. Decyzję o wyborze sposobu terapii dokonuje się m.in. na podstawie badania ostrej reaktywności naczyń płucnych na podanie kr&oacute;tkodziałających środk&oacute;w rozszerzających naczynia. Obecnie stosuje się donory tlenku azotu (tlenek azotu, nitrosprusydek sodu) i inhibitory fosfodiesterazy typu 3 (milrinon). Wśr&oacute;d metod leczenia nadciśnienia płucnego wyr&oacute;żniono farmakoterapię z uwzględnieniem doustnych lek&oacute;w przeciwkrzepliwych, diuretyk&oacute;w, glikozyd&oacute;w naparstnicy, dobutaminy, bloker&oacute;w kanał&oacute;w wapniowych, analog&oacute;w prostacykliny, antagonist&oacute;w receptor&oacute;w dla endoteliny-1 oraz inhibitor&oacute;w fosfodiesterazy typu 5 (sildenafilu).</p>
<p>Nie ma żadnej przyjętej metody leczenia pacjent&oacute;w z żylnym nadciśnieniem płucnym; sildenafili jest natomiast uznanym środkiem stosowanym w leczeniu tętniczego nadciśnienia płucnego. Stosowane powszechnie pr&oacute;by ostrego odwr&oacute;cenia nadciśnienia płucnego pozwoliły na wyselekcjonowanie grupy pacjent&oacute;w, u kt&oacute;rych nie stwierdza się odwracalności nadciśnienia płucnego lub jego obniżeniu towarzyszy jednoczaswoy spadek ciśnienia w krążeniu systemowym.</p>
<p>Obecny stan wiedzy pozwala na podjęcie pr&oacute;b diagnostyki (już rozpowszechnionej) i farmakoterapii, kt&oacute;ra wymaga standaryzacji i wprowadzenia środk&oacute;w wcześniej nie stosowanych, lecz zalecanych przez wiodące ośrodki medyczne i publikacje naukowe. Do tych środk&oacute;w zaliczyć należy sildenafil, jak dotąd stosowany w leczeniu tętniczego nadciśnienia płucnego. Jest to silny i selektywny inhibitor cGMP-fosfodiesterazy (PDE) typu 5, kt&oacute;rego farmakologiczne działanie polega na zwiększaniu wewnątrzkom&oacute;rkowego&nbsp;stężenia cGMP. Zwiększenie poziomu tego nukleotydu ma działanie antyproliferacyjne i wywołuje rozkurcz kom&oacute;rek mięśni gładkich naczyń&nbsp;krwionośnych.&nbsp;Ekspresja i aktywność genu dla PDE-5 zwiększa się w przewlekłym nadciśnieniu płucnym. Wskazuje to na potencjalny selektywny wpływ sildenafilu na naczynia krwionośne płuc jak i na możliwy korzystny efekt w terapii żylnego nadciśnienia płucnego, wsp&oacute;łtowarzyszącego niewydolności układu krążenia u potencjalnych biorc&oacute;w serca.</p>
<p>Istnieją doniesienia z kilku niekontrolowanych badań dotyczące korzystnego działania inhibitora fosfodiesterazy typu 5 - sildenafilu w postaci doustnej u pacjent&oacute;w z nadciśnieniem płucnym tętniczym. Lek stosowany trzy razy dziennie w zakresie dawek od 25 do 75 mg wydaje się poprawiać zar&oacute;wno parametry hemodynamiczne układu sercowo-płucnego, jak i wydolność wysiłkową. Wyniki randomizowanego kontrolowanego badania klinicznego prowadzonego metodą skrzyżowaną: stosowanie sildenafilu w dawce 25-100 mg trzy razy dziennie przez 22 pacjent&oacute;w z nadciśnieniem płucnym tętniczym zaliczonych do klasy II i III wydolności czynnościowej wg NYHA, wiązało się ze złagodzeniem objaw&oacute;w choroby po 6 tygodniach leczenia, poprawą wydolności wysiłkowej oraz poprawą parametr&oacute;w hemodynamicznych. </p>
<p>Optymalne przygotowanie biorcy serca z nadciśnienim płucnym do transplantacji jest warunkiem powodzenia tej procedury. Zastosowanie uniwersalnego schematu diagnostyki i terapii zmniejszy częstość występowania powikłań związanych z wczesną niewydolnością przeszczepionego serca, obniżając tym samym koszty tak drogiej procedury medycznej.</p>
<p>Wprowadzenie sildenafilu jako leku dla pacjent&oacute;w, dla&nbsp;kt&oacute;rych możliwości dalszego leczenia niewydolności krążen...",07/01/31,23,Zastosowanie doustnej postaci selektywnego inhibitora fosfodiestrazy typu 5 (sildenafilu) do obniżenia nadciśnienia płucnego u chorych z niewydolnością serca oczekujących na zabieg ortotopowego przeszczepienia serca,"naczyniowy opór płucny, nadciśnienie płucne, niewydolność serca, ortotopowe przeszczepienie serca, inhibitory fosfodiesterazy, sildenafil",173
147,4542,opisProjektuStanWiedzy,"<font size=""3"" style=""font-family: Arial;"">&nbsp;&nbsp; Trawienie białka i wchłanianie aminokwas&oacute;w w jelicie cienkim ptak&oacute;w jest uzależnione między innymi od podatności białek na hydrolizę enzymatyczną oraz ich umiejscowienia w strukturach kom&oacute;rkowych materiału roślinnego. Niekt&oacute;re substancje pochodzące z pasz i obecne w treści pokarmowej mogą wiązać w swojej strukturze aminokwasy uwolnione z białek, spowalniać przepływ treści jelit i utrudniać wchłanianie aminokwas&oacute;w, zaburzać funkcje gruczoł&oacute;w trawiennych i hormonalnych, ograniczać sekrecję enzymatyczną nabłonka jelit lub wręcz blokować działanie enzym&oacute;w proteolitycznych. Do czynnik&oacute;w, kt&oacute;re w spos&oacute;b pośredni lub bezpośrednio mogą ograniczać wykorzystanie przez ptaki aminokwas&oacute;w z białkowych produkt&oacute;w przetwarzania rzepaku należy duża zawartość wł&oacute;kna surowego, rozpuszczalnych frakcji węglowodan&oacute;w nieskrobiowych (NSP), glukozynolan&oacute;w alkenowych i produkt&oacute;w ich rozpadu, kwasu fitynowego i jego soli , a także obecność związk&oacute;w fenolowych i tanin (Banaszkiewicz, 2000; Smulikowska, 2004) W przypadku suszonych wywar&oacute;w zbożowych czynnikiem tym jest wysoki udział wł&oacute;kna (Świątkiewicz i Koreleski, 2003; </font><font size=""3"" style=""font-family: Arial;"">Koreleski i </font><font size=""3"" style=""font-family: Arial;"">Świątkiewicz, </font><font size=""3"" style=""font-family: Arial;"">2004). Na pogorszenie strawności i dostępności aminokwas&oacute;w wpływają dodatkowo ostre parametry termiczne przerobu. Zbyt wysoka temperatura kondycjonowania nasion, tostowania śruty lub suszenia wywar&oacute;w powoduje w pierwszym rzędzie spadek strawności i przyswajalności lizyny(Grala i in., 1994; Ergul i in., 2003). W krajowych badaniach wykazano, że strawność frakcji białkowych wytłok&oacute;w rzepakowych u kurcząt waha się od 72 do 83% i ulega obniżeniu przy ogrzewaniu surowca w temperaturach przekraczających&nbsp;90<sup>o</sup>C (Smulikowska, 2004). W śrucie i wytłokach z rzepaku oraz w suszonych wywarach może dochodzić zatem do obniżenia wartości białka wywołanego obecnością&nbsp; niekt&oacute;rych substancji antyodżywczych oraz działania czynnik&oacute;w technologicznych. Z drugiej strony wytłoki otrzymywane w procesie tłoczenia na zimno zawierają więcej glukozynolan&oacute;w i szkodliwych produkt&oacute;w ich rozpadu niż śruta (Podk&oacute;wka, 2004). Wzrost wykorzystania przez młody dr&oacute;b aminokwas&oacute;w z pasz rzepakowych następuje po obr&oacute;bce mechanicznej (obłuszczanie, frakcjonowanie) oraz hydrobarotermicznej (ekstrudowanie, ekspandowanie). Nakłady związane z tymi zabiegami są jednak znaczne. Dobrą skutecznością w zakresie poprawy wykorzystania aminokwas&oacute;w z tych pasz charakteryzują się także odpowiednio dobrane zestawy enzym&oacute;w paszowych pochodzenia mikrobiologicznego (Szczurek i in., 2000). <br />&nbsp;&nbsp; Jak wynika z danych opublikowanych przez r&oacute;żnych autor&oacute;w jednym ze sposob&oacute;w poprawy wykorzystania przez dr&oacute;b surowc&oacute;w paszowych o obniżonej dostępności aminokwas&oacute;w jest bilansowanie białka mieszanek z udziałem tych surowc&oacute;w w oparciu o zawartość aminokwas&oacute;w strawnych (Waldroup, 2000; Lemme i in., 2004). W odniesieniu do aminokwas&oacute;w (AA) termin &quot;dostępność&rdquo; jest najczęściej definiowany jako wydajność z jaką pobrany aminokwas jest wykorzystany do syntezy białka, w&oacute;wczas gdy jest on jedynym limitującym czynnikiem paszy. Dostępność jest własnością AA, na kt&oacute;rą składają się trzy zasadnicze procesy: trawienie (hydroliza białek paszy), wchłanianie jej produkt&oacute;w przez błonę śluzową jelita oraz wykorzystanie AA (retencja wchłoniętych AA). Chociaż nie zawsze wszystkie AA wchłonięte w jelicie cienkim mogą zostać przyswojone w procesach biosyntezy jest oczywiste, że nie strawione lub nie wchłonięte AA (tj. te, kt&oacute;re opuściły jelito cienkie i...",07/01/29,23,Jelitowa strawność aminokwasów produktów ubocznych przetwarzania rzepaku i pozyskiwania bioetanolu - zastosowanie oznaczonych współczynników dla poprawy wykorzystania tych pasz w żywieniu młodych kurcząt,"aminokwasy, rzeczywista strawność jelitowa, śruta rzepakowa, wytłoki, suszony wywar z kukurydzy, bilansowanie mieszanek, kurczęta brojlery ",183
148,4551,opisProjektuStanWiedzy,"<p align=""justify"">Kokcydioza jest jedną z najpoważniejszych przyczyn strat w hodowli drobiu. Dlatego też w hodowli przemysłowej&nbsp; pisklęta&nbsp;odchowywane na nioski otrzymują paszę z kokcydiostatykami do 12-16 tygodnia życia, natomiast&nbsp;&nbsp;ptaki hodowane na mięso - niemal przez cały okres tuczu (z zachowaniem okresu karencji przed ubojem, liczącym, zależnie od związku, 3-5 dni). <br /></p>
<p align=""justify"">Zgodnie z regulacją 1831/2003/EC, kokcydiostatyki dopuszczane są do stosowania jako dodatki paszowe. Mogą one być stosowane w określonym stężeniu w paszy przez zdefiniowany okres czasu. Nie mogą być stosowane u kur niosek. Niekt&oacute;re związki kokcydiob&oacute;jcze&nbsp; stosowane są u r&oacute;żnych zwierząt jako leki i dopuszczane są do stosowania po ustaleniu dla nich wartości MRL (najwyższej dopuszczalnej pozostałości).&nbsp; Wartości MRL ustalane są stopniowo r&oacute;wnież dla kokcydiostatyk&oacute;w stosowanych jako dodatki paszowe. Dotychczas oficjalne wartości MRL ustanowione przez EMEA/CVMP i obowiązujące w krajach Unii Europejskiej ustalono dla lazalocydu w tkankach drobiu: 100 &mu;/kg w wątrobie, sk&oacute;rze i tłuszczu w naturalnych proporcjach, 20 &mu;g/kg w mięśniach, 50 &mu;g/kg w nerkach i, tymczasowo, 150 <span>&mu;</span>g/kg w jajach, a także dla halofuginonu w tkankach bydła i toltrazurilu w tkankach drobiu i ssak&oacute;w.</p>
<p align=""justify"">Szerokie stosowanie i nie zawsze precyzyjne uregulowania prawne dotyczące produkcji i dystrybucji pasz pociągają za sobą możliwość powstawania&nbsp; pozostałości kokcydiostatyk&oacute;w w tkankach i jajach, co w przypadku gatunk&oacute;w dostarczających żywności może stanowić zagrożenie dla konsument&oacute;w. </p>
<p align=""justify"">Będące obecnie w użyciu kokcydiostatyki można podzielić na dwie grupy: powszechnie stosowane w hodowli, zwłaszcza drobiu, antybiotyki jonoforowe (lazalocyd, maduramycyna, monenzyna, narazyna, salinomycyna, semduramycyna) i znacznie rzadziej używane kokcydiostatyki chemiczne (amprolium, diklazuril, halofuginon, klopidol, klazuril, nikarbazyna, robenidyna, toltrazuril).&nbsp; </p>
<p align=""justify"">W większości przypadk&oacute;w przyczyną pozostałości kokcydiostatyk&oacute;w w żywności zwierzęcego pochodzenia są błędy człowieka popełnione najczęściej podczas produkcji premiks&oacute;w oraz pasz, rzadziej przy podawaniu lek&oacute;w (świadome lub przypadkowe przekroczenie dawki, niedotrzymanie okresu karencji czy podanie leku innemu niż przewidywany gatunkowi zwierząt). Inną przyczyną pozostałości mogą być indywidualne uwarunkowania biologiczne&nbsp;dotyczące zmienionego wchłaniania, metabolizmu, dystrybucji czy wydalania substancji aktywnych lub&nbsp;interakcje kokcydiostatyk&oacute;w z innymi lekami. Przy stosowaniu niekt&oacute;rych sposob&oacute;w hodowli zwierząt może r&oacute;wnież wystąpić zjawisko wt&oacute;rnego krążenia substancji macierzystych i ich aktywnych metabolit&oacute;w (recycling). </p>
<p align=""justify"">R&oacute;żnice we właściwościach farmakokinetycznych sprawiają, że pomiędzy poszczeg&oacute;lnymi kokcydiostatykami występują znaczące r&oacute;żnice w przyswajaniu, dystrybucji i metabolizmie w organizmie zwierząt. R&oacute;żnice te powodują r&oacute;żny stopień kumulacji związku w organizmie zwierzęcia i r&oacute;żny poziom pozostałości.&nbsp;Spośr&oacute;d antybiotyk&oacute;w jonoforowych najbardziej kumuluje się lazalocyd, a z kokcydiostatyk&oacute;w chemicznych -&nbsp;nikarbazyna i to one stanowią największy problem toksykologiczny. </p>
<p align=""justify"">Wszystkie kraje Unii Europejskiej mają, zgodnie z dyrektywą 96/23/EC, obowiązek kontrolowania pozostałości chemicznych w żywności pochodzenia zwierzęcego. Kokcydiostatyki, zgodnie z tą dyrektywą, należą do grupy B2b jako substancje dozwolone do stosowania u zwierząt.&nbsp;W szeregu kraj&oacute;w Unii Europejskiej, w tym w Polsce, stwierdzano w przeciągu ostatnich lat kokcydiostatyki w pewnym odsetku pr&oacute;bek jaj i tkanek zwierz...",07/01/31,23,Badania nad pozostałościami kokcydiostatyków w tkankach zwierząt i jajach,"kokcydiostatyki, pozostałości, tkanki, jaja, metoda oznaczania, metoda potwierdzająca",173
149,4553,opisProjektuStanWiedzy,"<p align=""justify""><font size=""3"">Kom&oacute;rki glejowe, kt&oacute;re w obwodowym układzie nerwowym nazywane są lemocytami lub kom&oacute;rkami Schwanna komunikują się między sobą oraz neuronami za pomocą szeregu cząstek wydzielanych zar&oacute;wno przez neurony jak i same kom&oacute;rki glejowe. Kom&oacute;rki glejowe w zależności od rodzaju oraz lokalizacji (ośrodkowy, bądź obwodowy układ nerwowy) posiadają m.in. receptory dla neuroprzekaźnik&oacute;w, czynnik&oacute;w wzrostu, hormon&oacute;w oraz szeregu r&oacute;żnych kanał&oacute;w jonowych [5,14,15]. Okazuje się, że <strong>uniwersalnym systemem komunikacji między kom&oacute;rkami glejowymi i nerwowymi jest system oparty o ATP i adenozynę</strong> (sygnalizacja purynergiczna). W układzie nerwowym ATP jest neuroprzekaźnikiem wydzielanym przez zakończenia nerwowe. Jednakże inaczej niż inne neurotransmitery nie podlega on wychwytowi z przestrzeni postsynaptycznej, lecz jest metabolizowany przez ektoenzymy do adenozyny oraz inozyny. Sw&oacute;j <strong>biologiczny efekt ATP oraz produkty jego metabolizmu tj. ADP i adenozyna wywierają poprzez związanie ze swoistymi receptorami zlokalizowanymi na kom&oacute;rkach glejowych oraz neuronach.</strong> Jak dotąd zidentyfikowano i scharakteryzowano 19 r&oacute;żnych receptor&oacute;w purynergicznych [10]. Wszystkie te receptory należą do dw&oacute;ch rodzaj&oacute;w tj. receptory typu P1 i P2. Receptory P1 są receptorami dla adenozyny, natomiast ligandami receptor&oacute;w P2 są ATP i ADP. </font></p>
<p align=""justify""><font size=""3""><strong>Rodzina receptor&oacute;w adenozynowych</strong> obejmuje cztery typy receptor&oacute;w tj. <strong>A1, A2A, A2B, i A3</strong>, z kt&oacute;rych każdy jest sprzężony ze specyficznym dla danego receptora białkiem G [16]. W zależności od typu receptora jego aktywacja prowadzi do stymulacji bądź zahamowania cyklazy adenylanowej, a ponadto w zależności od typu kom&oacute;rki r&oacute;wnież inne białka efektorowe mogą łączyć dany receptor adenozynowy (AR) z innymi szlakami przekaźnictwa kom&oacute;rkowego [16]. Zebrane dotąd dane wskazują, że AR mogą także oddziaływać z innymi receptorami w kom&oacute;rce zmieniając ich aktywność i odwrotnie, inne receptory mogą modulować aktywność AR. Wiele danych wskazuje, że takie wzajemne oddziaływania między r&oacute;żnego rodzaju receptorami stanowią raczej regułę niż wyjątek [17]. <strong>Ekspresję wszystkich czterech typ&oacute;w AR wykazano</strong> zar&oacute;wno w <strong>ośrodkowym (CNS)</strong> jak i <strong>obwodowym układzie nerwowym (PSN)</strong> [18].</font></p>
<p align=""justify""><font size=""3""><strong>Receptory adenozynowe A1 (AR-A1) zlokalizowane wzdłuż akson&oacute;w oraz na błonach post- i presynaptycznych uczestniczą w regulacji uwalniania neuroprzekaźnik&oacute;w </strong>(przeważnie hamowanie). Mediują one uspokajające, antykonwulsacyjne i zmniejszające ruchliwość działanie adenozyny. W sercu <strong>receptory A1 mediują chronotropowe, dromotropowe i inotropowe działanie adenozyny na mięsień sercowy</strong>. Badania przeprowadzone na myszach z &ldquo;wybitym&rdquo; genem dla receptora A1 potwierdziły dane uzyskane z doświadczeń farmakologicznych. Myszy <strong>pozbawione receptora A1 charakteryzują się zwiększoną lękliwością oraz nadwrażliwością na bodźce (b&oacute;l, czucie temperatury)</strong>. U zwierząt tych zniesiony jest r&oacute;wnież przeciwb&oacute;lowy efekt adenozyny podanej dooponowo [19]. Ponadto, w por&oacute;wnaniu do zwierząt normalnych u myszy pozbawionych funkcjonalnego receptora A1 hipoksja tylko w ograniczonym zakresie zmniejsza aktywność nerwową, czemu towarzyszy r&oacute;wnież słabszy powr&oacute;t do normalnej aktywności. Zebrane dotąd fakty doświadczalne wskazują, że jakkolwiek receptor A1 nie odgrywa decydującej roli w normalnym funkcjonowaniu tkanki nerwowej, to <strong>w warunkach patologicznych znaczenie protekcyjne AR-A1 jest istotne</strong>. </font></p>
<p align=""justify""><font size=""3""><strong>Recept...",07/01/10,23,Wpływ insuliny i glukozy na przemiany i działanie adenozyny w komórkach glejowych obwodowego układu nerwowego.,"cukrzyca, neuropatia cukrzycowa, komórki Schwanna, insulina, glukoza, adenozyna, receptory adenozynowe, transportery nukleozydowe, forskolina, cAMP, proliferacja, różnicowanie, mielina, glejowy czynnik wzrostu, IGF, białko P0, erbB2, białko S100ß, sygnalizacja komórkowa, siRNA",173
150,4555,opisProjektuStanWiedzy,"&nbsp;
<div align=""justify"">Na podstawie analizy opublikowanych dotychczas prac można stwierdzić, że w ostatnich latach wzrasta zainteresowanie zastosowaniem automat&oacute;w kom&oacute;rkowych do modelowania proces&oacute;w przer&oacute;bki plastycznej. Automaty kom&oacute;rkowe pozwalają modelować procesy zachodzące w materiałach zar&oacute;wno podczas odkształcenia, jak i po odkształceniu, podczas przemian fazowych. Za pomocą modeli opartych na automatach kom&oacute;rkowych opisywano takie zjawiska, jak krystalizacja [1, 2], rozrost ziarna [3, 4], rekrystalizacja statyczna [4, 5] i dynamiczna [6, 7], przemiany fazowe, procesy pękania, rozw&oacute;j pasm ścinania. Automaty kom&oacute;rkowe pozwalają na uzyskanie nie tylko jakościowych, ale r&oacute;wnież i ilościowych wynik&oacute;w. Jednak wyżej wymienione prace [3-7] rozpatrują i modelują te zjawiska oddzielnie i dotychczas nieznane są prace łączące w jedynym modelu zjawiska odkształcenia z rozdrobieniem ziaren i ich wpływem zar&oacute;wno na własności mechaniczne, jak i na przebieg procesu odkształcenia. Nie uwzględniają one r&oacute;wnież zmiany kształtu ziaren podczas odkszatłcenia. Bezpośrednie uwzględnienie własności materiału, a mianowicie gęstości dyslokacji i naprężenia uplastyczniającego prowadzono tylko podczas modelowania rekrystalizacji dynamicznej [6, 7]. Zastosowanie automat&oacute;w kom&oacute;rkowych do modelowania zjawisk zachodzących podczas odkształcania w niższych temperaturach nie jest szeroko rozpowszechnione. Niekt&oacute;rymi pracami w tym kirunku są zastosowanie automat&oacute;w kom&oacute;rkowych do modelowania propagacji pęknięć [8] i rozwoju mikropasm i pasm ścinania. Madej i in. [9] zastosowali klasyczne automaty, natomiast Lasko, Deryugin i Schmauder [10, 11] opracowali nową metodę relaksacyjnych element&oacute;w (Relaxation Element Method), kt&oacute;ra w połączeniu z automatami kom&oacute;rkowymi pozwala na dokładne przewidywanie naprężenia uplastyczniającego. Gł&oacute;wnym mechanizmem odkształcenia w tym podejściu jest poślizg i lokalizacja odkształceń. Metoda ta opracowana jest w wersji dwuwymiarowej z zastosowaniem heksagonalnych kom&oacute;rek. Gł&oacute;wną zaletą tej metody są małe nakłady obliczeniowe, a co za tym idzie kr&oacute;tki czas obliczeń. Jednak, poza dość dokładną symulacją efektu powstania mikropasm ścinania i przewidywania naprężenia uplastycznającego, zastosowane podejście nie pozwala przewidywać zmian zar&oacute;wno mikrostruktury, jak i rozwoju struktur dyslokacyjnych.&nbsp;Te ostatnie odgrywają szczeg&oacute;lnie istotną rolę przy dyżych odkształceniach i powstaniu struktury drobnoziarniastej. W takich automatach powstają dodatkowe problemy związane z kształtem kom&oacute;rki i wybranego otoczenia. Poza tym obszar zastosowań dwuwymiarowych automat&oacute;w kom&oacute;rkowych jest bardzo ograniczony [12].&nbsp;Mogą one być zastosowane wylącznie dla szczeg&oacute;lnych przypadk&oacute;w odkształcenia cienkich folii w niskich temperaturach.</div>
<div align=""justify"">Gęstość dyslokacji oraz&nbsp;struktury dyslokacyjne są istotnymi elementami, kt&oacute;re są wykorzystywane do modelowania rozwoju mikrostruktury za pomocą automat&oacute;w kom&oacute;rkowych. Niemniej jednak większość prac zakłada zmiany gęstości dyslokacji jako ustałoną zewnątrz procedurę, kt&oacute;ra opisuje ten rozw&oacute;j według teorii dyslokacji Taylora lub Orowana. Gł&oacute;wnie wykorzystuje się gęstość dyslokacji jako zmienną do obliczania naprężenia uplastyczniającego lub zapoczątkowania rekrystalizacji w wyższych temperaturach. Obecnie bezpośrednie modelowanie za pomocą automat&oacute;w kom&oacute;rkowych zmian wartości naprężenia uplastyczniającego jest ograniczone procesem odkształcenia z rekrystalizacją dynamiczną [13]. Natomiast stosowane powszechnie podejście polegające na obliczaniu naprężenia uplastyczniającego poprzez średną wartość gęstości dyslokacji w całej objętości automat&oacute;w kom&oacute;rkowych jest mocno dyskusyjne, o czy...",07/01/25,23,Opracowanie modelu oraz symulacja komputerowa rozwoju struktury w procesach uzyskiwania materiałów drobnoziarnistych,"mikrostruktura, struktura dyslokacyjna, automaty komórkowe, metoda elementów skończonych, własności materiałów, materiały drobnoziarniste",183
151,4561,opisProjektuStanWiedzy,"&nbsp;
<div align=""justify"">Technika miar niezwartości jest stosowana w teorii r&oacute;wnań r&oacute;żniczkowych i całkowych od ponad 30 lat. Zastosowania te są jednak sporadyczne i mają jedynie na celu dow&oacute;d pewnych twierdzeń egzystencjalnych, w założeniu kt&oacute;rych pojawia się warunek wypowiedziany przy pomocy jednej z klasycznych miar niezwartości (Kuratowskiego lub Hausdorffa). Typowymi przykładami takich twierdzeń są twierdzenia egzystencjalne dla r&oacute;wnań r&oacute;żniczkowych lub całkowych w przestrzeniach Banacha, kt&oacute;re były formułowane i dowodzone w latach 70-tych, 80-tych i 90-tych ubiegłego stulecia. Praktyczne zastosowanie tego typu twierdzeń jest jednak niewielkie, bowiem dla konkretnych r&oacute;wnań trudno jest warunek taki zweryfikować. Ponadto, twierdzenia egzystencjalne tego typu nie wnoszą żadnych informacji o własnościach rozwiązań, kt&oacute;rych istnienia się dowodzi, ani też o strukturze zbioru tych rozwiązań.</div>
<div align=""justify"">Począwszy od lat 90-tych ubiegłego stulecia zaczęto stosować taką technikę miar niezwartości do r&oacute;wnań r&oacute;żniczkowych, całkowych oraz funkcyjno-całkowych, kt&oacute;ra pozwalała uzyskiwać poręczne warunki gwarantujące istnienie rozwiązań rozważanych r&oacute;wnań. Warunki te są dość łatwe do zweryfikowania w konkretnych sytuacjach. Z drugiej strony twierdzenia egzystencjalne dowodzone przy pomocy wspomnianej techniki pozwalają znajdywać rozwiązania o r&oacute;żnych, z g&oacute;ry narzuconych własnościach (rozwiązania dodatnie, nieujemne, ograniczone, monotoniczne, znikające w nieskończoności, asymptotycznie stabilne itp.).</div>
<div align=""justify"">Pierwsze prace, kt&oacute;re zawierały realizację opisanej wyżej idei (z zastosowaniem odpowiedniej techniki miar niezwartości) były opublikowane przez kierownika niniejszego projektu (J. Banaś) we wsp&oacute;łpracy z innymi matematykami zajmującymi się teorią r&oacute;wnań r&oacute;żniczkowych i całkowych oraz miarami niezwartości (B. Rzepka, M. Lecko, K. Sadarangani, W.G. El-Sayed, J. Caballero). Idee te były r&oacute;wnież wykorzystane i kontynuowane przez innych matematyk&oacute;w (D. O&rsquo;Reagan, T.A. Burton, Bo Zhang, R.P. Agarwal, A.M.A. El-Sayed, H.A.H. Salem, M.A. Darwish, B.C. Dhage, N. Sherif, I.A. Ibrahim, S. Szufla, A. Szukała, I. Kubiaczyk, M. Cichoń, Z. Liu, S.M. Kang, M. V&auml;th i inni).</div>
<div align=""justify"">Oczywiście opisane tutaj wysiłki stanowią zaledwie wstępną fazę realizacji możliwych zastosowań techniki miar niezwartości w teorii r&oacute;wnań r&oacute;żniczkowych i całkowych i układ&oacute;w tych r&oacute;wnań. Technika ta wymaga dalszego rozwoju i wykorzystywania dużych możliwości, jakie tkwią w teorii miar niezwartości.</div>",07/01/18,23,"Miary niezwartości w badaniu rozwiązań nieliniowych równań różniczkowych, różnicowych i całkowych","Miara niezwartości, funkcyjna przestrzeń Banacha, ciągowa przestrzeń Banacha, równanie całkowe typu Hammersteina, równanie całkowe typu Urysohna, równanie całkowe typu Stieltjesa, kwadratowe równanie całkowe, nieliniowe równanie różniczkowe, równanie różniczkowe i równanie całkowe rzędu ułamkowego, nieliniowe równanie różnicowe, nieskończony układ równań różniczkowych, całkowych lub różnicowych, rozwiązanie ograniczone, rozwiązanie dodatnie, rozwiązanie nieujemne, rozwiązanie monotoniczne, rozwiązanie znikające w nieskończoności, rozwiązanie asymptotycznie stabilne.",173
152,4563,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""text-indent: 35.45pt; line-height: 150%; text-align: justify; font-family: Times New Roman;""><font size=""4"">Zadanie, którego celem jest opracowanie algorytmów oraz określenie wytycznych, które będą stanowić podstawę do budowy systemu wspomagającego proces rehabilitacji, jest ważnym problemem, który rozpatrywany jest zarówno przez informatyków, cybernetyków, jak również biomechaników oraz ludzi związanych z medycyną. W ośrodkach medycznych zajmujących się rehabilitacją osób cierpiących na spastyczność podkreśla się brak metod, które pozwalałyby na ilościową ocenę stanu zdrowia pacjenta. Dotychczas stosowane metody nie pozwalają na obiektywna ocenę aktualnego stanu zdrowia pacjenta. Diagnoza oparta na obiektywnych pomiarach oraz zautomatyzowane wspomaganie nadzoru nad przebiegiem rehabilitacji pozwoli na znaczne zwiększenie efektywności przeprowadzanego procesu. Dodatkową zaletą systemu jest możliwości dostosowania się do specyfiki schorzenia pacjenta.</font></p>
<p class=""MsoNormal"" style=""text-indent: 35.45pt; line-height: 150%; text-align: justify; font-family: Times New Roman;""><font size=""4"">W ramach realizacji tematu opracowano problem zastosowania identyfikacji systemów złożonych w wielopoziomowym systemie sterowania [1]. Istota problemu polega na podejmowaniu decyzji w oparciu o model matematyczny uzyskany na pierwszym stopniu. Zaproponowano pewien, nie rozpatrywany dotąd, schemat identyfikacji dwustopniowej (Rys. 1), oraz opracowano podstawy do jego praktycznego zastosowania w problemie biomedycznym polegającym na leczeniu spastyczności.</font></p>
<font size=""4"" style=""font-family: Times New Roman;""><br /></font>
<div style=""text-align: center; font-family: Times New Roman;""><font size=""4""><img align=""middle"" alt="""" style=""width: 475px; height: 342px;"" src=""/OSFImageLoader.do?idImageDB=3834"" /><br /></font>
<div style=""text-align: center;"">
<div style=""text-align: center;"">  </div>
<p class=""Legenda1"" style=""margin: 0cm 0cm 17pt; text-indent: 35.45pt; line-height: 150%; text-align: center;""><font size=""4"">Rys. 1 <span style=""font-weight: normal;"">Schemat systemu sterowania adaptacyjnego z wykorzystaniem identyfikacji dwustopniowej<o:p></o:p></span></font></p>
</div>
<div style=""text-align: left;"">
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 35.45pt; line-height: 150%;""><font size=""4"">Sterowanie polega na realizacji odpowiedniego dla aktualnego stanu zdrowia pacjenta scenariusza ćwiczeń rehabilitacyjnych [2]. O wyborze jednego z dostępnych scenariuszy decyduje wynik identyfikacji na pierwszym stopniu niemierzalnych bezpośrednio parametrów nieliniowego dynamicznego modelu układu neuro-mięśniowego. </font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 35.45pt; line-height: 150%;""><font size=""4"">Wybór właściwego harmonogramu działań jest wspomagany przez algorytmy oparte na metodach rozpoznawania obrazów. Proponowane podejście, w którym do sterowania procesem wykorzystuje się algorytmy klasyfikacji jest nowe i wymaga przebadania. Proponowane podejście wiąże się z rozwiązaniem szeregu problemów, które występują podczas sterowania procesów nieliniowych. </font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 35.45pt; line-height: 150%;""><font size=""4"">Algorytm identyfikacji nieliniowego dynamicznego obiektu na pierwszym stopniu został skonstruowany oraz przedstawiony w pracach [2] oraz [3]. Uzyskane rezultaty badań i obliczeń potwierdziły przydatność zaproponowanego podejścia oraz wskazały przyszłe kierunki rozwoju tematu. Dotyczą one w szczególności zadania rozpoznawania na drugim stopniu, które powinno poprzedzać etap sterowania obiektem, polegającego na realizacji wybranego scenariusza, zależnego z kolei od wyników identyfikacji na stopniu pierwszym.</font></p>
<p class=""MsoNormal"" style=""text-align: justify; text-indent: 35.45pt; line-height: 150%;""><font size=""4"">Do opracowania pozostał algorytm sterowania oparty na rozpoznaw...",07/01/26,23,Algorytm sterowania adaptacyjnego z wykorzystaniem identyfikacji dwustopniowej,"identyfikacja, systemy złożone, sterowanie wielopoziomowe, identyfikacja wielostopniowa",173
153,4567,opisProjektuStanWiedzy,"<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: Arial""><font size=""3"">&nbsp;&nbsp; Uprzednio wykazaliśmy, że narażenie w okresie prenatalnym manganem powoduje zmiany reaktywności ośrodkowych receptor&oacute;w dopaminowych, obniżenie intensywności syntezy dopaminy oraz zmniejszenie uwalniania dopaminy w prążkowiu dorosłych szczur&oacute;w (<strong>6,7</strong>). Ponadto należy dodać, że młode i rozwijające się organizmy są bardziej wrażliwe na neurotoksyczne działanie manganu (<strong>8</strong>).<o:p></o:p></font></span></p>
<p class=""MsoHeader"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify; tab-stops: 35.4pt""><font size=""3""><span style=""FONT-FAMILY: Arial""><span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;</span></span><span style=""FONT-SIZE: 12pt; FONT-FAMILY: Arial"">W Katedrze Farmakologii w Zabrzu od lat posługujemy się własnym zwierzęcym (szczurzym) modelem choroby Parkinsona, polegającym na uszkodzeniu ośrodkowego układu dopaminergicznego podaniem w okresie noworodkowym dom&oacute;zgowo neurotoksyny &ndash; 6-hydroxydopaminy (6-OHDA) <strong>(9-11).</strong> 6-OHDA podana szczurom 3-go dnia życia do bocznych kom&oacute;r m&oacute;zgu (ICV) w dawce 134 </span><span style=""FONT-SIZE: 12pt; FONT-FAMILY: Arial; mso-ascii-font-family: Arial; mso-hansi-font-family: Arial; mso-bidi-font-family: Arial; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">m</span></span><span style=""FONT-SIZE: 12pt; FONT-FAMILY: Arial"">g (po 67 </span><span style=""FONT-SIZE: 12pt; FONT-FAMILY: Arial; mso-ascii-font-family: Arial; mso-hansi-font-family: Arial; mso-bidi-font-family: Arial; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">m</span></span><span style=""FONT-SIZE: 12pt; FONT-FAMILY: Arial"">g do każdej bocznej komory) powoduje trwałe (na całe życie zwierzęcia) uszkodzenie ośrodkowego układu dopaminergicznego. Towarzyszą temu u dorosłych szczur&oacute;w zmiany w zachowaniu indukowane podaniem agonist&oacute;w i antagonist&oacute;w ośrodkowych receptor&oacute;w dopaminowych imitujące niekt&oacute;re objawy charakterystyczne dla choroby Parkinsona u ludzi, wynikające z braku dopaminy oraz ze zmian reaktywności ośrodkowych receptor&oacute;w dopaminowych (między innymi wzrost aktywności ruchowej, częstości ruch&oacute;w pyska, drażliwości, zmiany nasielnia katalepsji itp.). Opracowany spos&oacute;b uszkodzenia ośrodkowego układu dopaminergicznego uznano za zwierzęcy model choroby Parkinsona i jest używany do oceny metod leczenia powyższego schorzenia (<strong>10</strong>). Należy dodać, że głębokość uszkodzenia ośrodkowego układu dopaminergicznego u szczur&oacute;w zależy od wielkości dawki 6-OHDA podanej noworodkom (<strong style=""mso-bidi-font-weight: normal"">12</strong>). Uszkodzeniu układu dopaminergicznego towarzyszy kompensacyjny wzrost aktywności ośrodkowego układu serotoninergicznego (<strong style=""mso-bidi-font-weight: normal"">11</strong>).<o:p></o:p></span></font></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: Arial""><font size=""3""><span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Dopamina (DA) jest neuroprzekaźnikiem w układzie dopaminergicznym (OUN), uczestnicząc w szeregu procesach fizjologicznych zachodzących w organizmach jak kontroli aktywności ruchowej i poznawczej, nauce i zapamiętywaniu, procesach emocjonalnych i motywacyjnych, wydzielaniu hormon&oacute;w podwzg&oacute;rzowych, czynności ośrodka sercowo-naczyniowego i innych.<span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><o:p></o:p></font></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-ALIGN: justify""><span style=""FONT-FAMILY: Arial""><font size=""3""><span style=""mso-tab-count: 1"">&nbsp;&nbsp;&nbsp;&nbsp;&nb...",06/12/21,23,Molekularne mechanizmy neurotoksycznego działania manganu na ośrodkowy układ dopaminergiczny u szczurów ze zwierzęcym modelem choroby Parkinsona,"Szczury, mangan, 6-hydroksydopamina, ośrodkowy układ dopaminergiczny, aminy biogenne, rodniki wysokoenergetyczne, midrodializa mózgu, (3H)glukoza, behawior",183
154,4573,opisProjektuStanWiedzy,"<font size=""3"">Literatura dotycząca płynów o znaczeniu biologicznym jest ogromna i z konieczności oganiczymy się do zwięzłego podsumowania faktów, hipotez i otwartych pytań związanych bezpośrednio z niniejszym projektem. Od dawna anomalne własności wody wiązane są z tworzeniem sieci wiązań wodorowych. Coraz więcej danych z symulacji komputerowych [Brovchenko <span style=""font-style: italic;"">et al</span>.  (2005)] i eksperymentów [Kumar </font><font size=""3""> <span style=""font-style: italic;"">et al</span>.</font><font size=""3"">(2006)] potwierdza hipotezę wyjaśniającą anomalne własności wody w oparciu o metastabilny punkt krytyczny (T<sub>C</sub>=200K, P<sub>C</sub>=250 MPa), związany z metastabilnym przejściem fazowym między dwiema amorficznymi  postaciami  czystej wody, różniącymi się gęstością [Poole </font><font size=""3""> <span style=""font-style: italic;"">et al</span>.</font><font size=""3""> (1992)]. W fazie o niższej gęstości większy procent cząsteczek tworzy strukturę tetraedryczną.  Owe dwie fazy mają zapewne postać dwóch różnych rodzajów szkła. </font><font size=""3"">Ostatnio</font><font size=""3""> </font><font size=""3"">zaobserwowano  </font><font size=""3"">współistnienie dwóch faz amorficznych dla kilku jednoskładnikowych substancji [Chatterjee i Debenedetti (2006)] i efekt ten nazwano poliamorficznością.  <br /> Z wielu opublikowanych w ostatnich latach prac dotyczących poliamorficzności wody na uwagę zasługuje obserwacja, iż  zmiana konformacji uwodnionych makromolekuł  może wynikać z przejścia między dwiema fazami  wody uwięzionej w obrębie makromolekuły [</font><font size=""3"">Kumar </font><font size=""3""> <span style=""font-style: italic;"">et al</span>.</font><font size=""3"">(2006)</font><font size=""3"">]. Fazy metastabilne w układzie makroskopowym mogą bowiem stać się stabline  w  ograniczonej geometrii.  Powyższa obserwacja jest kolejnym potwierdzeniem hipotezy istnienia przejścia między dwiema ciekłymi fazami czystej wody. Na marginesie warto wspomnieć, że procesy biologiczne zwykle przebiegają w ograniczonej geometrii - w organellach lub w obrębie membran biologicznych. Wyniki otrzymane m.in. w ramach modelu CHS dla mieszanin wody, węglowodoru i surfaktanta [Ciach </font><font size=""3""> <span style=""font-style: italic;"">et al</span>. </font><font size=""3"">(1988)] wskazują na istotny wpływ rozmiarów i kształtu ścian ograniczających na własności mechaniczne i termodynamiczne płynów niejednorodnych [Tasinkevych i Ciach (2005)]; w szczególności zaobserwowano, że faza która w objętości jest tylko metastabilna, może stać się stabilna w szczelinie lub kapilarze.</font><font size=""3""> Warto też wspomnieć, że obecność membran ma istotny wpływ na dyfuzję cząstek biologicznych, zarówno w obrębie membrany, jak w obszarach między membranami.<br />Stosunkowo prosty model mezoskopowy czystej wody, przewidujący anomalne własności poniżej pewnego ciśnienia, zaproponowany został przez H. Tanaka [Tanaka (1998)]. W modelu tym występują dwa sprzężone parametry uporządkowania - gęstość i liczba wiązań wodorowych na jednostkę objętości. Model ten jest jakościowy i nie został dotąd rozszerzony na roztwory wodne. Z kolei rozwijane  do tej pory modele mezoskopowe dla mieszanin prawidłowo przewidują wiele własności. Na przykład model CHS poprawnie opisuje najważniejsze cechy mieszanin wody, węglowodoru i surfaktantów, prowadzące do powstania mikroemulsji i liotropowych faz typu ciekłokrystalicznego. Jednakże w żadnym z dotychczasowych modeli dla mieszanin nie jest uwzględniane istnienie sieci wiązań wodorowych, ani tym bardziej wpływ istnienia innych cząsteczek na tę sieć. Tymczasem w ostatnich latach zaobserwowano intrygujące i dotychczas niewyjaśnione własności roztworów wodnych cząsteczek polarnych [Kostko </font><font size=""3""> <span style=""font-style: italic;"">et al</span>.</font><font size=""3"">(2004)</font><font size=""3""><span style=""font-style: italic;""></span></font><font size=""3"">]. Na przykład obserwowane są znacząco różne wyniki doświ...",07/01/26,23,Mezoskopowa teoria zjawisk kolektywnych w płynach o znaczeniu biologicznym,"fizyka statystyczna, płyny, zjawiska kolektywne, przemiany fazowe",173
155,4575,opisProjektuStanWiedzy,"&nbsp;&nbsp; W przeszłości Karkonosze uważane były za najlepiej poznane pod względem briologicznym pasmo g&oacute;rskie Europy, co zaowocowało m.in. podaniem do 1930r. stanowisk 38 gatunk&oacute;w wysokog&oacute;rskich (W. Limpricht 1930). Jako ostoje roślinności (sub)arktyczno-(sub)alpejskiej wskazywano kotły polodowcowe oraz zbocza i szczyt Śnieżki (m.in. Nees von Eisenbeck 1838, 1840; K.G. Limpricht 1876, 1890; Milde 1861, 1869). <br />&nbsp;&nbsp; Stan poznania wsp&oacute;łczesnych stosunk&oacute;w briologicznych masywu, szczeg&oacute;lnie po polskiej stronie granicy, jest słaby, co wynika z braku systematycznych badań nad florą mszak&oacute;w w okresie powojennym (Wilczyńska 1996). Dopiero w latach 2000-2004 podjęto poszukiwania briologiczne w szczytowej części Karkonoszy, skoncentrowane wok&oacute;ł kotł&oacute;w polodowcowych w zachodniej ich części (Fudali 2001, 2003, 2004; Fudali, Kučera 2002, 2003) oraz torfowisk subalpejskich położonych na R&oacute;wni pod Śnieżką (Wojtuń 2003, 2006). W ich wyniku stwierdzono występowanie w polskiej części masywu 27 gatunk&oacute;w z grupy wysokog&oacute;rskich, w tym czterech wcześniej nie podawanych. Warto zaznaczyć, że dwa z nich: <em>Andreaea nivalis</em> (Fudali, Kučera 2003) i <em>Sphagnum lindbergii</em> (Wojtuń 2003) to gatunki po raz pierwszy znalezione w całych Karkonoszach. Rewizja materiał&oacute;w zielnikowych czeskiego briologa &Scaron;mardy z Karkonoszy zgromadzonych w Zielniku Uniwersytetu Karola w Pradze wykazała, że w 1949r. w obrębie Małego Śnieżnego Kotła zebrał on okazy subarktyczno-subalpejskiego mchu <em>Encalypta microstoma</em>, kt&oacute;ry okazał się nowym taksonem dla brioflory Polski (Kučera, Fudali 2003). Dotychczasowe badania nad bior&oacute;żnorodnością mch&oacute;w szczytowej części Karkonoszy polskich pokazały więc, że stare dane nie były kompletne w odniesieniu do gatunk&oacute;w (sub)arktyczno-(sub)alpejskich. Wykazały też, że w zachodnich Karkonoszach nie udało się odnaleźć wielu wcześniej notowanych gatunk&oacute;w wysokog&oacute;rskich. <br />&nbsp;&nbsp; W tym samym okresie w kotłach polodowcowych po czeskiej stronie masywu r&oacute;wnież prowadzone były intensywne badania briologiczne (Kučera, Buryov&aacute; 2001; Kučera i in. 2003a-c). Także czescy badacze wykazali, że brioflora Karkonoszy zachowując w dużym stopniu swoją naturalność i specyfikę, jest uboższa o wiele gatunk&oacute;w subarktyczno-subalpejskich w stosunku do danych historycznych. Wskazali oni ocieplenie klimatu jako przypuszczalną przyczynę tych zmian (Kučera i in. 2003b). Jednakże, niekt&oacute;re ze wskazanych&nbsp;jako nieodnalezione gatunki wysokog&oacute;rskie zostały stwierdzone po polskiej, p&oacute;łnocnej stronie; a jednocześnie po czeskiej, południowej stronie odnotowano stanowiska kilku&nbsp;gatunk&oacute;w mch&oacute;w&nbsp;nieodnalezionych po polskiej stronie w zachodnich Karkonoszach. Przeprowadzona przez masyw granica państwowa, powodując sztuczne rozdzielenie obszaru badań, może więc&nbsp;sprzyjać formułowaniu fałszywych, opartych o niekompletne dane, hipotez naukowych. <br />&nbsp;&nbsp; Już w roku 1836 Nees von Eisenbeck odnotował na szczycie Śnieżki obecność ruderalnego gatunku <em>Bryum caespiticium.</em> W 1865 roku Gustaw Limpricht zaobserwował tam inne gatunki synantropijne: <em>Ceratodon purpureus</em> i <em>Leptobryum pyriforme</em>, a w 1876 Kern zebrał okazy <em>Bryum argenteum</em> i <em>B. algovicum</em> (za Wilczyńską 1996). Kolejne dane dotyczące występowania mch&oacute;w synantropijnych w strefie szczytowej polskiej części Karkonoszy pochodzą dopiero z roku 2003. W&oacute;wczas stwierdzono występowanie <em>Ceratodon purpureus</em>&nbsp;w okolicy Śnieżki oraz <em>Didymodon rigidulus</em> i <em>Encalypta streptocarpa</em>&nbsp;na przełęczy pod Śnieżką (Fudali i in. 2003). Masowy ruch turystyczny odbywa się nie tylko wok&oacute;ł Śnieżki; popularne trasy turystyczne poprowadzone są także przez Śnieżne Kotły, przez Kocioł Łomniczki oraz wok&oacute;ł...",06/12/15,23,Refugia różnorodności gatunkowej mchów wysokogórskich w Karkonoszach polskich i jej zagrożenia.,"mchy wysokogórskie, bioróżnorodność, ekologia, zagrożenie mszaków, synantropizacja brioflory, Karkonosze",173
156,4577,opisProjektuStanWiedzy,"<p class=""MsoBodyTextIndent"" style=""MARGIN: 0cm 0cm 6pt 14.15pt; TEXT-INDENT: 30.85pt; TEXT-ALIGN: justify""><font size=""3""><font face=""Times New Roman"">W świetle<span style=""mso-spacerun: yes"">&nbsp; </span>dotychczasowych badań<span style=""mso-spacerun: yes"">&nbsp; </span>białka żywności<span style=""mso-spacerun: yes"">&nbsp; </span>stanowią bogate źr&oacute;dło inhibitor&oacute;w enzymu konwertującego angiotensynę (ACE) [EC 3.4.15.1] &ndash; peptyd&oacute;w odgrywających kluczową rolę w obniżaniu ciśnienia krwi. Wiele inhibitor&oacute;w ACE zostało scharakteryzowanych zar&oacute;wno w białkowych sekwencjach pochodzenia roślinnego jak i zwierzęcego.<span style=""mso-spacerun: yes"">&nbsp; </span>Proteazy przewodu pokarmowego<span style=""mso-spacerun: yes"">&nbsp; </span>mogą metabolizować inhibitory konwertazy angiotensyny i modyfikować w ten spos&oacute;b jej aktywność [10].<span style=""mso-spacerun: yes"">&nbsp; </span></font></font></p>
<p class=""MsoBodyTextIndent"" style=""MARGIN: 0cm 0cm 6pt 14.15pt; TEXT-INDENT: 30.85pt; TEXT-ALIGN: justify; tab-stops: list 0cm""><font face=""Times New Roman"" size=""3"">Badania <em><span style=""mso-spacerun: yes"">&nbsp;</span></em><span style=""mso-spacerun: yes"">&nbsp;</span>przeprowadzone w skali laboratoryjnej<span style=""mso-spacerun: yes"">&nbsp; </span>na szczurach oraz ludziach wykazały, że podawanie trypsynowych hydrolizat&oacute;w kazeiny mleka (szczury) lub kwaśnego mleka (ludzie) zawierających peptydowe inhibitory ACE wyraźnie obniżało ich ciśnienie [<span style=""FONT-VARIANT: small-caps"">11-13</span>]. Obecność peptyd&oacute;w o aktywności przeciwnadciśnieniowej wykazano także w innych surowcach (m.in. w hydrolizatach glutenu pszenicy, zeinie kukurydzy, białkach ryżu, alfa-proteinach soi, cytochromie i hemoglobinie). Zastosowanie technik komputerowych w badaniach <span style=""mso-spacerun: yes"">&nbsp;</span>poświęconych białkom jako prekursorom bioaktywnych peptyd&oacute;w pozwoliło na znalezienie sekwencji homologicznych z sekwencjami o aktywności przeciwnadciśnieniowej. Np. Dziuba i wsp. [14] stwierdzili obecność pięciu odcink&oacute;w o potencjalnie przeciwnadciśnieniowej aktywności w gliadynach pszenicy.<span style=""mso-spacerun: yes"">&nbsp;&nbsp; </span>Były to sekwencje złożone z trzech reszt aminokwasowych LQP (alfa-,beta- i gamma-gliadyny), PYP (<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol""><font face=""Times New Roman"">alfa-,beta- i gamma-gliadyny</font></span></span>), IPP (alfa i beta-gliadyny), LPP (gamma-gliadyny) i LVL (gamma<span style=""FONT-FAMILY: Arial; mso-ascii-font-family: ’Times New Roman’; mso-hansi-font-family: ’Times New Roman’; mso-char-type: symbol; mso-symbol-font-family: Symbol""><span style=""mso-char-type: symbol; mso-symbol-font-family: Symbol"">-</span></span>gliadyny), kt&oacute;re<span style=""mso-spacerun: yes"">&nbsp; </span>mogą być uwalniane przez następujące enzymy: endopeptydazę prolinową (EC 3.4.21.26), proteinazę K (EC 3.4.21.14), proteinazę <em>Streptococcus termophilus</em> (EC 3.4.24.4) oraz enzymy trawienne: chymotrypsynę (EC 3.4.21.1), trypsynę (EC 3.4.21.4), elastazę (EC 3.4.21.36) i pepsynę (EC 3.4.23.1).</font></p>
<p class=""MsoBodyTextIndent"" style=""MARGIN: 0cm 0cm 6pt 14.15pt; TEXT-INDENT: 30.85pt; TEXT-ALIGN: justify; tab-stops: list 0cm""><font face=""Times New Roman"" size=""3"">Wiele technik i narzędzi komputerowych jest stosowanych&nbsp; w naukach&nbsp;przyrodniczych [15, 16]. Wśr&oacute;d nich znajduje się wiele baz danych białek i motyw&oacute;w strukturalnych jak Swiss-Prot, PDB, CATH, Pfam, InterPro etc., program&oacute;w służących do przewidywania struktury białek (Homology, Predict 7), określania homologii białek (BLAST) oraz narzędzi służących do modelowania proces&oacute;w hydrolizy [17]. Poza wymienionymi narzędziami przeznaczonymi do anali...",07/01/29,23,Studia nad  badaniem zależności między  strukturą a funkcją peptydów o wybranej aktywności biologicznej,"białka, bioaktywne peptydy, bazy danych, QSAR, HSS",183
157,4579,opisProjektuStanWiedzy,"<span style=""font-weight: bold;"">Dryfujące subpulsy<br /><span style=""font-weight: bold;""><span style=""font-weight: bold;""><span style=""font-weight: bold;""><span style=""font-weight: bold;""></span></span></span></span></span>Aktualnie znamy blisko 100 pulsar&oacute;w o kt&oacute;rych wiadomo, że w ich promieniowaniu występują dryfujace subpulsy. Wiedza ta pochodzi gł&oacute;wnie z analizy dwu-wymiarowego spektrum modulacji [5], w wyniku kt&oacute;rego otrzymujemy wartość okresowości modulacji oraz informacje czy jest to modulacja fazy (dryf subpuls&oacute;w) czy tez modulacja natężenia, czy też obie naraz. W wiekszości wypadk&oacute;w analiza wykazuje modulację fazy z relatywnie kr&oacute;tkim okresem <span style=""font-style: italic;"">P<sub>3</sub> </span>rzędu kilku okres&oacute;w pulsara <span style=""font-style: italic;"">P </span>[6] <span style=""font-style: italic;"">.&nbsp; </span>W nielicznych przypadkach analiza wykazuje modulację intensywności z okresami rzędu kilkanaście-kilkadiesiąt&nbsp; <span style=""font-style: italic;"">P </span>[6,15], kt&oacute;re można powiązać z okresem cyrkulacji <span style=""font-style: italic;"">P<sub>4</sub></span> iskier dookoła czapy polarnej [9]. Jeszcze rzadziej takiej identyfikacji można dokonać wprost poprzez połączoną analizę spektralną oraz symulację ścieżek dryfu w ramach modelu karuzeli [7,16-18], lub innych metod specjalnie rozwiniętych na użytek konkretnych przypadk&oacute;w [17]. Jednak identyfikacji wprost można dokonać wyłącznie u silnych pulsar&oacute;w, w kt&oacute;rych wyrażnie widać dryfujące subpulsy w sekwencji co najmniej kilkuset puls&oacute;w. Dotychczas udało się wyznaczyć okres cyrkulacji iskier&nbsp; <span style=""font-style: italic;"">P<sub>4</sub></span> u czterech pulsar&oacute;w metodą &quot;wprost&quot;: B0943+10 [9], B0834+06 [19], B0826-34 [17] oraz B0818-41 [8], oraz czterech dalszych metodą &quot;nie wprost&quot;: B1133+16 [6,13], B0656+14 [13,15], B0628-28 [6,15] oraz B0809+74 [20]. Parametry tych pulsar&oacute;w są zebrane w Tabeli 1. Mamy nadzieję, że nasz projekt przyczyni się do wzrostu liczby pulsar&oacute;w ze znaną wartością okresu <span style=""font-style: italic;"">P<sub>4</sub></span> .<br /><br /><span style=""font-weight: bold;"">Promieniowanie rentgenowskie z czapy polarnej<br /></span>Jak dotąd jedyne pewne identyfikacje termicznego promieniowania z czapy polarnej uzyskano dla trzech pulsar&oacute;w z grupy tzw. Trzech Muszkieter&oacute;w [10]. Z uwagi na duże ilości zliczanych foton&oacute;w w widmach tych obiekt&oacute;w można wyraźnie i ponad wszelką watlipość wyr&oacute;żnić&nbsp; składowe termiczne odpowiadajace temperaturom (2-3) mln K, emitowane z niewielkiej powierzchni, zwykle znacznie mniejszej niż powierzchnia kanonicznej czapy polarnej. Uważa się [24], że jest to wynikiem istnienia silnego, niedipolowego magnetycznego pola powierzchniowego, o natężeniach przekraczajacych znacznie 10<sup>13</sup> G. Tak silne pola nie tylko ograniczają powierzchnię czapy polarnej (wskutek zachowania strumienia pola magnetycznej przenikającego cylinder prędkosci światła), ale r&oacute;wnież gwarantują wystarczająco dużą energię wiązania, aby możliwe było utworzenie przynajmniej częściowo ekranowanej przerwy polarnej [1, 11], niezbędnej dla istnienia wyładowań iskrowych. U jednego z tych pulsar&oacute;w (B0656+14) udało się znaleźć wartość okresu karuzeli <span style=""font-style: italic;"">P<sub>4</sub></span>&nbsp; (Tabela 1 i Rys. 2).<span style=""font-style: italic;""> <br /><br /><span style=""font-style: italic;""><span style=""font-style: italic;""></span></span></span>U kilku dalszych pulsar&oacute;w z wyznaczoną wartością <span style=""font-style: italic;"">P<sub>4</sub></span>&nbsp; stwierdzono przynajmniej niesprzeczność widma z alternatywnym modelem promieniowania termicznego z czapy polarnej. Do tej grupy należą B0943+10 [21], B1133+16 [22] oraz B0628+14 [23]. Chociaż&nbsp; modelu magnetosferycznego nie da się wykluczyć, to przyjmniej dla dw&oacute;ch ost...",07/01/30,23,Radiowe i rentgenowskie promieniowanie związane z obszarem wysokiego napięcia nad czapą polarną pulsarów,"pulsary, promieniowanie radiowe, promieniowanie rentgenowskie",183
158,4581,opisProjektuStanWiedzy,"Mimo, ze temat depresji wystęującej w przebiegu choroby niedokrwiennej serca istnieje w literaturze naukowej od ponad trzydziestu lat, nadal poświęca się mu wiele uwagi. Świadczy to o znaczeniu, a także złożoności tego problemu. Wiadomo, że obecność depresji po zawale mięśnia sercowego wpływa zdecydowanie negatywnie na przebieg i rokowanie w chorobie niedokrwiennej serca. Jak dotąd nie ustalono sposobu prognozowania pojawienia się depresji u tych chorych. Zakładana w pracy specyficzność odpowiedzi psychologicznej i biologicznej na stresor, jakim jest zawał m. sercowego, mogłaby być takim czynnikiem prognostycznym. W piśmiennictwie nie ma prac, które dotyczyłyby tego wybranego problemu.",07/01/08,23,Związek miedzy nasileniem i rodzajem reakcji stresowej w bezpośrednim następstwie zawału mięśnia sercowego a późniejszym pojawieniem się zespołu depresyjnego,"stres, depresja, zawał mięśnia sercowego",183
159,4585,opisProjektuStanWiedzy,"<p align=""justify""><font size=""4"">Projekt jest &nbsp;nowatorski i w pełni oryginalny. W Polsce dotychczas nie było&nbsp; badań w zakresie aeroponicznej uprawy roślin. Nie znane było pojęcie &quot;uprawa aeroponiczna&quot;. Pierwsze publikacje autora projektu ukazały się w latach 2003 i 2004 (w załączeniu). Były r&oacute;wnież szeroko komentowane w mediach (radio, telewizja, prasa). Aeroponiczna uprawa&nbsp; stwarza nowe&nbsp;możliwość prowadzenia badań podstawowych na systemach korzeniowych, ze względu na łatwość obserwacji i pobierania materiału badawczego do analiz, ma duże znaczenie dydaktyczne oraz&nbsp; uzasadnienie praktyczne. Podważa dotychczasowe poglądy w zakresie niezbędności gleby lub podłoża do uprawy roślin. Wskazuje na możliwość efektywnej uprawy roślin bez jakiegokolwiek podłoża. Podkreśla znaczenie właściwości fizycznych środowiska korzeniowego dla prawidłowego rozwoju system&oacute;w korzeniowych roślin i&nbsp; wysokiego plonowania. Stwarza nowe możliwości w zakresie badań niezbędności&nbsp; składnik&oacute;w pokarmowych oraz relacji&nbsp; ilościowych między składnikami pokarmowymi. </font></p>
<p align=""justify""><font size=""4"">Aeroponiczna uprawa roślin jest pierwszą technologią, w kt&oacute;rej nie występuje antagonizm między powietrzem i wodą. W tej metodzie uprawy w środowisku korzeniowym znajduje się maksymalna zawartość tlenu (21 %) i maksymalna zawartość wody (100 % wilgotność). Korzenie nie pokonują oporu glebowego. Silnie&nbsp;rozbudowany system korzeniowy jest&nbsp; w stanie w pełni zaspokoić potrzeby wodne i pokarmowe roślin (zdjęcia 1-5 z badań wstępnych prowadzonych przez kierownika projektu). Wydaje się, że maksymalny rozw&oacute;j system&oacute;w korzeniowych oraz możliwość precyzyjnego żywienia roślin stanowią&nbsp;gł&oacute;wne przesłanki&nbsp; wysokiego plonowania roślin, przekraczającego dotychczasowe efekty plonotw&oacute;rcze. </font></p>
<p align=""center""><img height=""300"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3608"" /></p>
<p align=""center""><img height=""300"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3607"" /></p>
<p align=""center""><font size=""4""><img height=""300"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3606"" /></font></p>
<p align=""center""><img height=""300"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3605"" /></p>
<p align=""center""><img height=""300"" alt="""" width=""400"" src=""/OSFImageLoader.do?idImageDB=3604"" /></p>",07/01/11,23,Aeroponiczna uprawa pomidora szklarniowego (Lycopersicon esculentum Mill.),"aeroponika, uprawa aeroponiczna, pomidor",173
160,4589,opisProjektuStanWiedzy,"<div>Historia polskiego językoznawstwa jest dziedziną dziś niespecjalnie popularną, zaś zagadnienia związane z opisami ewolucji pogląd&oacute;w gramatycznych obecnie niemal w og&oacute;le nie są podejmowane. Literatura przedmiotu, powstała w przeważającej części do lat&nbsp;80. ubiegłego wieku, pokazuje, że daleko jeszcze do kompletności stanu badań z tego zakresu.</div>
<div>Choć pojawiło się sporo artykuł&oacute;w i kilka monografii przestawiających historię polskiego językoznawstwa na poziomie og&oacute;lnym (np. Bajerowa 1978; Urbańczyk 1968, 1993; Zag&oacute;rski 1981), nadal brak szczeg&oacute;łowego om&oacute;wienia opis&oacute;w niekt&oacute;rych dział&oacute;w gramatyki. Do tej pory opracowane zostały: nauka o dźwiękach mowy (Jastrzębska-Golonka 2004), opisyy kategorii gramatycznych (Skarżyński 1994, 2001), opisy słowotw&oacute;rcze (Puzynina 1964, Grzegorczykowa 1964, Kawyn-Kurz 1964) oraz składnia (Podracki 1982). </div>
<div>Klasyfikacje fleksyjne pojawiające się w gramatykach XIX i początku XX w. zostały co prawda wyliczone w opracowaniu Z. Zag&oacute;rskiego, jednak nie pokazuje on kształtowania się pogląd&oacute;w na temat nauki o wyrazie. Stosunkowo wąska też jest podstawa materiałowa, preferująca gramatyki wydawane w Wielkopolsce.</div>
<div>W&nbsp;projekcie&nbsp;przyjmuje się znacznie szerszą podstawę materiałową w stosunku do ujęcia Zag&oacute;rskiego (1981) - w odniesieniu do wieku XIX&nbsp; i rozszerzoną poza rok 1918. Spos&oacute;b badania i opisu zbliżony będzie do metod stosowanych przez J. Podrackiego (1982) i Skarżyńskiego (1994) - chodzi bowiem&nbsp;o rekonstrukcję sposobu myślenia teoretycznego&nbsp;gramatyk&oacute;w i językoznawc&oacute;w, wskazanie ź&oacute;deł i uchwycenie linii rozwojowych opis&oacute;w na przestrzeni 121 lat.</div>
<div></div>",07/01/17,23,Opisy fleksyjne w gramatykach polskich lat 1817-1939,historia polskiego językoznawstwa; opisy gramatyczne polszczyzny; nauka o wyrazie; fleksja,173
161,4591,opisProjektuStanWiedzy,"Dotychczasowe metody formułowania strategii rozwoju gmin bazowały na og&oacute;lnej teorii system&oacute;w. Podstawowa konstrukcja teoretyczna polegała na wydzieleniu z otoczenia organizacji gminnej oraz badaniu relacji zachodzących pomiędzy organizacją a otoczeniem. Technika SWOT pozwalała na analizowanie słabych i silnych stron organizacji oraz szans i zagrożeń. Formułowana na podstawie analizy odpowiedź strategiczna była zazwyczaj kompromisem pomiędzy postulatami społecznymi a twardymi realiami gospodarczymi. Zasada zr&oacute;wnoważonego rozwoju wyznaczała kanon myślenia o strategii rozwoju gmin. <br />Szybki postęp technologii informacyjnych sprawił, że paradygmat m&oacute;wiący o sprzeczności cel&oacute;w społecznych i gospodarczych stracił na znaczeniu. Zaczęła dojrzewać myśl, że umiejętne regulowanie zasad wsp&oacute;łpracy partner&oacute;w zewnętrznych i wewnętrznych systemu powoduje rozw&oacute;j szeroko rozumianej organizacji sieciowej tworząc przewagę konkurencyjną w zajętej warstwie strategicznej. Powstały nowe metody diagnozowania sytuacji organizacji i badania ich relacji z otoczeniem. W szczeg&oacute;lności niezwykle trafne w opisie okazały się teorie rozwoju organizacji bazujące na analogii do biologicznych ekosystem&oacute;w. Zauważono, że w ekosystemie biznesowym, podobnie jak w biologicznym, występują firmy pełniące pewne szczeg&oacute;lne role regulacyjne. Autorzy Marco Iansiti, Roy Levien w artykule Strategia a ekosystem biznesowy (Harvard Business Review, Listopad 2006) nazwali tę rolę zwornikiem systemowym, gdyż od takiej firmy uzależniony jest los całego ekosystemu biznesowego. Autorzy ci obmyślili także narzędzia diagnostyczne zdrowia ekosystem&oacute;w biznesowych. <br />Prace badawcze w projekcie będą zmierzały do zaadaptowania nowych wątk&oacute;w myślenia strategicznego do reali&oacute;w polskich gmin. Przezwycieżone zostaną dotychczasowe trudności w definiowaniu granic organizacji gminnej oraz w analizie relacji z interesariuszami. Wykorzystany zostanie obecny dorobek autor&oacute;w wniosku w zakresie metod formułowania strategii rozwoju gmin oraz prace badawcze wiodących ośrodk&oacute;w akademickich na świecie. <br />",07/01/29,23,Zastosowanie teorii ekosystemów biznesowych do formułowania strategii rozwoju gminy,"teoria ekosystemów biznesowych, ekosystem gminy, formułowanie strategii, rozwój gminy",173
162,4593,opisProjektuStanWiedzy,"Glukany stanowią heterogeniczną grupę polimer&oacute;w glukozy, występujących jako fundamentalny składnik ścian kom&oacute;rkowych drożdży, grzyb&oacute;w oraz roślin zbożowych i wykazują aktywne oddziaływanie na funkcje układu immunologicznego (Czop 1986, Cleary i in. 1999, Kolman i in. 1998, Krakowski i in. 1999, Brown i Gordon 2001, Bridle i in. 2005). Immunomodulacyjne efekty glukan&oacute;w zależą od ich cech strukturalnych, takich jak ciężar molekularny, stopień bocznych rozgałęzień, konformacja helisy rozpuszczalność oraz zdolność tworzenia żelu (Bohn i BeMiller 1995). Bohn i BeMiller (1995) odkryli, że działanie aktywujące wykazują szczeg&oacute;lnie &beta;-D-glukany o wiązaniach glikozydowych 1,3. Cleary i in. (1999) wykazali, że efektywność stymulacji funkcji immunologicznych 1,3-&beta;-D-glukan&oacute;w zależy od ich ciężaru molekularnego oraz obecności wiązań 1,6. Stąd wysoko oceniany jako immunostymulant jest &beta;-1,3/1,6-glukan pozyskiwany na skalę przemysłową ze ścian kom&oacute;rkowych drożdży gatunku <em>Saccharomyces cerevisiae</em>. <br />Zasadniczy mechanizm działania glukan&oacute;w polega na aktywacji makrofag&oacute;w (Szabo i in. 1995, Compton i in. 1996, Thornton i in. 1996) tworzących pierwszą linię obrony organizmu. Stanowią one dla nich wysokoenergetyczną odżywkę, kt&oacute;ra na zasadzie reakcji łańcuchowej pobudza i stawia w stan gotowości system immunologiczny. Jest to możliwe dzięki specyficznym receptorom tych polisacharyd&oacute;w (CR 3), znajdującym się na makrofagach (Thornton i in. 1996). Wykazano r&oacute;wnież, że glukany mogą bezpośrednio wpływać na aktywność innych kom&oacute;rek immunokompetentnych, <br />w tym limfocyt&oacute;w T i B (Hashimoto i in. 1991), kom&oacute;rek NK (Di Renzo i in. 1991) oraz granulocyt&oacute;w kwasochłonnych (Mahauthaman i in. 1988, Cleary i in. 1999) i obojętnochłonnych (Czop i in. 1988, Cleary i in. 1999). <br />Glukany uznawane są też za istotny czynnik antynowotworowy. Przyjmuje się, że oddziałując na powierzchnię limfocyt&oacute;w lub na białka osocza stymulują one wytwarzanie interleukin IL-1, IL-2 oraz interferonu IFN-g, co w efekcie hamuje rozw&oacute;j kom&oacute;rek nowotworowych (Mizuno T. 1995 a i b). Najwyższą aktywnością przeciwnowotworową charakteryzują się &beta;-glukany o dużej masie cząsteczkowej zawierające gł&oacute;wnie wiązania &beta;-1,3 (Mizuno T. 1995 a). Nie wywołują one w organizmie reakcji alergicznych ani innych niepożądanych zmian, wykazują natomiast działanie cytotoksyczne w stosunku do kom&oacute;rek nowotworowych, co stwierdzono zar&oacute;wno w badaniach in vitro, jak i in vivo (Smith i in. 2002). <br />Stwierdzono, że &beta;-glukany utrzymują homeostazę organizmu, obniżają ciśnienie krwi, zapobiegają powstawaniu udar&oacute;w m&oacute;zgu, wykazują działanie antyoksydacyjne i hipoglikemizujące, a także mają właściwości przeciwbakteryjne, przeciwwirusowe i przeciwzapalne (Wasser i Weis 1997). <br />Podwyższoną w efekcie oddziaływania glukan&oacute;w aktywność układu immunologicznego obserwowano w badaniach na rybach ( Kolman i in. 1998, Cleary i in. 1999, Bridle in. 2005), klaczach (Krakowski i in. 1999) i prosiętach (Hiss i Sauerwein 2003). Może to oznaczać wzrost odporności, co przemawia szczeg&oacute;lnie za koncepcją zastosowania glukan&oacute;w w żywieniu owiec, kt&oacute;re poza nielicznymi wyjątkami, jak wrzos&oacute;wka, czy polska owca g&oacute;rska, charakteryzuje stosunkowo duża wrażliwość na niekorzystne oddziaływanie środowiska. Naturalną konsekwencją wspomagania ich systemu obronnego powinna być poprawa og&oacute;lnej sprawności, co z kolei może stymulowoć produkcyjność. Badania przeprowadzone na jagniętach w pełni potwierdziły tę tezę. Wykazano, że suplementacja mieszanki treściwej CJ stosowanej w ich żywieniu preparatem Biolex Beta HP, zawierającym 85% &beta;-1,3/1,6-D-glukanu separowanego ze ścian kom&oacute;rkowych drożdży gatunku <em>Saccharomyces cerevisiae</em>, wpłynęło korzystnie na tempo ich wzrost...",07/01/16,23,"Efekty stosowania w żywieniu owiec dodatków paszowych zawierających -1,3/1,6-glukan","owce, żywienie, glukany, mleczność",173
163,4595,opisProjektuStanWiedzy,"<p class=""western"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 35.45pt; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: Arial; mso-bidi-font-size: 10.0pt"">Dostępne są pojedyncze badania, w kt&oacute;rych analizowano w bardzo ograniczony spos&oacute;b stan układu krzepnięcia u pacjent&oacute;w z HCM. Yamamoto i wsp. (20) stwierdzili wzrost poziomu kompleksu trombina-antytrombina i fragmentu protrombiny F 1+2 w HCM oraz korelację tych parametr&oacute;w z wielkością lewego przedsionka. Nie wzięto jednak pod uwagę obecności lub braku zawężenia w drodze odpływu lewej komory oraz grupa badanych pacjent&oacute;w była bardzo mała (13 os&oacute;b). Istnieją r&oacute;wnież doniesienia na temat aktywności czynnik&oacute;w zapalnych w HCM. Hogye i wsp. (21) w swoim badaniu potwierdzili wzrost IL-6 (przy prawidłowym poziomie TNF &ndash; alfa) jednak podobnie jak w poprzednim badaniu nie uwzględniono obecności lub braku zawężenia w drodze odpływu lewej komory co ma być przedmiotem naszych badań. </span></p>",07/01/24,23,"Wpływ zwężenia drogi odpływu lewej komory w spoczynku i po stymulacji fizjologicznym wysiłkiem na aktywację czynników krzepnięcia, zapalenia i aktywności płytek u chorych z kardiomiopatią przerostową","kardiomiopatia przerostowa, generacja trombiny, powikłania zakrzepowo-zatorowe",173
164,4603,opisProjektuStanWiedzy,"<div style=""TEXT-ALIGN: left"" align=""justify""><font style=""FONT-FAMILY: Arial;"" size=""2""><span style=""FONT-SIZE: 12.5pt""></span></font><font size=""4""><font style=""FONT-FAMILY: Arial;""><span style=""FONT-SIZE: 12.5pt"">Zagadnienie to wymaga obecnie bardziej metodycznego podejścia, zwłaszcza że<span>&nbsp; </span>opisane dotychczas w literaturze badania właściwości powierzchniowych poliuretan&oacute;w wykonywane były zazwyczaj przyczynkowo, przy okazji og&oacute;lnego charakteryzowania wytwarzanych materiał&oacute;w do r&oacute;żnych cel&oacute;w np. jako implanty medyczne (poliuretan typu Pelletane<sup>&reg;</sup>), dla kt&oacute;rych istotne są m.in. trombotoksyczne cechy materiał&oacute;w organicznych, stykających się z kom&oacute;rkami krwinek [14].<o:p></o:p></span></font><br /><font style=""FONT-FAMILY: Arial;""><span style=""FONT-SIZE: 12.5pt"">Uważamy, że obecny stan wiedzy w tym temacie i nasze doświadczenia naukowe w zakresie chemii i technologii poliuretan&oacute;w oraz dostęp do specjalistycznej aparatury pozwalają na zrealizowanie takiego przedsięwzięcia i podjęcia pr&oacute;by dokonania dla poliuretan&oacute;w uog&oacute;lnienia wsp&oacute;łzależności strukturalnych i właściwości powierzchniowych. Liczymy się z faktem, że temat ten może przerosnąć realne możliwości jego wykonania<span>&nbsp; </span>w ramach jednego tylko projektu, ale wtedy w zależności od postępu prac możliwe byłoby jego kontynuowanie w ramach innych przedsięwzięć naukowych.<o:p></o:p></span></font><br /><font style=""FONT-FAMILY: Arial;""><span style=""FONT-SIZE: 12.5pt"">W naszych wcześniejszych pracach zwr&oacute;ciliśmy uwagę, że możliwe jest takie zaprojektowanie procesu stopniowej poliaddycji, aby nowe właściwości poliuretan&oacute;w uzyskać w w oparciu o dostępną bazę surowcową, ale poprzez ukierunkowane zmiany procesowe, kt&oacute;re pozwalają na preferowane powstawanie makrocząsteczek o określonej polidyspersyjności, tak składu chemicznego jak i masy cząsteczkowej [15]. Zdobyte podczas tych badań doświadczenia preparatywne zamierzamy wykorzystać w tym projekcie.<o:p></o:p></span></font><br /><font style=""FONT-FAMILY: Arial;""><span style=""FONT-SIZE: 12.5pt"">Stwierdziliśmy r&oacute;wnież, że właściwości powierzchniowe poliuretan&oacute;w odgrywają bardzo istotną rolę w przypadku zastosowania tych polimer&oacute;w jako wodorozcieńczalnych spoiw dla proszkowej ceramiki tlenkowej (Al<sub>2</sub>O<sub>3</sub> i ZrO<sub>2</sub>), niezbędnych na etapie formowania wyrob&oacute;w w tzw. stanie zielonym. Spoiwa te nadają wyrobom ceramicznym na tym etapie procesu wytwarzania wytrzymałość mechaniczną pozwalajacą na wykonywanie obr&oacute;bki mechanicznej i nadawanie zasadniczych kształt&oacute;w wyrobom jeszcze przed ich ostatecznym wypaleniem, po kt&oacute;rym obr&oacute;bka mechaniczna jest już dużo bardziej trudna i wymaga użycia drogich narzędzi diamentowych [9] (green maschining). Właściwości powierzchniowe są ważne także w przypadku wytwarzania ze specjalnie preparowanej wodnej gęstwy ceramiczno-polimerowej wg technologii <em>tape casting</em> lub <em>doctor blade</em> cienkich powłok ceramicznych. Zagadnienia te, niezależnie od niniejszego projektu, są przedmiotem naszej wsp&oacute;łpracy naukowej z Instytutem Energetyki, Oddział Ceramiki CEREL w Boguchwale k. Rzeszowa [16].<o:p></o:p></span></font><br /><font style=""FONT-FAMILY: Arial;""><span style=""FONT-SIZE: 12.5pt"">Jako spoiwa polimerowe zastosowaliśmy w tym przypadku jonomery poliuretanowe, kt&oacute;re syntezowaliśmy w wyniku przedłużania w środowisku wodnym zdyspergowanych prepolimer&oacute;w izocyjanianowych za pomocą diamin o zr&oacute;żnicowanej hydrofilowości. Analizując energię powierzchniową <span>g<sub>L</sub></span></span><sub><span style=""FONT-SIZE: 12.5pt""></span></sub><span style=""FONT-SIZE: 12.5pt""> otrzymanych dyspersji poprzez pomiary kąt&oacute;w zwilżania <span>Q</span> jakie tworzą te ciecze na wzorcowej powierzchni poli(tetrafluroetylenu) o znanej energii powierzchniowej <span>g<...",06/12/29,23,Nowoczesne kierunki syntezy i aplikacji tworzyw poliuretanowych o regulowanej energii powierzchniowej,"poliuretany, swobodna energia powierzchniowa, struktura chemiczna, struktura nadcząsteczkowa powłoki ochronne, spoiwa,  odporność na biodegradację ",173
165,4605,opisProjektuStanWiedzy,"Dotychczasowe badania koncentrowały się na wpływie środowiska  na podstawowe cechy użytkowe jakimi są w przypadku sarny (Capreolus capreolus) masa ciała i masa poroża u samców. Badania biometryczne obejmowały oprócz masy ciala tworzenie tzw. indeksów budowy ciała na podstawie pomiarów post mortem poszczgólnych partii ciała z wykorzystaniem istniejących modeli matematycznych stosowanych u zwierząt gospodarskich. Duża zmienność w obrębie gatunku takich parametrow jak masa ciała czy forma poroża skłania niektórych autorów do wyodrębnienia podgatunków czy subpopulacji w zależności o rozmieszczenia geograficznego a co za tym idzie różnych walorow środowiska zewnętrznego takich jak; żer, jego dostępnośći jakość. Możliwość określenia typu filogeograficznego z wykorzystaniem drzewa filogenetycznego u sarny z jednoczsnym przedstawieniem graficznym dystansów genetycznych pomiedzy populacjami, ale również pomiędzy poszczególnymi osobnikami może byc pomocna w definitywnym rozstrzygnięciu dyskusji na temat odrębności populacji sarny polnej czy też kniejowej.",07/01/30,23,Ocena różnorodności genetycznej  sarny (Capreolus capreolus) na podstawie polimorfizmu jądrowego i mitochondrialnego DNA,"Capreolus, mtDNA,filogeneza, polimorfizm",173
166,4607,opisProjektuStanWiedzy,"<p align=""justify"">Prowadzonych jest wiele prac nad użyciem membran ciekłych do usuwania chromu z roztwor&oacute;w wodnych. Dotyczą one jednak w zdecydowanej większości chromu(VI) [16, 17]. Jest to spowodowane przede wszystkim udowodnionym negatywnym wpływem tej formy chromu na człowieka i środowisko naturalne. Chrom(III) jako forma nietoksyczna pozostaje przez wielu badaczy pomijany. <br />M. Chaudry i inni [2], wykorzystując obszerny dorobek naukowy dotyczący możliwość usuwania chromu(VI) w układach z membraną ciekłą zaproponowali aby chrom(III) wstępnie utlenić do Cr(VI), a następnie wydzielić chromiany wykorzystując jako przenośnik TOA (tri-n-octyamina). Gęgała i Walkowiak [17] zaproponowali separację Cr(VI)/Cr(III) z kwaśnych roztwor&oacute;w w układzie z unieruchomioną membraną ciekłą, kt&oacute;rą stanowiła polimerowa matryca Celgard 2500 nasycona roztworem Cyanexu 921 w nafcie. Czynnikiem decydującym w dużej mierze o efektywności procesu był wzajemny stosunek rozdzielanych form chromu w fazie zasilającej. Dla niewielkich początkowych stężeń Cr(VI) rzędu 10-4M następuje praktycznie całkowite usunięcie chromu w przeciągu 10 h. Z drugiej strony więcej niż 99% Cr(III) pozostaje w fazie zasilającej. Stopień usunięcia Cr(VI) maleje wraz ze wzrostem jego stężenia początkowego. <br />R. Molinari i inni [4] oraz R. Gawroński i P. Religa [5, 6] zajmowali się badaniem możliwości bezpośredniego usuwania chromu(III) z roztwor&oacute;w wodnych przy użyciu membran ciekłych. R. Molinari i inni przeprowadzili badania w układzie z membraną podpartą (SLM). Podłoże polimerowe membrany stanowiła mikroporowata polipropylenowa folia Accurel o porowatości 17%, grubości 150 i rozmiarach por&oacute;w 0,2 . Jako przenośnika użyto DNNSA (kwas dinonylonaftalenosulfonowy) rozpuszczonego w r&oacute;żnych rozpuszczalnikach organicznych (nafta, o-xylen, heksann, n-heptan). Ze względu na mniejszą lepkość związk&oacute;w aromatycznych w stosunku do lepkości węglowodor&oacute;w alifatycznych największy strumień chromu(III) uzyskano dla o-xylenu. Niestety, słabe siły kapilarne utrzymujące membranę w porach nośnika polimerowego, powodują małą trwałość membrany. Autorzy sugerują stosowanie mieszaniny rozpuszczalnik&oacute;w: o-xylenu i nafty. Dodatek nafty zwiększa co prawda lepkość membrany, a tym samym strumień przenoszonej substancji ulega zmniejszeniu, ale zdecydowanie wpływa na polepszenie trwałości membrany. <br /><br />Ponieważ transport jon&oacute;w chromu(III) jest transportem sprzężonym gdzie rolę przeciwnie transportowanego jonu spełnia jon H+, dlatego Molinari i inni [4] zalecają prowadzenie procesu przy następujących wartościach pH faz wodnych: fazy zasilającej w granicach 4,2 &ndash; 4,5 (powyżej pH 4,8 Cr(III) wytrąca się w postaci wodorotlenku); fazy odbierającej w granicach 0 - 0,5. W swojej pracy autorzy obserwowali r&oacute;wnież duży wpływ temperatury oraz stężenia początkowego chromu w roztworze na wydajność procesu w badanym układzie. Wielkość strumienia zwiększyła się 2-krotnie przy temperaturze 35oC w stosunku do temperatury 25oC. Jednocześnie uległ skr&oacute;ceniu czas po jakim została osiągnięta maksymalna wydajność procesu. Wraz ze wzrostem stężenia obserwowano spadek szybkość oraz wydajności procesu. <br />Badania R. Gawrońskiego i P. Religi [5, 6] przeprowadzone w układzie z BLM potwierdzają dobre właściwości transportowe kwasu DNNS względem chromu(III). Autorzy zaobserwowali r&oacute;wnież dużą zależność szybkości procesu w zależności od ilości użytego przenośnika. Istnieje pewne progowe jego stężenie przy kt&oacute;rym proces zachodzi z maksymalną prędkością [18]. Wysunięto nawet przypuszczenie, że w tych warunkach następuje zmiana mechanizmu transportu prostego przenośnikowego na tzw. transport &bdquo;przeskokowy&rdquo;. Rezultaty prac opisanych w [6] wskazują na brak wpływu rodzaju kwasu użytego jako faza odbierająca. Istotnym jest natomiast jego stężenie. Duże stężenie jon H+ w fazie odbierającej jest pożądane, zwiększa on...",07/01/31,23,"Transport chromu(III) w układzie z podpartą membraną ciekłą  kinetyka i mechanizm transportu, trwałość i stabilność membrany, modelowanie procesu","SLM, modelowanie, chrom(III), mechanizm transportu, kinetyka, stabilność membrany",173
167,4617,opisProjektuStanWiedzy,"<p class=""MsoBodyText"" style=""text-align: justify; text-indent: 35.4pt; font-family: Times New Roman;""><font size=""3""><span style=""font-weight: bold;"">Znaczenie jakości skorupy jaja i czynniki żywieniowe kształtujące ten parametr</span>. Głównym zadaniem skorupy jest ochrona treści jaja przed szkodliwym wpływem zewnętrznych czynników środowiskowych, a w szczególności mikroorganizmów patogennych. Skorupa odgrywa ważną rolę w rozwoju zarodkowym, umożliwiając rozwijającemu się zarodkowi wymianę gazową oraz regulując wilgotność wnętrza jaja. Jakość skorup, rozumiana jako ich wytrzymałość na pękanie, jest również ważnym czynnikiem wpływającym na opłacalność towarowej produkcji jaj. Według niektórych danych straty wynikające ze stłuczeń i mikropęknięć skorupy stanowią 6-10% sumy wszystkich wyprodukowanych jaj, co w skali ogólnoświatowej można przeliczyć na około 500 milionów dolarów lub na 2 dolary w przeliczeniu na 1 nioskę (Bell, 1998; Gomez-Basauri, 1997). Przeprowadzając badania w dużych sklepach samoobsługowych stwierdzono, że w około 45% opakowań zbiorczych przynajmniej jedno jajo wykazuje pęknięcia skorupy (Bell i in., 1997), co może powodować odrzucenie całego opakowania przez kupujących.<br /></font></p>
<p class=""MsoBodyText"" style=""text-align: justify; text-indent: 35.4pt; font-family: Times New Roman;""><font size=""3"">Jakość skorupy jest istotna również ze względu na bezpieczeństwo konsumentów, gdyż brak jakichkolwiek jej uszkodzeń warunkuje dobrą ochronę treści jaja przed wtórnym skażeniem bakteryjnym np. Salmonellą. Aspekt ten nabiera obecnie szczególnego znaczenia ze względu na postępującą w Europie rezygnację z klatkowego systemu utrzymania kur nieśnych na rzecz chowu na ściółce, gdzie jaja mają większy kontakt z odchodami i są bardziej narażone na zakażenia drobnoustrojami. W przeprowadzonych ostatnio badaniach De Reu i in. (2005) wykazali, że w porównaniu z jajami pochodzącymi od kur utrzymywanych w klatkach, jaja z chowu na ściółce charakteryzują się znacznie większym zanieczyszczeniem skorup bakteriami tlenowymi. <br /></font></p>
<p class=""MsoBodyText"" style=""text-align: justify; text-indent: 35.4pt; font-family: Times New Roman;""><font size=""3"">Ze względu na budowę chemiczną oraz przebieg procesu kalcyfikacji skorupy jaja, najważniejszym spośród czynników żywieniowych, wpływających na jej jakość jest odpowiednie zaopatrzenie organizmu nioski w wapń. Wyniki wielu doświadczeń wskazują jednak, że samo zwiększanie zawartości tego makroelementu w diecie ponad wartości normatywne nie poprawia jakości skorupy (Keshavarz, 2003; Keshavarz i Nakijama, 1993; Leeson i in., 1993). Korzystne natomiast okazuje się stosowanie źródeł Ca o dużej średnicy cząstek, np. żwiru wapiennego lub muszli ostryg, zapewniających równomierne zaopatrzenie w ten pierwiastek w ciągu doby oraz zachowanie w paszy odpowiedniego stosunku Ca:P (Ahmad i Balander, 2003; Jamroz i in., 2000; Koreleski i Świątkiewicz, 2004; Zhang i Coon, 1997).<br />Zbyt wysoki, przekraczający 0,40-0,45%, udział fosforu dostępnego w paszy dla niosek ma negatywny wpływ na jakość skorupy jaj (Hossain i Bertechini, 1998; Nys, 2001; Vandepopuliere i Lyons, 1992; Waldroup i in., 2005). Efekt ten jest szczególnie silny przy stosowaniu mieszanek charakteryzujących się niską zawartością wapnia, gdyż łatwo wówczas o nadmierne zawężenie stosunku Ca do P ogólnego, który powinien wynosić około 7 do 1. Według zaleceń norm poziom P przyswajalnego w dawkach dla kur nieśnych typu lekkiego powinien wynosić 0,36-0,37%, w zależności od tempa nieśności (Normy Żywienia Drobiu, 2005). Biorąc pod uwagę zagrożenie nadmierną akumulacją fosforu w środowisku i wyniki wielu opublikowanych w ostatnich latach prac badawczych, cytowany poziom wydaje się jednak zbyt wysoki. W licznych doświadczeniach obniżenie zawartości P dostępnego do 0,3% (Frost i Roland, 1993; Summers, 1995), a nawet do 0,2% (Keshavarz, 2003; Keshavarz i Nakijama, 1993; Usayran i in., 2001) nie pogorszyło wyników produkcyjnych i j...",07/01/29,23,Wpływ fruktanów i kwasów organicznych na wykorzystanie składników mineralnych u drobiu,"Kurczęta brojlery, kury nieśne, fruktany, kwasy organiczne, składniki mineralne",183
168,4618,opisProjektuStanWiedzy,"<p align=""left""><strong>IMMUNOMODULACJA </strong><br />Zjawiskiem immunomodulacji jako sposobem modyfikowania (wzmacniania bądź osłabiania) reakcji odpornościowej organizmu, lekarze oraz naukowcy zajmują się od stuleci. Już w czasach starożytnych podejmowano pierwsze pr&oacute;by modelowania działania układu immunologicznego poprzez podsk&oacute;rne iniekcje czynnik&oacute;w chorobotw&oacute;rczych (np. wyciąg z krost ospy wietrzej). Pomimo uzyskiwania pozytywnych rezultat&oacute;w &oacute;wcześni badacze nie byli w stanie wytłumaczyć tego zjawiska. Udało się tego dokonać wsp&oacute;łczesnym naukowcom, przede wszystkim dzięki rozwojowi obecnej wiedzy i techniki [Baschang G. 1989]. Związki stymulujące lub hamujące kom&oacute;rkowe i humoralne mechanizmy obronne nazwano immunomodulatorami (odpowiednio: immnostymulatorami i immunosupresorami), a samo zjawisko określono jako immunomodulację [Siwicki A.K. 2003]. <br />Obecnie dużą rolę w immunomodulacji odgrywają związki pochodzenia mikrobiologicznego np. muramylodipeptyd (MDP, fragment bakteryjnych ścian kom&oacute;rkowych) [Bardana E.J. 1985; Siwicki A.K. 2003], jak r&oacute;wnież peptydy pochodzenia zwierzęcego, w tym tuftsyna &ndash; naturalny tetrapeptyd o wybitnych właściwościach stymulujących fagocytarną aktywność granulocyt&oacute;w obojętnochłonnych [Dzierzbicka K. 2000]. <br /><strong>MURAMYLODIPEPTYD</strong> <br />Muramylopeptydy to produkty rozpadu peptydoglikan&oacute;w (PGN) naturalnie występujących w ścianach bakterii gramm dodatnich (G+) i gramm ujemnych (G-). Ich powstawanie wiąże się z aktywnością enzym&oacute;w np. lizosomalnych bądź też amidaz. W przypadku wchłonięcia bakterii przez kom&oacute;rkę organizmu ludzkiego, muramylopeptydy warunkują sygnalizację wewnątrzkom&oacute;rkową prowadzącą do zmiany ekspresji gen&oacute;w oraz do aktywacji odpowiedzi immunologicznej [Traub S. 2006]. Jednym z najbardziej znanych czynnik&oacute;w modulujących reakcję immunologiczną kom&oacute;rek, wywodzącym się z PGN jest muramylodipeptyd (MDP). Związek ten znany jest od lat 70-tych XX w. i uważany za najmniejszą strukturę wykazującą aktywność adjuwantową. [Dzierzbicka K. 1997; Kołodziejczyk A.M. 1997]. <br />Docelowymi kom&oacute;rkami dla muralmylopeptyd&oacute;w są przede wszystkim monocyty i makrofagi, ale r&oacute;wnież kom&oacute;rki dendrytyczne [Todate A. 2001] czy kom&oacute;rki NK [Sosnowska D. 1997]. <br />Pod wpływem muramylopeptyd&oacute;w makrofagi jak i monocyty zwiększają fagocytozę, sekrecję szeregu substancji stymulujących lub hamujących inne kom&oacute;rki układu immunologicznego [Dzierzbicka K. 2003; Sosnowska D. 1997], a także aktywność cytostatyczną i cytotoksyczną w stosunku do kom&oacute;rek obcych, w tym nowotworowych [Baschang G. 1989; Kołodziejczyk A.M. 1997]. Natomiast kom&oacute;rki NK reagują zwiększeniem cytotoksyczności w stosunku do standardowej linii białaczki K562 jak i w stosunku do nowotwor&oacute;w, np. czerniaka amelanotycznego Ab Bomirskiego. [Sosnowska D. 1997]. <br />Muramylodipeptyd stymuluje niespecyficzną odporność organizmu przeciw patogenom bakteryjnym, wirusowym oraz pasożytniczym. Wykazuje r&oacute;wnież działanie immunomodulacyjne polegające na oddziaływaniu synergistycznym z innymi substancjami czynnymi [Dzierzbicka K. 2003]. A także powoduje wzrost aktywności przeciwbakteryjnej i intensywności fagocytozy poprzez zwiększenie ekspresji cząsteczek powierzchniowych zaangażowanych w adhezję i kostymulację do prezentacji antygen&oacute;w [Darcissac E.C. 1996; Heinzelmann M. 1997]. <br />W ostatnich latach udało się zidentyfikować struktury wewnątrzkom&oacute;rkowe odpowiedzialne za rozpoznawanie i wiązanie MDP. Muramylopeptydy mogą aktywować kom&oacute;rki odpornościowe poprzez r&oacute;żne typy receptor&oacute;w powierzchniowych, w tym receptory TLR [Inohara N. 2003], CD14 [Wolfert M. A. 2002], czy receptor 5-HT [Sevcik J. 1999; Sevcik J. 2002] <br />Rodzina receptor&oacute;w błonowych zaangażowanych w rozpoznawanie element&oacute...",07/01/03,23,Immunomodulacyjne właściwości nowych analogów muramylodipeptydu (MDP) z tuftsyną,"immunomodulacja, muramylodipeptyd, tuftsyna, komórki dendrytyczne",173
169,4619,opisProjektuStanWiedzy,"<p align=""left""><font size=""3""></font></p>
<p align=""left""><font size=""3"">&nbsp;&nbsp; Prolaktyna (PRL) jest hormonem wytwarzanym przez przedni płat przysadki m&oacute;zgowej. Jej podstawową funkcją jest zapoczątkowanie i utrzymanie laktacji oraz wpływa na syntezę białek mleka, laktozy, tłuszczu i wszystkich najważniejszych składnik&oacute;w mleka (Le Provost i in. 1994). Wiele badań wykazało, że PRL jest stymulatorem wzrostu gruczołu mlekowego i laktogenezy. W ostatnich latach w przeprowadzonych badaniach u gryzoni, stwierdzono, że PRL jest jednym z gł&oacute;wnych endokrynologicznych czynnik&oacute;w kontrolujących obumieranie kom&oacute;rek podczas inwolucji. Accorsi i in. (2002) stwierdzili, że przy braku PRL w ciągu 48 h nastąpiło obniżenie wydajności mleka oraz ubytek ok. 20-25% sekrecyjnych kom&oacute;rek w gruczole mlekowym. Wyżej wymienieni autorzy sugerują, że ten hormon r&oacute;wnież odgrywa zasadniczą rolę w kontrolowaniu inwolucji kom&oacute;rek nabłonkowych wymienia u bydła. Bydlęcy gen PRL jest zlokalizowany w 23 chromosomie, składa się z pięciu ekson&oacute;w i czterech intron&oacute;w (Dybus 2002 za Barendse i in. 1997). Stwierdzona w trzecim eksonie bydlęcego genu PRL mutacja A-G powoduje wprowadzenie miejsca restrykcyjnego dla enzymu RsaI. W badaniach Chunga i in. (1996) wykazano, że polimorfizm PRL-RsaI wpływał istotnie na wydajność mleka i procent tłuszczu.&nbsp;<br />&nbsp;&nbsp;&nbsp;&nbsp;Przysadkowy transkrypcyjny czynnik Pit-1 odgrywa istotną rolę w procesach rozwoju organizmu jako aktywator genu hormonu wzrostu (GH), prolaktyny (PRL) i tyreotropiny (TSH) (Augustijn i in. 2003). Gen tego czynnika z powodu wielokierunkowego oddziaływania swego produktu został wybrany jako gen kandydujący do badań nad związkami z wieloma cechami produkcyjnymi m.in. ze wzrostem zwierzęcia, cechami poubojowymi i wydajnością mleczną (Stancekova i in. 1999). Locus Pit-1 położony jest u bydła w chromosomie 1 (Moody i in. 1995). Jedno z miejsc polimorficznych tego genu zlokalizowane jest w rejonie niekodującym (intron 5) i nie zmienia sekwencji aminokwas&oacute;w w białku. Badania Renavilla i in. (1997) wskazują jednak na wpływ tej mutacji na cechy produkcyjne u bydła holsztyńskiego.&nbsp;&nbsp;<br />&nbsp;&nbsp;&nbsp; Pit-1 włączony jest nie tylko w regulację gen&oacute;w prolaktyny, hormonu wzrostu ale bierze udział w procesach proliferacji i śmierci kom&oacute;rek, poprzez kontrolę gen&oacute;w regulatorowych m. in. receptor&oacute;w zwiazanych ze wzrostem, gen&oacute;w c-fos stąd określenie polimorfizmu może być istotnym elementem wskazującym na przebieg proces&oacute;w zachodzących nie tylko na poziomie przysadki m&oacute;zgowej.&nbsp;</font><font size=""3""> Badania krajowe i zagraniczne dotyczace&nbsp;zależności między genotypami&nbsp;&nbsp;Pit-1 a cechami użytkowości mlecznej były prowadzone praktycznie na jednej mutacji, natomiast w&nbsp;przedstawionym projekcie planuje się określenie związku&nbsp;dla czterech mutacji. Zwierzchowski i wsp. (2002) sugerują, że genotypy Pit-1 mogą mieć znaczny wpływ w doskonaleniu cech mleczności u bydła.&nbsp;Mutacje&nbsp;prolaktyny były&nbsp; badane przez Dybusa (2002) i Bryma i wsp.&nbsp;(2005)&nbsp;autorzy ci wykazali, że&nbsp;frekwencja niekt&oacute;rych homozygot była niska, lecz poprzez&nbsp;badanie większej liczby kr&oacute;w oraz poprzez zastosowanie odpowiednich modeli statystycznych można tą trudność pokonać.&nbsp;Dybus (2002)&nbsp;stwierdził związek jednej homozygoty genu PRL z wyższą &nbsp;zawartością białka w mleku,&nbsp;natomiast Brym i wsp. (2005) wykazali większą wydajność mleczną kr&oacute;w o układzie hererozygotycznym innego locus PRL.&nbsp;Jak dotąd wyniki przeprowadzonych badań nad&nbsp;wpływem polimorfizmu tych gen&oacute;w na użytkowość mleczną kr&oacute;w nie są jednoznaczne, stąd w przedstawionym projekcie proponuje się wykonanie badań nad wiekszą liczbą mutacji&nbsp;w powiązaniu z szerszą grupą cech użytkowych.&nbsp;</font><font size=""3"">Nie badano jednoczesn...",07/01/25,23,Polimorfizm genów prolaktyny (PRL) i czynnika transkrypcyjnego Pit-1 u bydła,"bydło mleczne, polimorfizm PRL, polimorfizm Pit-1",173
170,4629,opisProjektuStanWiedzy,"<p>Końcowym efektem realizowanego projektu będzie znalezienie oryginalnej&nbsp;formuły prowadzenia postępowania wyjaśniającego, kt&oacute;ra pozwoli z jednej strony na pełną realizację działań organu zmierzających do wszechstronnego ustalenia obiektywnie istniejącego stanu faktycznego sprawy,&nbsp;a z drugiej&nbsp;strony, na&nbsp;poprowadzenie postępowania wyjaśniającego w spos&oacute;b efektywny, skuteczny i tani. Realizacji powyższego założenia dokonana zostanie po dokonaniu analizy rozwiązań prawnych i organizacyjnych w wybranych modelach proceduralnych. &nbsp;&nbsp;</p>
<p>Efektywne kształtowanie&nbsp;przebiegu postępowania wyjaśniającego, polegające na wyborze&nbsp;właściwej - dla danego rodzaju sprawy - formy procesowej&nbsp;postępowania&nbsp;administracyjnego jest problemem wielu państw. W r&oacute;żnych modelach stosuje się r&oacute;żnego rodzaju uproszczenia i ułatwienia procesowe, zmierzające do odformalizowania przebiegu postępowania i co za tym idzie - do szybszego załatwienia sprawy. Nie zawsze jednak jest to działanie efektywne w pełnym tego słowa znaczeniu. Uproszczenie niekt&oacute;rych etap&oacute;w postępowania administracyjnego, pozwala wprawdzie w szybszy spos&oacute;b podjąć decyzję w sprawie, jednak powoduje ryzyko konieczności ponownego rozpatrzenia sprawy w wyniku wszczęcia kontroli instancyjnej przez niezadowodlonego adresata rozstrzygnięcia. W konsekwencji więc, postępowania staje się dłuższe&nbsp;i droższe.&nbsp;Problem nie jest więc nowy, ale i nie jest jednoznacznie rozwiązany, zaś wykorzystanie rozwiązań pochodzących z innych modeli procesowych wymaga poprowadzenia głębokiej analizy systemowej&nbsp;oraz daleko posuniętej ostrożności w odpowiednim inkorporowaniu rozwiązań procesowych do polskich&nbsp;procedur administracyjnych. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>",07/01/19,23,Rozprawa administracyjna jako forma postępowania wyjasniającego,"rozprawa administracyjna, postępowanie wyjaśniające ",173
171,4633,opisProjektuStanWiedzy,"<p class=""MsoBodyTextIndent3"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 42.55pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span lang=""EN-US"" style=""FONT-SIZE: 11pt; FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">Zagadnienie polityki dywidend jest stosunkowo dobrze znane w obcojęzycznej literaturze przedmiotu, szczeg&oacute;lnie amerykańskiej. Chociaż w ramach szerszych opracowań z zakresu finans&oacute;w, materiały dotyczące podziału nadwyżki finansowej w dużej mierze koncentrują się wyłącznie got&oacute;wkowych wypłatach z zysku netto. <o:p></o:p></font></span></p>
<p class=""MsoBodyTextIndent"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 42.55pt; LINE-HEIGHT: 150%"" align=""justify""><span lang=""EN-US""><font face=""Times New Roman"" size=""3"">Na tym tle&nbsp;niezbyt bogato&nbsp;prezentuje się dostępna polskojęzyczna literatura z podejmowanej problematyki &ndash; istnieją dwie polskojęzyczne pozycje zwarte. Warto zauważyć, ze jedna z nich, autorstwa M. Sierpińskiej,<span style=""mso-spacerun: yes"">&nbsp; </span>pochodzi z 1999 roku, oraz druga, autorstwa A. N. Duraj, z 2002 roku. Na uwagę zasługują także opracowania dotyczące dywidendy przygotowywane prze M. Wypycha, kt&oacute;ry w swoich opracowaniach politykę dywidendy przedstawia w nowym świetle i z uwzględnieniem wielu czynnik&oacute;w np. związku z corporate governance. Podkreślić także należy publikacje przygotowywane przez A. Szablewskiego odnoszące się do badania związku dywidendy i wykupu akcji z kursami akcji. Jednocześnie przygotowano zaledwie kilka rozpraw doktorskich oraz zrealizowano kilka projekt&oacute;w badawczych dotyczących tej tematyki. Artykuły naukowe na temat wspomnianej problematyki, przygotowywane przez r&oacute;żnych autor&oacute;w, poruszają jednak jedynie wybrane zagadnienia cząstkowe i odnoszą się do analizowanej tematyki w spos&oacute;b og&oacute;lny. </font></span></p>
<p class=""MsoNormal"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 42.55pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span lang=""EN-US"" style=""FONT-SIZE: 11pt""><font face=""Times New Roman""><font size=""3"">Prezentowana w powyżej przywołanych publikacjach tematyka odnosi się w dużej mierze, nie umniejszając ich znaczenia, do przedstawiania osiągnięć zagranicznych ośrodk&oacute;w naukowych tj. prezentowania teorii polityki dywidend, praktycznych rozwiązań w zakresie polityki dywidend, wyceny kapitału własnego czy kosztu kapitału własnego z uwzględnieniem formuł uwzględniających dywidendy, wpływu got&oacute;wkowych wypłat na kurs akcji. Większość tych zagadnień odnosi się do relacji między dywidendą a kursem akcji, traktując dywidendę jako czynnik determinujący wartość rynkową akcji. Takie ujęcie może być charakterystyczne dla badania stopnia efektywności rynku (reakcja rynku na informacje, decyzje i działania zachodzące w sp&oacute;łce) lub dla badania hipotezy zawartości sygnalizacyjnej dywidendy (sytuacji, gdy informacja o dywidendzie posiada zawartość informacyjną o aktualnej lub przyszłej sytuacji sp&oacute;łki).<o:p></o:p></font></font></span></p>
<p class=""MsoBodyText"" style=""MARGIN: 0cm 0cm 0pt; TEXT-INDENT: 42.55pt; LINE-HEIGHT: 150%; TEXT-ALIGN: justify""><span style=""FONT-SIZE: 11pt; FONT-FAMILY: &quot;Times New Roman&quot;""><font size=""3"">Niniejszy projekt charakteryzować się będzie nowatorskim podejściem w stosunku do prezentowanych w dotychczasowych opracowaniach. Prezentowane w ramach niniejszej koncepcji badań założenia odnoszą się bowiem do badania związku między wytworzoną nadwyżką finansową, wszystkimi formami jej podziału (w tym m.in. dywidenda got&oacute;wkowa, umorzenie akcji, dywidenda wypłacana w akcjach i inne) oraz dochodem realizowanym przez akcjonariuszy. Podstawowe kwestie podejmowane w niniejszym projekcie koncentrują się wok&oacute;ł następujących zagadnień: na ile wytworzona przez przedsiębiorstwo wartość dla akcjonariuszy jest odzwierciedlana w dochodzie realizowanym przez akcjonariuszy, na ile spos&oacute;b podziału wpływa na możliwości tworzenia wartości prze...",07/01/15,23,Procesy tworzenia i podziału nadwyżki finansowej,"polityka dywidend, wartość dochodowa przedsiębiorstwa, wartość rynkowa przedsiębiorstwa, teoria agencji, asymetria informacji, struktura kapitału",173
172,4643,opisProjektuStanWiedzy,"<div style=""text-align: justify;"">    <span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: -0.05pt;"">Stosowanie typowych laserów impulsowych Nd:YAG, wykorzystywanych do tej pory w dalmierzach i lidarach, jest obecnie bardzo ograniczone lub wręcz zakazane, ze względu na głęboką penetrację światła o długości fali 1 µm w tkance, stwarzającą realne zagrożenie dla narządu wzroku. Wynika stąd narastające w ostatnich dwóch dekadach zainteresowanie konstrukcjami wydajnych laserów „bezpiecznych dla wzroku” (tj. generujących </span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: 0.05pt;"">w zakresie powyżej 1.5 </span>μ<span style=""font-size: 12pt; font-family: Symbol; color: black; letter-spacing: 0.05pt;""><span style=""""></span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: 0.05pt;"">m</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: -0.05pt;""> odpowiadającym prawie całkowitej absorpcji promieniowania w przedniej części oka)</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: 0.05pt;"">. Warto zwrócić uwagę, że pasmo widma generacji „ bezpiecznej dla wzroku” ma bardzo ważne własności propagacyjne w atmosferze. Dla długości fal</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: -0.05pt;""> powyżej 2 </span>μ<span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: 0.05pt;"">m</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: -0.05pt;""> (rys. 2) trans</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;"">misja atmosfery jest bardzo duża, natomiast pochłanianie na parze wodnej oraz wybranych ośrodkach biologicznych <span style=""letter-spacing: -0.05pt;"">i zanieczyszczeniach atmosfery jest bardzo silne.</span></span><br /><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;""><span style=""letter-spacing: -0.05pt;""></span></span></span></div>
<span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;""><span style=""letter-spacing: -0.05pt;""><img style=""width: 800px; height: 383px;"" src=""/OSFImageLoader.do?idImageDB=4825"" alt="""" /><br /></span></span>
<div style=""text-align: center;""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;"">Rys. 2. Charakterystyka transmisji atmosfery w zależności od długości fal</span><br /><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;""></span></div>
<span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;""><br /></span>
<div style=""text-align: justify;""><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;"">    </span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black; letter-spacing: -0.05pt;"">Z tego względu, a także z uwagi na możliwość wykrycia chmur (dużego stężenia pary wodnej) poszukiwano laserów generujących promieniowanie o długości fali z zakresu 2 – 2.2 µm, któremu odpowiada zmienna w funkcji długości fali, dość znaczna absorpcja w atmosferze. Promieniowanie takie nie jest szkodliwe dla oczu, a dość silna absorpcja uniemożliwiająca głęboką penetrację, bardzo rozszerza gamę różnych (m.in. medycznych) zastosowań. Potencjal</span><span style=""font-size: 12pt; font-family: &quot;Times New Roman&quot;; color: black;"">ne zastosowania laserów generujących promieniowanie „bezpieczne dla wzro<span style=""letter-spacing: -0.05pt;"">ku” obejmują takie obszary techniki, jak: telekomunikacja, radary optyczne, pomiary odle</span>głości, zdalna detekcja zanieczyszczeń, </span><span style=""font-size: 12pt; font-family: &quot;Times New...",07/01/29,23,Przestrajalny laser impulsowy generujący promieniowanie w obszarze 2 - 2.15 μm wykonany w technologii hybrydowej,"laser hybrydowy, generacja impulsowa, Ho:YAG, Tm:fiber, „eye safe”",183
173,4645,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"" size=""3"">Literatura przedmiotu na temat finans&oacute;w i skarbowości w Polsce jest dość bogata. Na uwagę zasługują prace autor&oacute;w przedwojennych m.in. A. Grodka, R. Rybarskiego, R. Grodeckiego, W. Kornatowskiego i I. Weinfelda; opracowania, kt&oacute;re ukazały się w Polsce Ludowej - Z. Landau, J. Tomaszewskiego, K. Ostrowskiego, Z. Karpińskiego, W. Terleckiego </font><font face=""Times New Roman"" size=""3"">i R. Kiersnowskiego oraz wydane w okresie III Rzeczpospolitej: J.A.Szwagrzyka, Z. Landau, W. Morawskiego, L. Balcerowicza i pod red. B. Pietrzaka i Z. Polańskiego.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""3"">Opracowania wymienionych autor&oacute;w, a&nbsp;także wielu innych, są bardzo wartościowe. Niemniej, następujące argumenty przemawiają za podjęciem nowego kierunku badań:</font></p>
<ul>
    <li><font face=""Times New Roman"" size=""3"">nie ukazała się dotychczas tego typu seria wydawnicza;</font> </li>
    <li><font face=""Times New Roman"" size=""3"">istnieje potrzeba nowego spojrzenia na dokonania wybitnych reformator&oacute;w;</font> </li>
    <li><font face=""Times New Roman"" size=""3"">rzeczywistość polska zachęca do połączenia aspekt&oacute;w działalności ekonomicznej z postawami etycznymi.</font> </li>
</ul>",06/12/29,23,Wybitni reformatorzy i kreatorzy polskiego pieniądza a ich oblicze moralne,"reformy ekonomiczne, kreacja polskiego pieniądza, moralność reformatorów",173
174,4647,opisProjektuStanWiedzy,"<p>Nieznana jest sytuacja dotycząca epidemiologii&nbsp;zakazeń miejsca operowanego (ZMO)&nbsp;na oddziałach ortopedycznych. Nie prowadzono w Polsce dotychczas badań wieloośrodkowych obejmujących te populację. Jednak nawet nieliczne publikacje potwierdzają poniższe obserwacje autor&oacute;w. </p>
<p>Katedra Mikrobiologii CMUJ wsp&oacute;łpracuje od roku 2002 z Krakowskim Centrum Rehabilitacyjnym w zakresie zar&oacute;wno nadzoru epidemiologicznego, jak i mikrobiologicznego nad zakażeniami. W roku 2005 prowadzono pilotażowe badania&nbsp;z zastosowaniem&nbsp;nadzoru celowanego obejmującego 2 wybrane typy zabieg&oacute;w, tj. endoprotezowania stawu biodrowego (463 operacje) i kolanowego (166 procedury). </p>
<p>Wśr&oacute;d rozpoznanych przypadk&oacute;w ZMO &ndash; 12 dotyczyło pacjent&oacute;w poddanych zabiegom endoprotezoplastyki stawu biodrowego i 10 &ndash; stawu kolanowego. W populacji operowanych dominowały kobiety i stanowiły dla PROT-BIO i PROT-KOL prawie 70% i 92%, a średni wiek pacjent&oacute;w to odpowiednio: 68 i 70 lat. Nie stwierdzono zależności pomiędzy występowaniem przypadk&oacute;w ZMO u pacjent&oacute;w poddawanych r&oacute;żnym typom zabieg&oacute;w w zależności od płci oraz wieku. Poddano analizie wpływu długości zabiegu na pojawienie się zakażenia, istotną statystycznie r&oacute;żnicę zaobserwowano w populacji os&oacute;b operowanych w zakresie całkowitej endoprotezoplastyce stawu kolanowego (ICD-9: 81.54), gdzie pacjenci dłużej operowani cechowali się niższym prawdopodobieństwem wystąpienia zakażenia (T=2,0087, p=0,0485), jednak w tym wypadku stwierdzono bardzo niewielkie r&oacute;żnice og&oacute;łem w długościach zabieg&oacute;w pomiędzy operacjami, po kt&oacute;rych stwierdzono ZMO, a tymi bez objaw&oacute;w zakażenia, odpowiednio: 64,4 oraz 68,1 minut. W pozostałych typach zabieg&oacute;w &ndash; brak r&oacute;żnic. Mikrobiologiczna czystość pola nie wpływała na zachorowalność, natomiast stan pacjenta podczas kwalifikacji do zabiegu (wyrażony skalą ASA), istotnie wpływał na ryzyko wystąpienia ZMO w zabiegach całkowitej endoprotezoplastyki stawu biodrowego (chi kwadrat=8,106, p=0,0493). Pozostałe typy zabieg&oacute;w &ndash; brak r&oacute;żnic. <br />Przypadki ZMO rozpoznawane były zar&oacute;wno podczas pierwszej hospitalizacji, kt&oacute;ra trwała odpowiednio: 14 i 17 dni, jak w trakcie wizyt kontrolnych w poradni przyszpitalnej. Ponowna hospitalizacja konieczna była dla 33% os&oacute;b po PROT-KOL i 10% po PROT-BIOD. Zakażenie rozpoznawane było po (odpowiednio) 29 i 51 dniach po zabiegu, jednak najwięcej przypadk&oacute;w wykryto przed upływem 30 dni, co wiąże się, z dominacją wśr&oacute;d form ZMO: zakażeń powierzchownych, zar&oacute;wno po zabiegach PROT-BIO, jak i PROT-KOL. <br /></p>
<p>Analiza indeksu ryzyka możliwa była w odniesieniu do ok. 80% danych, pozostała cześć rekord&oacute;w zawierała braki, gł&oacute;wnie w zakresie opisu stanu pacjenta za pomocą skali ASA. Spośr&oacute;d pozostałych pacjent&oacute;w podanych zabiegowi w zakresie PROT-BIO &ndash; 41% stanowiły osoby, u kt&oacute;rych nie stwierdzono czynnik&oacute;w ryzyka, u 32% stwierdzono 1 z analizowanych czynnik&oacute;w ryzyka, zachorowalność sięgnęła 2,0%, natomiast populacja operowanych obciążonych większą liczbą czynnik&oacute;w ryzyka (2 lub3) stanowiła 6,5%. Podobny rozkład uzyskano w przypadku PROT-KOL, gdzie poszczeg&oacute;lne grupy stanowiły odpowiednio 50%, 24,7% oraz 8,4%. Stwierdzona zachorowalność najwyższa była u pacjent&oacute;w z 2 lub 3 czynnikami ryzyka poddanych procedurze endoprotezoplastyki staw&oacute;w biodrowych oraz z 1 czynnikiem ryzyka po zabiegach protezowania staw&oacute;w kolanowych. Og&oacute;łem wyższa dotyczyła rzadziej wykonywanych procedur PROT-KOL (tabela). <br /></p>
<p>Standaryzowany indeks ryzyka, por&oacute;wnujący zachorowalność pomiędzy pacjentami niemieckich szpitali (projekt Krankenhaus Infectionen Surveilllance System) i badanego oddziału polskiego szpitala wskazuje na istotnie sta...",07/01/26,23,Nadzór nad zakażeniami miejsca operowanego po zabiegach endoprotezoplastyki stawu kolanowego,"nadzór nad zakażeniami, zakażenie miejsca operowanego, endoprotezoplastyka, staw kolanowy",173
175,4650,opisProjektuStanWiedzy,"<p align=""justify""><font face=""Times New Roman"" size=""4"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mając na uwadze rosnącą pozycję przemysłu drzewnego w całej gospodarce krajowej, w szczeg&oacute;lności przemysłu meblarskiego, będącego obecnie czwartym w świecie eksporterem mebli, oraz stale zaostrzające się wymagania w zakresie ochrony środowiska, widzi się konieczność naukowego wsparcia w staraniach podejmowanych dla neutralizacji wszelkich zagrożeń dla środowiska występujących w tej sferze działalności wytw&oacute;rczej. Istnieje zatem pełne uzasadnienie zdecydowanego wspierania wysiłk&oacute;w ukierunkowanych na problemy zwalczania zapylenia w drzewnictwie.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""4"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Obecny stan wiedzy w odniesieniu do stosowania filtracji w procesach separacji najdrobniejszych pył&oacute;w powstałych podczas obr&oacute;bki drewna i materiał&oacute;w z niego pozyskiwanych opiera się w gł&oacute;wnej mierze na wynikach badań skoncentrowanych na wybranych zagadnieniach przebiegu odpylania filtracyjnego w przemyśle drzewnym. Rezultaty osiągnięte w Katedrze Inżynierii Środowiska Pracy wskazują wyraźnie na właściwe ukierunkowanie tych badań, na dobrze zorganizowaną bazę do ich prowadzenia &ndash; oryginalne i wszechstronnie sprawdzone w swej przydatności stanowisko doświadczalne, bardzo dobre wyposażenie aparaturowe i starannie wypracowaną metodykę prowadzenia wieloaspektowych badań o walorach poznawczych bądź ściśle ukierunkowanych na zastosowania praktyczne.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""4"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wszystko to sprawia, iż jesteśmy jedynym w kraju ośrodkiem prowadzącym prace naukowe z zakresu przemysłowego stosowania technik filtracyjnych w odpylaniu. Potwierdzeniem uznania czołowej pozycji w badaniach proces&oacute;w filtracyjnych w ścisłym powiązaniu z rzeczywistymi warunkami użytkowania struktur wł&oacute;knistych są wielorakie dokonania uzyskanie we wsp&oacute;łpracy we wcześniejszych okresach z Instytutem Technicznych Wyrob&oacute;w Wł&oacute;kienniczych Moratex a następnie z Instytutem Wł&oacute;kiennictwa w Łodzi. Prace, jakie w ramach projekt&oacute;w badawczych prowadzonych przez te Instytuty, były zadaniami samodzielnie realizowanymi w naszej Katedrze. Przyczyniły się do uzyskania rezultat&oacute;w bardzo wysoko ocenianych przez niezależnych recenzent&oacute;w. Bliskie kontakty z producentami i dostawcami samych materiał&oacute;w filtracyjnych oraz z producentami specjalistycznego wyposażenia odpylającego dla drzewnictwa trafnie ukierunkowuje realizowane w nieprzerwanym cyklu prace badawcze na problemy najważniejsze dla bezpośrednich odbiorc&oacute;w technik filtracyjnych o tym przeznaczeniu.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""4"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font><font face=""Times New Roman"" size=""4"">Przeprowadzone w ostatnim czasie prace badawcze w skali zwiększonej objęły, między innymi, istotne problemy wpływu wilgotności względnej powietrza na kształtowania się opor&oacute;w przepływu przez warstwę pyłową oraz zagadnienie stosowania medi&oacute;w filtracyjnych z powierzchniową warstwą mikrowł&oacute;kien. Dały one potwierdzenie zasadności stosowania wykończenia hydrofobowego powierzchni wł&oacute;knin filtracyjnych, użytkowanych w warunkach znacznego zawilgocenia. Uzyskano r&oacute;wnież wyniki świadczące o wysokiej skuteczności separacyjnej wł&oacute;knin o powierzchni modyfikowanej mikrowł&oacute;knami w zastosowaniu do wysoko rozdrobnionego pyłu ze szlifowania gładzącego element&oacute;w meblowych z drewna bukowego.</font></p>
<p align=""justify""><font face=""Times New Roman"" size=""4"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Powiązanie dotychczasowych osiągnięć naukowych i praktycznych w jedną monolityczną ca...",07/07/20,27,Badania efektu separacyjnego i nakładów energetycznych przy zastosowaniu najnowszych struktur włóknistych do filtracyjnego oczyszczania powietrza z pyłów powstających w procesach obróbki materiałów drzewnych,"filtracja, opory przepływu, skuteczność separacyjna, media filtracyjne, pył drzewny",173
176,4651,opisProjektuStanWiedzy,"<p>Charakterystyczne morfologicznie, o ciele scyzorykowato składanym, gatunki mechowc&oacute;w (Acari, Oribatida) należą do grupy Ptyctima i wykazują specyficzne wymagania ekologiczne, wynikające ze szczeg&oacute;lnych wymagań pokarmowych. Większość gatunk&oacute;w to ksylofagi, kt&oacute;re występują wszędzie tam, gdzie zalega martwa materia organiczna. Zasadnicza ich rola polega na mechanicznej fragmentacji ale r&oacute;wnież biorą udział w procesach rozkładu i mineralizacji materii organicznej, przy udziale symbiotycznych mikroorganizm&oacute;w. <br />Ptyctima znane są najprawdopodobniej od końca Jury, czyli jeszcze przed podziałem Pangei i dryfem kontynent&oacute;w (Niedbała 1991). Określenie centr&oacute;w pochodzenia i specjacji tej fauny jest możliwe po opracowaniu i poznaniu fauny całego świata, a zwłaszcza istotne było opracowanie obszar&oacute;w byłej Gondwany. <br />Fauna światowa, do roku 1980, przed podjęciem badań autora niniejszego projektu była bardzo słabo znana, liczyła kilkadziesiąt gatunk&oacute;w. Obecnie liczy ponad 700 gatunk&oacute;w. <br />Fauna Ptyctima jest coraz lepiej znana, jednak pozostaje jeszcze do opracowania najbardziej zr&oacute;żnicowana fauna Regionu Palearktycznego. Fauna ta znana jest ze starszych opracowań, często mało precyzyjnych, niekiedy błędnych, co uniemożliwia ustalenie poprawnej klasyfikacji i utrudnia wnioskowanie zoogeograficzne. <br />Ocena pochodzenia fauny możliwa jest w oparciu o dwie koncepcje zoogeograficzne: wikariant&oacute;w i panbiogeografii czyli, w pewnym uproszczeniu w oparciu o zasadę dryfu kontynent&oacute;w i tektoniki płyt oraz o zasadę dyspersji biernej. Doświadczenia zdobyte z opracowania fauny wysp Pacyfiku (Niedbała 1998a) udowodniły kapitalne znaczenie dyspersji biernej w rozmieszczeniu geograficznym. Do tego r&oacute;wnież opracowanie fauny Ptyctima Ameryki Centralnej (Niedbała, Schatz 1996), rzuciło pewne światło na możliwości dyspersyjne. W obu pracach wskazano na silny związek fauny oceanicznych wysp wulkanicznych z fauną wysp kontynentalnych i kontynent&oacute;w. Obecność licznych gatunk&oacute;w endemicznych na stosunkowo młodych wyspach wskazuje, że fauna ta jest młoda i często niekt&oacute;re gatunki nie przekraczają wieku kilku milion&oacute;w lat. Z kolei wiek gatunk&oacute;w Ptyctima, datujący się naco najmniej 200 mln lat umożliwić może określenie roli dryfu kontynentalnego na wsp&oacute;łczesne rozmieszczenie tych roztoczy. <br />Poznaniu fauny Ptyctima służyły kolejne granty badawcze: &bdquo;Geneza i wiek fauny Euptyctima (Acari, Oribatida) wysp Pacyfiku&rdquo;, nr 6 6003 9203, w latach 1993-1995, drugi projekt badawczy: nr 6 PO4C 080 10, w latach 1996-1998 - &bdquo;Poznanie fauny Euptyctima (Acari, Oribatida) i poszukiwanie centr&oacute;w ich pochodzenia: fauny Orientu i Regionu Australijskiego&rdquo;, trzeci projekt nr 6 PO4C 062 17 &bdquo;Poznanie fauny Mesoplophoroidea i Phthiracaroidea (Acari, Oribatida) oraz poszukiwanie centr&oacute;w ich pochodzenia: fauna Regionu Etiopskiego&rdquo;, zakończony VI 2001 r., czwarty, grant nr 6PO4C 034 21 &bdquo;Ocena r&oacute;żnorodności fauny Ptyctima (Acari, Oribatida) oraz poszukiwanie centr&oacute;w ich pochodzenia: fauna Regiony Neotropikalnego&rdquo;, zakończony w 2004 r. oraz piąty &bdquo;Ocena r&oacute;żnorodności fauny Ptyctima (Acari, Oribatida) oraz poszukiwania centr&oacute;w ich pochodzenia: fauna Regionu Afryki Południowej&rdquo;, projekt MNiI, nr rej. 6PO4C 068 27, zakończony 31 XII 2006 r. Fauna Nearktyki opracowana została w ramach polsko-amerykańskiego grantu z fundacji M. Skłodowskiej-Curie i zakończona w 2002 r. <br />Wszystkie te projekty badawcze uwieńczone zostały opublikowaniem kolejnych opracowań poszczeg&oacute;lnych region&oacute;w zoogeograficznych. Każdy z temat&oacute;w realizowany w ramach nadrzędnego tematu &bdquo;Wiek i pochodzenie fauny Ptyctima&rdquo; jest samodzielny i został tak przygotowany, że stanowił oddzielną monografię. <br />Faunie Ptyctima Palearktyki poświ...",06/12/29,23,"Ocena różnorodności fauny Ptyctima (Acari, Oribatida) oraz poszukiwanie centrów pochodzenia gatunków: fauna Regionu Palearktycznego","Ptyctima, centra pochodzenia, Palearktyka",173
